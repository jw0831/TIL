# 0922 class

## Decision tree 알고리즘 (의사결정 tool)

> 의사결정 트리: 
>
> - 트리 구조를 사용함
>
> - **랜덤포레스트** 의 기반은 의사결정 트리이다.
>
> - 의사결정 트리에서는 플로차트의 조건<마름모>의 설정이 매우 중요하다.
>
>   - ex) `<연봉 4000 넘어요?>` 네 ,` <출퇴근시간 1시간?>` 네, `<커피줘요?>` 아니오 : `안가요`(y data)  
>     이렇게 되면 대부분 취업이 되지 않는다..
>   - x data: `<결정 features>`    /    y data: `취업 여부 (o, x)`
>
> - data -> 의사결정 트리 (학습) -> 모델
>
> - 트리의 깊이가 얼마나 깊었을때 답을 얻을 수 있는가?
>
>   - 트리의 깊이가 깊어질수록 (분할선<기준>이 늘어날수록) 모델을 만드는데에 사용되어지는 훈련 데이터 `x data` 정확도는 올라간다. 하지만 예측데이터는 정확하지 않을 수도 있다.
>
>   - 데이터를 세분화 하였지만 패턴화는 잘 되어져 있지 않기때문에 적정한 정도가 좋다.
>
>     - 그러므로, 일반화가 잘된 모델이 더 좋다 : **Normal-fitting **
>
>     - 데이터가 과하게 맞추어져 있는 경우: **Over-fitting** (예측 오류 증가)
>
>     - 심하게 잘 맞추어져 있지 않는경우: **Under-fitting** (예측 불가)
>
>   - 적정한 수준에서 더 깊이 내려가는 것을 중단!
>
> Tip: **좋은 질문**을 찾아서 제일 **처음** 제시 할수록 좋다.
>
> #### 신용 평가 모델
>
> - 고객 행동 마케팅
> - 측정, 진단
> - 옛날에는 의사결정 트리를 많이 사용했지만, 여러 예측 불가능한 상황 때문에 A.I. 사용
> - 상대적으로 정확도가 낮다.
>   - 하지만, 조건이 **명확하게 떨어지거나, 문서화가 잘 되어있을경우**에 사용된다.
>
> Decision Tree: *data analysis* => **패턴을 예측 가능한 규칙들의 조합으로 표현한 것**
>
> - Terminal node 에는 교집합이 없다. 모두 전체 데이터의 부분집합이 되어버린다.
>
> 
>
> ------
>
> ## 분할 정복법 : **divide and conquer**
>
> - 주어진 문제를 작은 사례로 나누고(Divide) 각각의 작은 문제들을 해결하여 정목 (Conquer)하는 방법이다.
> - 문제의 사례를 2개 이상의 더 작은 사례로 나눈다. 이 작은 사례는 주로 원래 문제에서 따온다. 
>   - 나눈 작은 사례의 해답을 바로 얻을 수 있으면 해를 구하고, 아니면 더 작은 사례로 나눈다.
> - **해를 구할 수 있을 만큼 충분히 더 작은 사례로 나누어 해결하는 방법**이다.
> - 하향식 (top-down) 접근 방법으로 최상위 사례의 해답은 아래로 내려가면서 작은 사례에 대한 해답을 구함으로써 구한다.
>
> #### **분할정복법의 설계전략**
>
> 1. 문제 사례를 하나 이상의 작은 사례로 분할(Divide)한다.
> 2. 작은 사례들을 각각 정복(Conquer)한다. 작은 사례가 충분히 작지 않은 이상 재귀를 사용한다.
> 3. 필요하다면, 작은 사례에 대한 해답을 통합(Combine)하여 원래 사례의 해답을 구한다.
>
> #### 분할정복법의 장단점
>
> 장점: 문제를 나눔으로써 어려운 문제를 해결할 수 있다는 엄청나게 중요한 장점이 있다. 그리고 이 방식이 그대로 사용되는 효율적인 알고리즘들도 여럿 있으며, 문제를 나누어 해결한다는 특징상 병렬적으로 문제를 해결하는 데 큰 강점이 있다. 
>
> 단점: 함수를 재귀적으로 호출한다는 점에서 함수 호출로 인한 오버헤드가 발생하며, 스택에 다양한 데이터를 보관하고 있어야 하므로 [스택 오버플로우](https://namu.wiki/w/버퍼 오버플로)가 발생하거나 과도한 메모리 사용을 하게 되는 단점
>
> 출처: [알고리즘-분할정복법-Divide-and-Conquer [코드 저장소]](https://kimch3617.tistory.com/entry/)
>
> 
>
> ### 최고의 분할 선택 : purity (순도)
>
> - 순수 (Pure) : 영역이 단일 클레스로 이루어진 부분집합
> - 의사결정트리에서 각각의 영역에대한 순도가 증가 하는지 보아야함. 순도가 증가할수록 명확한 구분선임.
>   - 불순도가 최소화 하는 방향으로 구분선을 찾는다 = **학습한다**
>   - `<순도 증가 = 불순도 감소>`  => **정보획득**
>   - 정보획득이 최대인 구분선을 찾는것이 목표
>
> ### 순도를 측정하는 방법 (얼마만큼 해당영역이 순수한지)
>
> - **엔트로피** *(대표적)* (entropy) : 복잡도
>
>   - ex) 방안이 어지러울때, 무엇을 치웠을때 가장 깔끔할까?
>
>   - $$
>     Entropy(A) = 
>     
>     H(p)=−∑^n_iP(i)log_2P(i)
>     $$
>
>   참조:  
>
>   - [딥러닝에서 Cross Entropy란?](https://ditu.tistory.com/23)
>
>   - [정보이론 및 신호 복잡도 측정 / entropy](https://horizon.kias.re.kr/12415/)
>
> - **정보 획득량** : 분할 전 (s1)영역에 대한 엔트로피 - 분할 후 생성된 (s2) 영역에 대한 엔트로피
>
>   - information gain = Entropy (s1) - Entropy (s2)
>   - 정보 획득량을 최대로 하는것을 목표!
>
> 
>
> #### XGBoost : 여러 개의 Decision Tree 조합 (앙상블: Ensemble)을 알려준다.
>
> - 앙상블 기법 (여러 모델의 조합: 두가지 방법으로 나뉜다) 
>
>   - 배깅 (Bagging) 을 사용한 모델이 바로 [**랜덤 포레스트**](https://bkshin.tistory.com/entry/머신러닝-5-랜덤-포레스트Random-Forest와-앙상블Ensemble) 입니다. (병렬 parallel)
>
>   - 부스팅 (Boosting) (직렬 sequential, series)
>     - 부스팅은 배깅에 비해 error가 적다. 즉, 성능이 좋다. 하지만 속도가 느리고 오버피팅이 될 가능성이 있다. 실제 사용은 상황에 따라 다르고, 개별 결정 트리의 낮은 성능이 문제라면 부스팅이 적합, 오버피팅이 문제라면 배깅이 적합
>     - 머신러닝의 기술중 하나
>
>   [앙상블 방법의 이해](https://untitledtblog.tistory.com/156)
>
>   [앙상블 학습: 배깅과 부스팅](https://bkshin.tistory.com/entry/머신러닝-11-앙상블-학습-Ensemble-Learning-배깅Bagging과-부스팅Boosting)
>
>   [앙상블, 알고리즘의 variance와 bias](https://dambaekday.tistory.com/5)



------

## Random forest

>- Decision tree 기반
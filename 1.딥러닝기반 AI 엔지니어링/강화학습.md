# 강화학습 (reinforcement-learning)

Alphago -> reinforcement learning 

- 시행착오로 부터 배운다.
- 강화학습 앞으로 각광받는 분야
- 주식또한 강화학습이 가장 잘 한다. (펀드 메니저)
- 지금 당장 취업에는 도움이 되지 않지만 앞으로 많은 도움이 된다.



#### 다이나믹 프로그래밍 (Dynamic Programming)

큰문제를 해결하기 위해서, 작은문제부터 해결해 나아가는 것

제사용 가능

- 궁금하면 강사님에게로부터 개인적으로 배울 수 있음.



```python
# 강화 학습을 연습하기 위한 환경(체육관)을 제공해주는 패키지
pip install gym
```

```python
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import gym
from gym.envs.registration import register

# 아래의 방식을 통해 게임을 등록함. 특히 is_slippery는 매우 중요함. 
"""
- 새로운 게임 형식을 만들어줌.
"""
#체육관 등록 옵션 : 미끌거리지 않게 해주세요 로 등록
register(
    id='FrozenLakeNotSlippery-v1',
    entry_point='gym.envs.toy_text:FrozenLakeEnv',
    kwargs={'map_name' : '4x4', 'is_slippery': False},
) 
```

```python
env = gym.make("FrozenLakeNotSlippery-v1")
env.reset()
"""
0: left, 1: down, 2: right, 3: up 
"""
complete_actions = [2, 2, 1, 1, 1, 2]
for action in complete_actions:
    #action = env.action_space.sample() # randomly select action 
    """
    - new_state: 액션을 취해서 새롭게 옮겨진 위치 
    - reward: 보상을 얻었는지 여부
    - done: 게임이 끝났는지 여부(hole에 빠지거나, )
    """
    new_state, reward, done, _ = env.step(action)# action을 적용하고 업데이트
    env.render() #현재 상황을 보여줌
    if done is True:
        print("done, reward: {}".format(reward))
```

```python
env = gym.make("FrozenLakeNotSlippery-v1")
observation = env.reset()

"""
- Q는 개별 observation(현재 말의 위치)와 해당 obs에서 취할 수 있는 action별로 이득이 표현된 테이블입니다. 
- 뒤쪽에서 다시 설명되겠지만, 게임이 종료되었을 때 얻을 수 있는 reward가 앞으로 전달되어, 각 값을 업데이트해줍니다. 
"""
Q = np.zeros([env.observation_space.n, env.action_space.n])

num_episodes = 800 # 일종의 epoch, 혹은 라이프.
decay_rate = 0.99 
# reward는 전달 과정에서 누가 계속 까먹기 때문에, decay rate가 발생함. 
# 사실 맨 끝에서 얻은 Reward가 앞가지 전달되는데 아무런 변화가 없어야 한다는 것도, 직관적으로 이상하지 않나요? 
# 또한, 이렇게 모델링 해야, 짧은 path를 거쳐 지나온 놈의 경우에 step에서 높은 Q(partial reward)를 가짐 

rlist = []
for i in range(0, num_episodes):
    state = env.reset()
    e = 1. / ((i//100)+1)
    """
    goal에 이르는 답이 여러가지일 수 있는데, 어느 정도의 랜덤성을 통해서 탐험을 하지 않을 경우, 초기의 답만을 가지게 됨. 
    사실 미로의 경우는 탐험을 통해 더 짧은 길을 찾을 수도 있습니다. 
    따라서, 여기서는 이미, 최적의 path를 찾았다고 해도 다시 더 좋은 path를 찾을 수 있도록 exploration을 보장합니다. 
    """
    #env.render()
    rAll = 0 # 한 episode 별로 얻을 수 있는 reward의 총합
    done = False # hole에 빠지거나, Goal에 도달하면 True
    while not done:
        if np.random.randn(1) < e: # 탐험을 통해 더 좋은 길을 확보
            # np.random.randn 은 norm(0, 1)
            # 즉, 에피소드가 반복되어 e가 감소되어도, 대략 0.5 정도의 확률로 탐험을 하는 것이 가능해야 함. 
            action = env.action_space.sample()
        else:
            action = np.argmax(Q[state, :])
        new_state, reward, done, _ = env.step(action)
        # 다음 단계로 가서 리워드를 얻는 다면, 이전단계까지 잘 온 것이므로 이전 단계에도 리워드를 준다. 
        Q[state, action] = reward + decay_rate*np.max(Q[new_state, :])
        rAll+=reward
        state = new_state # state update
    rlist.append(rAll)
print('complete')
plt.figure(figsize=(12, 2))
plt.scatter(range(0, len(rlist)), rlist, marker='^', s=1, color='red')
#plt.savefig('../../assets/images/markdown_img/180625_reinforcement_base.svg')
plt.show()
Q_df = pd.DataFrame(Q, index = [(i//4, i%4) for i in range(0, Q.shape[0])], 
                    columns=['left', 'down', 'right', 'up'])
print(Q_df)
```

## 20.12.14

```python
import gym
import random
from gym.envs.registration import register
import matplotlib.pyplot as plt

register(
    id='FrozenLake-v3',
    entry_point='gym.envs.toy_text:FrozenLakeEnv',
    kwargs={'map_name': '4x4',
            'is_slippery': False}
)

env = gym.make('FrozenLake-v0')
env.render()

num_episodes = 2000
rList = []
for i in range(num_episodes):
    # Reset environment and get first new observation
    env.reset()
    rAll = 0
    done = False

    while not done:
        # Random action
        action = random.randint(0, env.action_space.n - 1)

        # Get new state and reward from environment
        _state, reward, done, _info = env.step(action)

        # rAll will be 1 if success, otherwise
        rAll += reward
    rList.append(rAll)
print("Success rate: " + str(sum(rList) / num_episodes))
plt.plot(rList)
plt.show()
```

<img src="image.assets/스크린샷 2020-12-14 오전 10.44.31.png" alt="result" style="zoom:50%;" />

```python
import gym
import numpy as np
import matplotlib.pyplot as plt
from gym.envs.registration import register
import random as pr

# https://gist.github.com/stober/1943451


def rargmax(vector):
    """ Argmax that chooses randomly among eligible maximum indices. """
    m = np.amax(vector)
    indices = np.nonzero(vector == m)[0]
    return pr.choice(indices)


register(
    id='FrozenLake-v33',
    entry_point='gym.envs.toy_text:FrozenLakeEnv',
    kwargs={'map_name': '4x4',
            'is_slippery': False}
)
env = gym.make('FrozenLake-v3')

# Initialize table with all zeros
Q = np.zeros([env.observation_space.n, env.action_space.n])
# Set learning parameters
num_episodes = 2000

# create lists to contain total rewards and steps per episode
rList = []
for i in range(num_episodes):
    # Reset environment and get first new observation
    state = env.reset()
    rAll = 0
    done = False

    # The Q-Table learning algorithm
    while not done:
        action = rargmax(Q[state, :])

        # Get new state and reward from environment
        new_state, reward, done, _ = env.step(action)

        # Update Q-Table with new knowledge using learning rate
        Q[state, action] = reward + np.max(Q[new_state, :])

        rAll += reward
        state = new_state

    rList.append(rAll)

print("Success rate: " + str(sum(rList) / num_episodes))
print("Final Q-Table Values")
print("LEFT DOWN RIGHT UP")
print(Q)
plt.bar(range(len(rList)), rList, color="blue")
plt.show()
# Success rate = 0.955 거의 한번 틀린 경우

```

<img src="image.assets/스크린샷 2020-12-14 오전 10.52.10.png" alt="result" style="zoom:50%;" />

```python
import gym
import numpy as np
import matplotlib.pyplot as plt
from gym.envs.registration import register

register(
    id='FrozenLake-v3',
    entry_point='gym.envs.toy_text:FrozenLakeEnv',
    kwargs={'map_name': '4x4',
            'is_slippery': False}
)

env = gym.make('FrozenLake-v3')

# Initialize table with all zeros
Q = np.zeros([env.observation_space.n, env.action_space.n])
# Discount factor
dis = .99
num_episodes = 2000
rList = []

for i in range(num_episodes):
    # Reset environment and get first new observation
    state = env.reset()
    rAll = 0
    done = False

    # The Q-Table learning algorithm
    while not done:
        # Choose an action by greedily (with noise) picking from Q table
        action = np.argmax(Q[state, :] + np.random.randn(1,
                                                         env.action_space.n) / (i + 1))

        # Get new state and reward from environment
        new_state, reward, done, _ = env.step(action)

        # Update Q-Table with new knowledge using decay rate
        Q[state, action] = reward + dis * np.max(Q[new_state, :])
        
        rAll += reward
        state = new_state
        
    rList.append(rAll)
    
print("success rate: " + str(sum(rList) / num_episodes))
print("Final Q-Table Values")
print(Q)
plt.bar(range(len(rList)), rList, color = "blue")
plt.show()
```

<img src="image.assets/스크린샷 2020-12-14 오후 2.11.53.png" alt="result" style="zoom:50%;" />

## 참고자료:

[딥러닝 책](https://www.deeplearningbook.org/)

[강화학습 강좌](https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37)

[네이버-딥러닝과 강화 학습으로 나보다 잘하는 쿠키런 AI 구현하기](https://www.slideshare.net/deview/ai-67608549)

[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/bookdraft2018mar21.pdf)

[Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)

- Frozen Lake Environment
  - f: frozen
  - h: hole
  - g: goal (최종 목표)

[Introduction to Reinforcement Learning](https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf)

- https://www.davidsilver.uk/teaching/

https://dohk.tistory.com/239

http://jaynewho.com/post/10


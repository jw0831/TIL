{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN : 딥러닝의 flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "# 안 나누어도 되지만 정규화를 진행하면 편협적인 모델의 생성은 막을 수 있다.\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 5 4 1 5 4 7 6 0 0]\n",
      "[7 8 1 3 6 0 5 4 2 9]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(10,10)) #복원추출\n",
    "print(np.random.choice(10,10, replace = False)) #비복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.6509804 ,\n",
       "        0.75686276, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.99215686,\n",
       "        0.94509804, 0.8156863 , 0.5411765 , 0.02745098, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.03529412, 0.99215686,\n",
       "        0.9882353 , 0.9882353 , 0.9882353 , 0.11372549, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30980393, 0.99215686,\n",
       "        0.9882353 , 0.9882353 , 0.9882353 , 0.11372549, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.1764706 , 0.99215686,\n",
       "        0.9882353 , 0.9882353 , 0.9882353 , 0.11372549, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.58431375, 0.99215686,\n",
       "        0.9882353 , 0.9882353 , 0.92941177, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.09411765, 0.79607844, 0.99215686,\n",
       "        0.9882353 , 0.9882353 , 0.5176471 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38431373, 0.9882353 , 0.99215686,\n",
       "        0.9882353 , 0.9882353 , 0.5176471 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.27058825, 0.93333334, 0.9882353 , 0.99215686,\n",
       "        0.9882353 , 0.9764706 , 0.43137255, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3529412 , 0.88235295, 0.7490196 , 0.99215686,\n",
       "        0.9882353 , 0.92941177, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.9411765 , 0.99215686, 0.99215686, 1.        ,\n",
       "        0.99215686, 0.93333334, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1254902 , 0.94509804, 0.9882353 , 0.9882353 , 0.99215686,\n",
       "        0.9882353 , 0.7882353 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "        0.5529412 , 0.9882353 , 0.9882353 , 0.9882353 , 0.99215686,\n",
       "        0.972549  , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.11764706,\n",
       "        0.9882353 , 0.9882353 , 0.9882353 , 0.9882353 , 0.99215686,\n",
       "        0.47843137, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.11764706,\n",
       "        0.9882353 , 0.9882353 , 0.9882353 , 0.9882353 , 0.99215686,\n",
       "        0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.11764706,\n",
       "        0.9882353 , 0.9882353 , 0.6392157 , 0.9882353 , 0.7019608 ,\n",
       "        0.05098039, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.11764706,\n",
       "        0.9882353 , 0.9882353 , 0.9882353 , 0.9882353 , 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.11764706,\n",
       "        0.9882353 , 0.9882353 , 0.9882353 , 0.43529412, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.02745098,\n",
       "        0.5372549 , 0.9137255 , 0.9137255 , 0.33333334, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23921569, 0.58431375, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape #(700, 784)\n",
    "# X_train[0].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=2 , input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax')) #0~9까지의 숫자니까 unit=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 2.2325 - accuracy: 0.1500 - val_loss: 2.1515 - val_accuracy: 0.2200\n",
      "Epoch 2/300\n",
      "70/70 [==============================] - 0s 757us/step - loss: 2.1456 - accuracy: 0.1871 - val_loss: 2.0724 - val_accuracy: 0.2433\n",
      "Epoch 3/300\n",
      "70/70 [==============================] - 0s 732us/step - loss: 2.0924 - accuracy: 0.2143 - val_loss: 2.0314 - val_accuracy: 0.2733\n",
      "Epoch 4/300\n",
      "70/70 [==============================] - 0s 717us/step - loss: 2.0491 - accuracy: 0.2400 - val_loss: 1.9814 - val_accuracy: 0.2733\n",
      "Epoch 5/300\n",
      "70/70 [==============================] - 0s 751us/step - loss: 2.0113 - accuracy: 0.2457 - val_loss: 1.9499 - val_accuracy: 0.2967\n",
      "Epoch 6/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 1.9752 - accuracy: 0.2600 - val_loss: 1.9151 - val_accuracy: 0.3000\n",
      "Epoch 7/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 1.9403 - accuracy: 0.2686 - val_loss: 1.8810 - val_accuracy: 0.2933\n",
      "Epoch 8/300\n",
      "70/70 [==============================] - 0s 714us/step - loss: 1.9086 - accuracy: 0.2800 - val_loss: 1.8520 - val_accuracy: 0.3033\n",
      "Epoch 9/300\n",
      "70/70 [==============================] - 0s 717us/step - loss: 1.8789 - accuracy: 0.2800 - val_loss: 1.8268 - val_accuracy: 0.3233\n",
      "Epoch 10/300\n",
      "70/70 [==============================] - 0s 741us/step - loss: 1.8500 - accuracy: 0.2800 - val_loss: 1.8042 - val_accuracy: 0.3067\n",
      "Epoch 11/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 1.8239 - accuracy: 0.2900 - val_loss: 1.7859 - val_accuracy: 0.3267\n",
      "Epoch 12/300\n",
      "70/70 [==============================] - 0s 722us/step - loss: 1.7989 - accuracy: 0.2900 - val_loss: 1.7683 - val_accuracy: 0.3400\n",
      "Epoch 13/300\n",
      "70/70 [==============================] - 0s 714us/step - loss: 1.7769 - accuracy: 0.3071 - val_loss: 1.7456 - val_accuracy: 0.3300\n",
      "Epoch 14/300\n",
      "70/70 [==============================] - 0s 710us/step - loss: 1.7551 - accuracy: 0.3186 - val_loss: 1.7303 - val_accuracy: 0.3400\n",
      "Epoch 15/300\n",
      "70/70 [==============================] - 0s 748us/step - loss: 1.7356 - accuracy: 0.3343 - val_loss: 1.7160 - val_accuracy: 0.3567\n",
      "Epoch 16/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.7168 - accuracy: 0.3443 - val_loss: 1.7018 - val_accuracy: 0.3667\n",
      "Epoch 17/300\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.6990 - accuracy: 0.3700 - val_loss: 1.6870 - val_accuracy: 0.3633\n",
      "Epoch 18/300\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.6812 - accuracy: 0.3686 - val_loss: 1.6788 - val_accuracy: 0.3700\n",
      "Epoch 19/300\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.6660 - accuracy: 0.3771 - val_loss: 1.6626 - val_accuracy: 0.3833\n",
      "Epoch 20/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 1.6504 - accuracy: 0.3843 - val_loss: 1.6544 - val_accuracy: 0.3867\n",
      "Epoch 21/300\n",
      "70/70 [==============================] - 0s 747us/step - loss: 1.6350 - accuracy: 0.3857 - val_loss: 1.6442 - val_accuracy: 0.3867\n",
      "Epoch 22/300\n",
      "70/70 [==============================] - 0s 722us/step - loss: 1.6203 - accuracy: 0.4014 - val_loss: 1.6376 - val_accuracy: 0.4000\n",
      "Epoch 23/300\n",
      "70/70 [==============================] - 0s 719us/step - loss: 1.6067 - accuracy: 0.3957 - val_loss: 1.6238 - val_accuracy: 0.4033\n",
      "Epoch 24/300\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.5937 - accuracy: 0.4129 - val_loss: 1.6151 - val_accuracy: 0.4033\n",
      "Epoch 25/300\n",
      "70/70 [==============================] - 0s 713us/step - loss: 1.5799 - accuracy: 0.4057 - val_loss: 1.6102 - val_accuracy: 0.4133\n",
      "Epoch 26/300\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.5670 - accuracy: 0.4057 - val_loss: 1.6015 - val_accuracy: 0.4133\n",
      "Epoch 27/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 1.5553 - accuracy: 0.4186 - val_loss: 1.5931 - val_accuracy: 0.4133\n",
      "Epoch 28/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 1.5438 - accuracy: 0.4243 - val_loss: 1.5835 - val_accuracy: 0.4167\n",
      "Epoch 29/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 1.5314 - accuracy: 0.4214 - val_loss: 1.5754 - val_accuracy: 0.4200\n",
      "Epoch 30/300\n",
      "70/70 [==============================] - 0s 704us/step - loss: 1.5219 - accuracy: 0.4314 - val_loss: 1.5713 - val_accuracy: 0.4133\n",
      "Epoch 31/300\n",
      "70/70 [==============================] - 0s 731us/step - loss: 1.5094 - accuracy: 0.4429 - val_loss: 1.5641 - val_accuracy: 0.4300\n",
      "Epoch 32/300\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.5008 - accuracy: 0.4171 - val_loss: 1.5587 - val_accuracy: 0.4233\n",
      "Epoch 33/300\n",
      "70/70 [==============================] - 0s 769us/step - loss: 1.4897 - accuracy: 0.4457 - val_loss: 1.5507 - val_accuracy: 0.4300\n",
      "Epoch 34/300\n",
      "70/70 [==============================] - 0s 749us/step - loss: 1.4800 - accuracy: 0.4386 - val_loss: 1.5445 - val_accuracy: 0.4300\n",
      "Epoch 35/300\n",
      "70/70 [==============================] - 0s 732us/step - loss: 1.4697 - accuracy: 0.4429 - val_loss: 1.5436 - val_accuracy: 0.4133\n",
      "Epoch 36/300\n",
      "70/70 [==============================] - 0s 730us/step - loss: 1.4588 - accuracy: 0.4386 - val_loss: 1.5338 - val_accuracy: 0.4200\n",
      "Epoch 37/300\n",
      "70/70 [==============================] - 0s 858us/step - loss: 1.4505 - accuracy: 0.4471 - val_loss: 1.5349 - val_accuracy: 0.4300\n",
      "Epoch 38/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.4430 - accuracy: 0.4400 - val_loss: 1.5274 - val_accuracy: 0.4400\n",
      "Epoch 39/300\n",
      "70/70 [==============================] - 0s 735us/step - loss: 1.4330 - accuracy: 0.4543 - val_loss: 1.5318 - val_accuracy: 0.4233\n",
      "Epoch 40/300\n",
      "70/70 [==============================] - 0s 736us/step - loss: 1.4242 - accuracy: 0.4514 - val_loss: 1.5129 - val_accuracy: 0.4467\n",
      "Epoch 41/300\n",
      "70/70 [==============================] - 0s 723us/step - loss: 1.4156 - accuracy: 0.4514 - val_loss: 1.5107 - val_accuracy: 0.4300\n",
      "Epoch 42/300\n",
      "70/70 [==============================] - 0s 732us/step - loss: 1.4063 - accuracy: 0.4614 - val_loss: 1.5135 - val_accuracy: 0.4200\n",
      "Epoch 43/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.3956 - accuracy: 0.4757 - val_loss: 1.5201 - val_accuracy: 0.4267\n",
      "Epoch 44/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.3898 - accuracy: 0.4571 - val_loss: 1.4987 - val_accuracy: 0.4400\n",
      "Epoch 45/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.3816 - accuracy: 0.4643 - val_loss: 1.4927 - val_accuracy: 0.4400\n",
      "Epoch 46/300\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.3720 - accuracy: 0.4586 - val_loss: 1.4890 - val_accuracy: 0.4367\n",
      "Epoch 47/300\n",
      "70/70 [==============================] - 0s 716us/step - loss: 1.3630 - accuracy: 0.4729 - val_loss: 1.4834 - val_accuracy: 0.4467\n",
      "Epoch 48/300\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.3566 - accuracy: 0.4786 - val_loss: 1.4784 - val_accuracy: 0.4533\n",
      "Epoch 49/300\n",
      "70/70 [==============================] - 0s 736us/step - loss: 1.3455 - accuracy: 0.4857 - val_loss: 1.4670 - val_accuracy: 0.4567\n",
      "Epoch 50/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.3431 - accuracy: 0.4714 - val_loss: 1.4702 - val_accuracy: 0.4433\n",
      "Epoch 51/300\n",
      "70/70 [==============================] - 0s 735us/step - loss: 1.3335 - accuracy: 0.4814 - val_loss: 1.4665 - val_accuracy: 0.4400\n",
      "Epoch 52/300\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.3271 - accuracy: 0.4871 - val_loss: 1.4688 - val_accuracy: 0.4467\n",
      "Epoch 53/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 1.3200 - accuracy: 0.4914 - val_loss: 1.4629 - val_accuracy: 0.4433\n",
      "Epoch 54/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 1.3135 - accuracy: 0.4871 - val_loss: 1.4592 - val_accuracy: 0.4533\n",
      "Epoch 55/300\n",
      "70/70 [==============================] - 0s 731us/step - loss: 1.3054 - accuracy: 0.4971 - val_loss: 1.4471 - val_accuracy: 0.4533\n",
      "Epoch 56/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.2996 - accuracy: 0.4929 - val_loss: 1.4447 - val_accuracy: 0.4467\n",
      "Epoch 57/300\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.2916 - accuracy: 0.5071 - val_loss: 1.4403 - val_accuracy: 0.4433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "70/70 [==============================] - 0s 754us/step - loss: 1.2853 - accuracy: 0.5129 - val_loss: 1.4372 - val_accuracy: 0.4533\n",
      "Epoch 59/300\n",
      "70/70 [==============================] - 0s 742us/step - loss: 1.2791 - accuracy: 0.5057 - val_loss: 1.4321 - val_accuracy: 0.4567\n",
      "Epoch 60/300\n",
      "70/70 [==============================] - 0s 706us/step - loss: 1.2741 - accuracy: 0.5186 - val_loss: 1.4332 - val_accuracy: 0.4600\n",
      "Epoch 61/300\n",
      "70/70 [==============================] - 0s 730us/step - loss: 1.2675 - accuracy: 0.5129 - val_loss: 1.4298 - val_accuracy: 0.4467\n",
      "Epoch 62/300\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.2605 - accuracy: 0.5157 - val_loss: 1.4233 - val_accuracy: 0.4533\n",
      "Epoch 63/300\n",
      "70/70 [==============================] - 0s 753us/step - loss: 1.2551 - accuracy: 0.5243 - val_loss: 1.4129 - val_accuracy: 0.4567\n",
      "Epoch 64/300\n",
      "70/70 [==============================] - 0s 748us/step - loss: 1.2491 - accuracy: 0.5243 - val_loss: 1.4195 - val_accuracy: 0.4633\n",
      "Epoch 65/300\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.2436 - accuracy: 0.5329 - val_loss: 1.4087 - val_accuracy: 0.4667\n",
      "Epoch 66/300\n",
      "70/70 [==============================] - 0s 725us/step - loss: 1.2368 - accuracy: 0.5343 - val_loss: 1.4096 - val_accuracy: 0.4667\n",
      "Epoch 67/300\n",
      "70/70 [==============================] - 0s 711us/step - loss: 1.2318 - accuracy: 0.5329 - val_loss: 1.4052 - val_accuracy: 0.4667\n",
      "Epoch 68/300\n",
      "70/70 [==============================] - 0s 722us/step - loss: 1.2265 - accuracy: 0.5357 - val_loss: 1.3937 - val_accuracy: 0.4800\n",
      "Epoch 69/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 1.2220 - accuracy: 0.5386 - val_loss: 1.3984 - val_accuracy: 0.4800\n",
      "Epoch 70/300\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.2169 - accuracy: 0.5414 - val_loss: 1.3917 - val_accuracy: 0.4667\n",
      "Epoch 71/300\n",
      "70/70 [==============================] - 0s 717us/step - loss: 1.2121 - accuracy: 0.5443 - val_loss: 1.3898 - val_accuracy: 0.4767\n",
      "Epoch 72/300\n",
      "70/70 [==============================] - 0s 718us/step - loss: 1.2060 - accuracy: 0.5457 - val_loss: 1.3845 - val_accuracy: 0.4767\n",
      "Epoch 73/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.2013 - accuracy: 0.5543 - val_loss: 1.3827 - val_accuracy: 0.4767\n",
      "Epoch 74/300\n",
      "70/70 [==============================] - 0s 731us/step - loss: 1.1980 - accuracy: 0.5457 - val_loss: 1.3784 - val_accuracy: 0.4800\n",
      "Epoch 75/300\n",
      "70/70 [==============================] - 0s 723us/step - loss: 1.1928 - accuracy: 0.5443 - val_loss: 1.3790 - val_accuracy: 0.4900\n",
      "Epoch 76/300\n",
      "70/70 [==============================] - 0s 741us/step - loss: 1.1867 - accuracy: 0.5514 - val_loss: 1.3755 - val_accuracy: 0.4967\n",
      "Epoch 77/300\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.1809 - accuracy: 0.5414 - val_loss: 1.3790 - val_accuracy: 0.4800\n",
      "Epoch 78/300\n",
      "70/70 [==============================] - 0s 778us/step - loss: 1.1775 - accuracy: 0.5629 - val_loss: 1.3797 - val_accuracy: 0.4767\n",
      "Epoch 79/300\n",
      "70/70 [==============================] - 0s 755us/step - loss: 1.1740 - accuracy: 0.5643 - val_loss: 1.3684 - val_accuracy: 0.5000\n",
      "Epoch 80/300\n",
      "70/70 [==============================] - 0s 752us/step - loss: 1.1699 - accuracy: 0.5543 - val_loss: 1.3693 - val_accuracy: 0.4933\n",
      "Epoch 81/300\n",
      "70/70 [==============================] - 0s 757us/step - loss: 1.1654 - accuracy: 0.5586 - val_loss: 1.3715 - val_accuracy: 0.5033\n",
      "Epoch 82/300\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.1608 - accuracy: 0.5557 - val_loss: 1.3693 - val_accuracy: 0.5033\n",
      "Epoch 83/300\n",
      "70/70 [==============================] - 0s 751us/step - loss: 1.1576 - accuracy: 0.5529 - val_loss: 1.3705 - val_accuracy: 0.5033\n",
      "Epoch 84/300\n",
      "70/70 [==============================] - 0s 722us/step - loss: 1.1541 - accuracy: 0.5571 - val_loss: 1.3643 - val_accuracy: 0.5033\n",
      "Epoch 85/300\n",
      "70/70 [==============================] - 0s 730us/step - loss: 1.1508 - accuracy: 0.5586 - val_loss: 1.3646 - val_accuracy: 0.5067\n",
      "Epoch 86/300\n",
      "70/70 [==============================] - 0s 716us/step - loss: 1.1449 - accuracy: 0.5657 - val_loss: 1.3634 - val_accuracy: 0.5100\n",
      "Epoch 87/300\n",
      "70/70 [==============================] - 0s 721us/step - loss: 1.1411 - accuracy: 0.5657 - val_loss: 1.3606 - val_accuracy: 0.5033\n",
      "Epoch 88/300\n",
      "70/70 [==============================] - 0s 723us/step - loss: 1.1376 - accuracy: 0.5643 - val_loss: 1.3635 - val_accuracy: 0.5033\n",
      "Epoch 89/300\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.1330 - accuracy: 0.5714 - val_loss: 1.3622 - val_accuracy: 0.5033\n",
      "Epoch 90/300\n",
      "70/70 [==============================] - 0s 762us/step - loss: 1.1299 - accuracy: 0.5700 - val_loss: 1.3675 - val_accuracy: 0.5100\n",
      "Epoch 91/300\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.1259 - accuracy: 0.5629 - val_loss: 1.3613 - val_accuracy: 0.5133\n",
      "Epoch 92/300\n",
      "70/70 [==============================] - 0s 751us/step - loss: 1.1232 - accuracy: 0.5729 - val_loss: 1.3564 - val_accuracy: 0.5133\n",
      "Epoch 93/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 1.1232 - accuracy: 0.5671 - val_loss: 1.3660 - val_accuracy: 0.5067\n",
      "Epoch 94/300\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.1169 - accuracy: 0.5786 - val_loss: 1.3630 - val_accuracy: 0.5067\n",
      "Epoch 95/300\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.1117 - accuracy: 0.5686 - val_loss: 1.3510 - val_accuracy: 0.5133\n",
      "Epoch 96/300\n",
      "70/70 [==============================] - 0s 766us/step - loss: 1.1118 - accuracy: 0.5800 - val_loss: 1.3570 - val_accuracy: 0.5200\n",
      "Epoch 97/300\n",
      "70/70 [==============================] - 0s 929us/step - loss: 1.1063 - accuracy: 0.5757 - val_loss: 1.3551 - val_accuracy: 0.5100\n",
      "Epoch 98/300\n",
      "70/70 [==============================] - 0s 806us/step - loss: 1.1022 - accuracy: 0.5800 - val_loss: 1.3518 - val_accuracy: 0.5167\n",
      "Epoch 99/300\n",
      "70/70 [==============================] - 0s 846us/step - loss: 1.0987 - accuracy: 0.5829 - val_loss: 1.3603 - val_accuracy: 0.5200\n",
      "Epoch 100/300\n",
      "70/70 [==============================] - 0s 755us/step - loss: 1.0960 - accuracy: 0.5800 - val_loss: 1.3587 - val_accuracy: 0.5133\n",
      "Epoch 101/300\n",
      "70/70 [==============================] - 0s 761us/step - loss: 1.0917 - accuracy: 0.5857 - val_loss: 1.3517 - val_accuracy: 0.5267\n",
      "Epoch 102/300\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.0885 - accuracy: 0.5871 - val_loss: 1.3566 - val_accuracy: 0.5267\n",
      "Epoch 103/300\n",
      "70/70 [==============================] - 0s 751us/step - loss: 1.0885 - accuracy: 0.5786 - val_loss: 1.3546 - val_accuracy: 0.5133\n",
      "Epoch 104/300\n",
      "70/70 [==============================] - 0s 759us/step - loss: 1.0832 - accuracy: 0.5843 - val_loss: 1.3481 - val_accuracy: 0.5333\n",
      "Epoch 105/300\n",
      "70/70 [==============================] - 0s 751us/step - loss: 1.0801 - accuracy: 0.5700 - val_loss: 1.3500 - val_accuracy: 0.5400\n",
      "Epoch 106/300\n",
      "70/70 [==============================] - 0s 756us/step - loss: 1.0768 - accuracy: 0.5800 - val_loss: 1.3475 - val_accuracy: 0.5267\n",
      "Epoch 107/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 1.0719 - accuracy: 0.5771 - val_loss: 1.3794 - val_accuracy: 0.5167\n",
      "Epoch 108/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.0714 - accuracy: 0.6157 - val_loss: 1.3639 - val_accuracy: 0.5267\n",
      "Epoch 109/300\n",
      "70/70 [==============================] - 0s 757us/step - loss: 1.0675 - accuracy: 0.6200 - val_loss: 1.3517 - val_accuracy: 0.5367\n",
      "Epoch 110/300\n",
      "70/70 [==============================] - 0s 753us/step - loss: 1.0669 - accuracy: 0.6014 - val_loss: 1.3499 - val_accuracy: 0.5367\n",
      "Epoch 111/300\n",
      "70/70 [==============================] - 0s 758us/step - loss: 1.0626 - accuracy: 0.6071 - val_loss: 1.3556 - val_accuracy: 0.5367\n",
      "Epoch 112/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.0632 - accuracy: 0.6043 - val_loss: 1.3551 - val_accuracy: 0.5267\n",
      "Epoch 113/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 1.0575 - accuracy: 0.6171 - val_loss: 1.3505 - val_accuracy: 0.5367\n",
      "Epoch 114/300\n",
      "70/70 [==============================] - 0s 725us/step - loss: 1.0547 - accuracy: 0.6171 - val_loss: 1.3465 - val_accuracy: 0.5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.0522 - accuracy: 0.6114 - val_loss: 1.3529 - val_accuracy: 0.5367\n",
      "Epoch 116/300\n",
      "70/70 [==============================] - 0s 732us/step - loss: 1.0495 - accuracy: 0.6114 - val_loss: 1.3507 - val_accuracy: 0.5400\n",
      "Epoch 117/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 1.0462 - accuracy: 0.6186 - val_loss: 1.3499 - val_accuracy: 0.5267\n",
      "Epoch 118/300\n",
      "70/70 [==============================] - 0s 735us/step - loss: 1.0440 - accuracy: 0.6129 - val_loss: 1.3474 - val_accuracy: 0.5367\n",
      "Epoch 119/300\n",
      "70/70 [==============================] - 0s 724us/step - loss: 1.0426 - accuracy: 0.6114 - val_loss: 1.3463 - val_accuracy: 0.5300\n",
      "Epoch 120/300\n",
      "70/70 [==============================] - 0s 777us/step - loss: 1.0388 - accuracy: 0.6157 - val_loss: 1.3549 - val_accuracy: 0.5367\n",
      "Epoch 121/300\n",
      "70/70 [==============================] - 0s 819us/step - loss: 1.0355 - accuracy: 0.6214 - val_loss: 1.3463 - val_accuracy: 0.5433\n",
      "Epoch 122/300\n",
      "70/70 [==============================] - 0s 747us/step - loss: 1.0346 - accuracy: 0.6071 - val_loss: 1.3555 - val_accuracy: 0.5333\n",
      "Epoch 123/300\n",
      "70/70 [==============================] - 0s 741us/step - loss: 1.0316 - accuracy: 0.6257 - val_loss: 1.3602 - val_accuracy: 0.5367\n",
      "Epoch 124/300\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.0293 - accuracy: 0.6171 - val_loss: 1.3482 - val_accuracy: 0.5467\n",
      "Epoch 125/300\n",
      "70/70 [==============================] - 0s 742us/step - loss: 1.0271 - accuracy: 0.6186 - val_loss: 1.3497 - val_accuracy: 0.5567\n",
      "Epoch 126/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.0254 - accuracy: 0.6171 - val_loss: 1.3710 - val_accuracy: 0.5233\n",
      "Epoch 127/300\n",
      "70/70 [==============================] - 0s 724us/step - loss: 1.0235 - accuracy: 0.6129 - val_loss: 1.3611 - val_accuracy: 0.5233\n",
      "Epoch 128/300\n",
      "70/70 [==============================] - 0s 721us/step - loss: 1.0195 - accuracy: 0.6200 - val_loss: 1.3585 - val_accuracy: 0.5367\n",
      "Epoch 129/300\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.0171 - accuracy: 0.6257 - val_loss: 1.3720 - val_accuracy: 0.5300\n",
      "Epoch 130/300\n",
      "70/70 [==============================] - 0s 742us/step - loss: 1.0153 - accuracy: 0.6229 - val_loss: 1.3506 - val_accuracy: 0.5467\n",
      "Epoch 131/300\n",
      "70/70 [==============================] - 0s 735us/step - loss: 1.0137 - accuracy: 0.6214 - val_loss: 1.3611 - val_accuracy: 0.5267\n",
      "Epoch 132/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 1.0113 - accuracy: 0.6186 - val_loss: 1.3512 - val_accuracy: 0.5367\n",
      "Epoch 133/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 1.0111 - accuracy: 0.6229 - val_loss: 1.3595 - val_accuracy: 0.5267\n",
      "Epoch 134/300\n",
      "70/70 [==============================] - 0s 723us/step - loss: 1.0088 - accuracy: 0.6200 - val_loss: 1.3629 - val_accuracy: 0.5267\n",
      "Epoch 135/300\n",
      "70/70 [==============================] - 0s 736us/step - loss: 1.0063 - accuracy: 0.6143 - val_loss: 1.3511 - val_accuracy: 0.5433\n",
      "Epoch 136/300\n",
      "70/70 [==============================] - 0s 698us/step - loss: 1.0039 - accuracy: 0.6186 - val_loss: 1.3629 - val_accuracy: 0.5300\n",
      "Epoch 137/300\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.0024 - accuracy: 0.6229 - val_loss: 1.3599 - val_accuracy: 0.5300\n",
      "Epoch 138/300\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.0006 - accuracy: 0.6200 - val_loss: 1.3677 - val_accuracy: 0.5367\n",
      "Epoch 139/300\n",
      "70/70 [==============================] - 0s 718us/step - loss: 0.9971 - accuracy: 0.6243 - val_loss: 1.3872 - val_accuracy: 0.5267\n",
      "Epoch 140/300\n",
      "70/70 [==============================] - 0s 725us/step - loss: 0.9959 - accuracy: 0.6200 - val_loss: 1.3707 - val_accuracy: 0.5233\n",
      "Epoch 141/300\n",
      "70/70 [==============================] - 0s 714us/step - loss: 0.9926 - accuracy: 0.6314 - val_loss: 1.3868 - val_accuracy: 0.5267\n",
      "Epoch 142/300\n",
      "70/70 [==============================] - 0s 774us/step - loss: 0.9883 - accuracy: 0.6271 - val_loss: 1.3636 - val_accuracy: 0.5233\n",
      "Epoch 143/300\n",
      "70/70 [==============================] - 0s 741us/step - loss: 0.9896 - accuracy: 0.6271 - val_loss: 1.3654 - val_accuracy: 0.5367\n",
      "Epoch 144/300\n",
      "70/70 [==============================] - 0s 721us/step - loss: 0.9864 - accuracy: 0.6243 - val_loss: 1.3790 - val_accuracy: 0.5267\n",
      "Epoch 145/300\n",
      "70/70 [==============================] - 0s 716us/step - loss: 0.9885 - accuracy: 0.6243 - val_loss: 1.3678 - val_accuracy: 0.5300\n",
      "Epoch 146/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 0.9838 - accuracy: 0.6343 - val_loss: 1.3592 - val_accuracy: 0.5400\n",
      "Epoch 147/300\n",
      "70/70 [==============================] - 0s 721us/step - loss: 0.9844 - accuracy: 0.6257 - val_loss: 1.3621 - val_accuracy: 0.5433\n",
      "Epoch 148/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 0.9811 - accuracy: 0.6200 - val_loss: 1.3618 - val_accuracy: 0.5433\n",
      "Epoch 149/300\n",
      "70/70 [==============================] - 0s 735us/step - loss: 0.9788 - accuracy: 0.6286 - val_loss: 1.3716 - val_accuracy: 0.5333\n",
      "Epoch 150/300\n",
      "70/70 [==============================] - 0s 751us/step - loss: 0.9770 - accuracy: 0.6329 - val_loss: 1.3758 - val_accuracy: 0.5367\n",
      "Epoch 151/300\n",
      "70/70 [==============================] - 0s 759us/step - loss: 0.9747 - accuracy: 0.6243 - val_loss: 1.3758 - val_accuracy: 0.5300\n",
      "Epoch 152/300\n",
      "70/70 [==============================] - 0s 724us/step - loss: 0.9723 - accuracy: 0.6286 - val_loss: 1.3657 - val_accuracy: 0.5233\n",
      "Epoch 153/300\n",
      "70/70 [==============================] - 0s 733us/step - loss: 0.9721 - accuracy: 0.6314 - val_loss: 1.3683 - val_accuracy: 0.5300\n",
      "Epoch 154/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 0.9691 - accuracy: 0.6314 - val_loss: 1.3775 - val_accuracy: 0.5333\n",
      "Epoch 155/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 0.9676 - accuracy: 0.6329 - val_loss: 1.3733 - val_accuracy: 0.5400\n",
      "Epoch 156/300\n",
      "70/70 [==============================] - 0s 746us/step - loss: 0.9663 - accuracy: 0.6329 - val_loss: 1.3744 - val_accuracy: 0.5333\n",
      "Epoch 157/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 0.9637 - accuracy: 0.6429 - val_loss: 1.3751 - val_accuracy: 0.5267\n",
      "Epoch 158/300\n",
      "70/70 [==============================] - 0s 736us/step - loss: 0.9637 - accuracy: 0.6300 - val_loss: 1.3724 - val_accuracy: 0.5333\n",
      "Epoch 159/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 0.9603 - accuracy: 0.6343 - val_loss: 1.3745 - val_accuracy: 0.5433\n",
      "Epoch 160/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 0.9587 - accuracy: 0.6371 - val_loss: 1.3708 - val_accuracy: 0.5500\n",
      "Epoch 161/300\n",
      "70/70 [==============================] - 0s 738us/step - loss: 0.9586 - accuracy: 0.6329 - val_loss: 1.3818 - val_accuracy: 0.5167\n",
      "Epoch 162/300\n",
      "70/70 [==============================] - 0s 738us/step - loss: 0.9547 - accuracy: 0.6371 - val_loss: 1.3747 - val_accuracy: 0.5500\n",
      "Epoch 163/300\n",
      "70/70 [==============================] - 0s 724us/step - loss: 0.9544 - accuracy: 0.6371 - val_loss: 1.3823 - val_accuracy: 0.5333\n",
      "Epoch 164/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 0.9527 - accuracy: 0.6457 - val_loss: 1.3851 - val_accuracy: 0.5467\n",
      "Epoch 165/300\n",
      "70/70 [==============================] - 0s 707us/step - loss: 0.9486 - accuracy: 0.6471 - val_loss: 1.3883 - val_accuracy: 0.5300\n",
      "Epoch 166/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 0.9501 - accuracy: 0.6457 - val_loss: 1.3915 - val_accuracy: 0.5400\n",
      "Epoch 167/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 0.9470 - accuracy: 0.6357 - val_loss: 1.3779 - val_accuracy: 0.5533\n",
      "Epoch 168/300\n",
      "70/70 [==============================] - 0s 741us/step - loss: 0.9444 - accuracy: 0.6400 - val_loss: 1.3998 - val_accuracy: 0.5267\n",
      "Epoch 169/300\n",
      "70/70 [==============================] - 0s 761us/step - loss: 0.9445 - accuracy: 0.6443 - val_loss: 1.3958 - val_accuracy: 0.5300\n",
      "Epoch 170/300\n",
      "70/70 [==============================] - 0s 730us/step - loss: 0.9435 - accuracy: 0.6386 - val_loss: 1.3829 - val_accuracy: 0.5467\n",
      "Epoch 171/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 725us/step - loss: 0.9416 - accuracy: 0.6443 - val_loss: 1.3826 - val_accuracy: 0.5400\n",
      "Epoch 172/300\n",
      "70/70 [==============================] - 0s 714us/step - loss: 0.9392 - accuracy: 0.6471 - val_loss: 1.3920 - val_accuracy: 0.5433\n",
      "Epoch 173/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.9368 - accuracy: 0.6486 - val_loss: 1.3825 - val_accuracy: 0.5333\n",
      "Epoch 174/300\n",
      "70/70 [==============================] - 0s 735us/step - loss: 0.9373 - accuracy: 0.6514 - val_loss: 1.3865 - val_accuracy: 0.5333\n",
      "Epoch 175/300\n",
      "70/70 [==============================] - 0s 747us/step - loss: 0.9327 - accuracy: 0.6514 - val_loss: 1.3986 - val_accuracy: 0.5400\n",
      "Epoch 176/300\n",
      "70/70 [==============================] - 0s 733us/step - loss: 0.9330 - accuracy: 0.6443 - val_loss: 1.3852 - val_accuracy: 0.5333\n",
      "Epoch 177/300\n",
      "70/70 [==============================] - 0s 746us/step - loss: 0.9320 - accuracy: 0.6457 - val_loss: 1.3979 - val_accuracy: 0.5500\n",
      "Epoch 178/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 0.9304 - accuracy: 0.6429 - val_loss: 1.3953 - val_accuracy: 0.5400\n",
      "Epoch 179/300\n",
      "70/70 [==============================] - 0s 724us/step - loss: 0.9292 - accuracy: 0.6443 - val_loss: 1.3909 - val_accuracy: 0.5100\n",
      "Epoch 180/300\n",
      "70/70 [==============================] - 0s 763us/step - loss: 0.9275 - accuracy: 0.6443 - val_loss: 1.3998 - val_accuracy: 0.5433\n",
      "Epoch 181/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.9274 - accuracy: 0.6529 - val_loss: 1.3861 - val_accuracy: 0.5467\n",
      "Epoch 182/300\n",
      "70/70 [==============================] - 0s 772us/step - loss: 0.9251 - accuracy: 0.6486 - val_loss: 1.3993 - val_accuracy: 0.5367\n",
      "Epoch 183/300\n",
      "70/70 [==============================] - 0s 738us/step - loss: 0.9210 - accuracy: 0.6557 - val_loss: 1.3917 - val_accuracy: 0.5433\n",
      "Epoch 184/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 0.9211 - accuracy: 0.6486 - val_loss: 1.3960 - val_accuracy: 0.5533\n",
      "Epoch 185/300\n",
      "70/70 [==============================] - 0s 756us/step - loss: 0.9185 - accuracy: 0.6500 - val_loss: 1.4141 - val_accuracy: 0.5400\n",
      "Epoch 186/300\n",
      "70/70 [==============================] - 0s 733us/step - loss: 0.9174 - accuracy: 0.6471 - val_loss: 1.3966 - val_accuracy: 0.5367\n",
      "Epoch 187/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 0.9177 - accuracy: 0.6500 - val_loss: 1.3961 - val_accuracy: 0.5400\n",
      "Epoch 188/300\n",
      "70/70 [==============================] - 0s 734us/step - loss: 0.9159 - accuracy: 0.6600 - val_loss: 1.4054 - val_accuracy: 0.5467\n",
      "Epoch 189/300\n",
      "70/70 [==============================] - 0s 743us/step - loss: 0.9150 - accuracy: 0.6500 - val_loss: 1.4011 - val_accuracy: 0.5500\n",
      "Epoch 190/300\n",
      "70/70 [==============================] - 0s 749us/step - loss: 0.9113 - accuracy: 0.6586 - val_loss: 1.4033 - val_accuracy: 0.5300\n",
      "Epoch 191/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 0.9132 - accuracy: 0.6571 - val_loss: 1.4090 - val_accuracy: 0.5400\n",
      "Epoch 192/300\n",
      "70/70 [==============================] - 0s 744us/step - loss: 0.9092 - accuracy: 0.6586 - val_loss: 1.4006 - val_accuracy: 0.5367\n",
      "Epoch 193/300\n",
      "70/70 [==============================] - 0s 739us/step - loss: 0.9086 - accuracy: 0.6529 - val_loss: 1.4064 - val_accuracy: 0.5333\n",
      "Epoch 194/300\n",
      "70/70 [==============================] - 0s 748us/step - loss: 0.9063 - accuracy: 0.6586 - val_loss: 1.4098 - val_accuracy: 0.5367\n",
      "Epoch 195/300\n",
      "70/70 [==============================] - 0s 746us/step - loss: 0.9045 - accuracy: 0.6600 - val_loss: 1.4231 - val_accuracy: 0.5467\n",
      "Epoch 196/300\n",
      "70/70 [==============================] - 0s 739us/step - loss: 0.9046 - accuracy: 0.6586 - val_loss: 1.4083 - val_accuracy: 0.5467\n",
      "Epoch 197/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.9034 - accuracy: 0.6600 - val_loss: 1.4132 - val_accuracy: 0.5200\n",
      "Epoch 198/300\n",
      "70/70 [==============================] - 0s 736us/step - loss: 0.9019 - accuracy: 0.6586 - val_loss: 1.4124 - val_accuracy: 0.5367\n",
      "Epoch 199/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.8993 - accuracy: 0.6657 - val_loss: 1.4118 - val_accuracy: 0.5333\n",
      "Epoch 200/300\n",
      "70/70 [==============================] - 0s 746us/step - loss: 0.8992 - accuracy: 0.6571 - val_loss: 1.4131 - val_accuracy: 0.5300\n",
      "Epoch 201/300\n",
      "70/70 [==============================] - 0s 738us/step - loss: 0.8952 - accuracy: 0.6671 - val_loss: 1.4152 - val_accuracy: 0.5333\n",
      "Epoch 202/300\n",
      "70/70 [==============================] - 0s 739us/step - loss: 0.8946 - accuracy: 0.6671 - val_loss: 1.4159 - val_accuracy: 0.5433\n",
      "Epoch 203/300\n",
      "70/70 [==============================] - 0s 741us/step - loss: 0.8937 - accuracy: 0.6557 - val_loss: 1.4172 - val_accuracy: 0.5267\n",
      "Epoch 204/300\n",
      "70/70 [==============================] - 0s 738us/step - loss: 0.8917 - accuracy: 0.6629 - val_loss: 1.4178 - val_accuracy: 0.5433\n",
      "Epoch 205/300\n",
      "70/70 [==============================] - 0s 752us/step - loss: 0.8915 - accuracy: 0.6700 - val_loss: 1.4272 - val_accuracy: 0.5300\n",
      "Epoch 206/300\n",
      "70/70 [==============================] - 0s 742us/step - loss: 0.8914 - accuracy: 0.6643 - val_loss: 1.4098 - val_accuracy: 0.5333\n",
      "Epoch 207/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 0.8890 - accuracy: 0.6657 - val_loss: 1.4151 - val_accuracy: 0.5400\n",
      "Epoch 208/300\n",
      "70/70 [==============================] - 0s 751us/step - loss: 0.8883 - accuracy: 0.6614 - val_loss: 1.4260 - val_accuracy: 0.5400\n",
      "Epoch 209/300\n",
      "70/70 [==============================] - 0s 725us/step - loss: 0.8855 - accuracy: 0.6586 - val_loss: 1.4161 - val_accuracy: 0.5267\n",
      "Epoch 210/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 0.8859 - accuracy: 0.6643 - val_loss: 1.4218 - val_accuracy: 0.5400\n",
      "Epoch 211/300\n",
      "70/70 [==============================] - 0s 751us/step - loss: 0.8825 - accuracy: 0.6786 - val_loss: 1.4231 - val_accuracy: 0.5400\n",
      "Epoch 212/300\n",
      "70/70 [==============================] - 0s 752us/step - loss: 0.8827 - accuracy: 0.6714 - val_loss: 1.4217 - val_accuracy: 0.5333\n",
      "Epoch 213/300\n",
      "70/70 [==============================] - 0s 725us/step - loss: 0.8822 - accuracy: 0.6729 - val_loss: 1.4336 - val_accuracy: 0.5467\n",
      "Epoch 214/300\n",
      "70/70 [==============================] - 0s 731us/step - loss: 0.8805 - accuracy: 0.6714 - val_loss: 1.4328 - val_accuracy: 0.5333\n",
      "Epoch 215/300\n",
      "70/70 [==============================] - 0s 715us/step - loss: 0.8792 - accuracy: 0.6629 - val_loss: 1.4261 - val_accuracy: 0.5367\n",
      "Epoch 216/300\n",
      "70/70 [==============================] - 0s 761us/step - loss: 0.8776 - accuracy: 0.6743 - val_loss: 1.4381 - val_accuracy: 0.5300\n",
      "Epoch 217/300\n",
      "70/70 [==============================] - 0s 741us/step - loss: 0.8778 - accuracy: 0.6714 - val_loss: 1.4336 - val_accuracy: 0.5300\n",
      "Epoch 218/300\n",
      "70/70 [==============================] - 0s 710us/step - loss: 0.8752 - accuracy: 0.6743 - val_loss: 1.4333 - val_accuracy: 0.5400\n",
      "Epoch 219/300\n",
      "70/70 [==============================] - 0s 759us/step - loss: 0.8740 - accuracy: 0.6771 - val_loss: 1.4501 - val_accuracy: 0.5367\n",
      "Epoch 220/300\n",
      "70/70 [==============================] - 0s 731us/step - loss: 0.8743 - accuracy: 0.6643 - val_loss: 1.4333 - val_accuracy: 0.5333\n",
      "Epoch 221/300\n",
      "70/70 [==============================] - 0s 748us/step - loss: 0.8713 - accuracy: 0.6729 - val_loss: 1.4425 - val_accuracy: 0.5333\n",
      "Epoch 222/300\n",
      "70/70 [==============================] - 0s 735us/step - loss: 0.8726 - accuracy: 0.6757 - val_loss: 1.4359 - val_accuracy: 0.5400\n",
      "Epoch 223/300\n",
      "70/70 [==============================] - 0s 740us/step - loss: 0.8719 - accuracy: 0.6686 - val_loss: 1.4373 - val_accuracy: 0.5367\n",
      "Epoch 224/300\n",
      "70/70 [==============================] - 0s 736us/step - loss: 0.8681 - accuracy: 0.6829 - val_loss: 1.4372 - val_accuracy: 0.5367\n",
      "Epoch 225/300\n",
      "70/70 [==============================] - 0s 731us/step - loss: 0.8672 - accuracy: 0.6757 - val_loss: 1.4363 - val_accuracy: 0.5367\n",
      "Epoch 226/300\n",
      "70/70 [==============================] - 0s 730us/step - loss: 0.8653 - accuracy: 0.6757 - val_loss: 1.4442 - val_accuracy: 0.5367\n",
      "Epoch 227/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 767us/step - loss: 0.8647 - accuracy: 0.6729 - val_loss: 1.4421 - val_accuracy: 0.5333\n",
      "Epoch 228/300\n",
      "70/70 [==============================] - 0s 747us/step - loss: 0.8631 - accuracy: 0.6786 - val_loss: 1.4469 - val_accuracy: 0.5367\n",
      "Epoch 229/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.8620 - accuracy: 0.6843 - val_loss: 1.4548 - val_accuracy: 0.5400\n",
      "Epoch 230/300\n",
      "70/70 [==============================] - 0s 732us/step - loss: 0.8597 - accuracy: 0.6814 - val_loss: 1.4477 - val_accuracy: 0.5367\n",
      "Epoch 231/300\n",
      "70/70 [==============================] - 0s 731us/step - loss: 0.8598 - accuracy: 0.6829 - val_loss: 1.4483 - val_accuracy: 0.5400\n",
      "Epoch 232/300\n",
      "70/70 [==============================] - 0s 718us/step - loss: 0.8574 - accuracy: 0.6757 - val_loss: 1.4466 - val_accuracy: 0.5300\n",
      "Epoch 233/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 0.8585 - accuracy: 0.6800 - val_loss: 1.4424 - val_accuracy: 0.5333\n",
      "Epoch 234/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 0.8563 - accuracy: 0.6814 - val_loss: 1.4480 - val_accuracy: 0.5433\n",
      "Epoch 235/300\n",
      "70/70 [==============================] - 0s 761us/step - loss: 0.8562 - accuracy: 0.6814 - val_loss: 1.4481 - val_accuracy: 0.5400\n",
      "Epoch 236/300\n",
      "70/70 [==============================] - 0s 715us/step - loss: 0.8561 - accuracy: 0.6843 - val_loss: 1.4577 - val_accuracy: 0.5367\n",
      "Epoch 237/300\n",
      "70/70 [==============================] - 0s 779us/step - loss: 0.8553 - accuracy: 0.6700 - val_loss: 1.4560 - val_accuracy: 0.5367\n",
      "Epoch 238/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.8519 - accuracy: 0.6771 - val_loss: 1.4572 - val_accuracy: 0.5333\n",
      "Epoch 239/300\n",
      "70/70 [==============================] - 0s 719us/step - loss: 0.8514 - accuracy: 0.6914 - val_loss: 1.4593 - val_accuracy: 0.5400\n",
      "Epoch 240/300\n",
      "70/70 [==============================] - 0s 722us/step - loss: 0.8470 - accuracy: 0.6929 - val_loss: 1.4556 - val_accuracy: 0.5467\n",
      "Epoch 241/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 0.8464 - accuracy: 0.6829 - val_loss: 1.4680 - val_accuracy: 0.5400\n",
      "Epoch 242/300\n",
      "70/70 [==============================] - 0s 754us/step - loss: 0.8481 - accuracy: 0.6843 - val_loss: 1.4640 - val_accuracy: 0.5333\n",
      "Epoch 243/300\n",
      "70/70 [==============================] - 0s 755us/step - loss: 0.8465 - accuracy: 0.6786 - val_loss: 1.4678 - val_accuracy: 0.5367\n",
      "Epoch 244/300\n",
      "70/70 [==============================] - 0s 763us/step - loss: 0.8456 - accuracy: 0.6829 - val_loss: 1.4638 - val_accuracy: 0.5367\n",
      "Epoch 245/300\n",
      "70/70 [==============================] - 0s 750us/step - loss: 0.8440 - accuracy: 0.6914 - val_loss: 1.4662 - val_accuracy: 0.5500\n",
      "Epoch 246/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.8438 - accuracy: 0.6871 - val_loss: 1.4611 - val_accuracy: 0.5367\n",
      "Epoch 247/300\n",
      "70/70 [==============================] - 0s 745us/step - loss: 0.8417 - accuracy: 0.6886 - val_loss: 1.4688 - val_accuracy: 0.5300\n",
      "Epoch 248/300\n",
      "70/70 [==============================] - 0s 767us/step - loss: 0.8418 - accuracy: 0.6900 - val_loss: 1.4677 - val_accuracy: 0.5400\n",
      "Epoch 249/300\n",
      "70/70 [==============================] - 0s 753us/step - loss: 0.8407 - accuracy: 0.6971 - val_loss: 1.4693 - val_accuracy: 0.5333\n",
      "Epoch 250/300\n",
      "70/70 [==============================] - 0s 722us/step - loss: 0.8385 - accuracy: 0.6886 - val_loss: 1.4667 - val_accuracy: 0.5367\n",
      "Epoch 251/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 0.8389 - accuracy: 0.6900 - val_loss: 1.4671 - val_accuracy: 0.5333\n",
      "Epoch 252/300\n",
      "70/70 [==============================] - 0s 772us/step - loss: 0.8367 - accuracy: 0.6843 - val_loss: 1.4721 - val_accuracy: 0.5400\n",
      "Epoch 253/300\n",
      "70/70 [==============================] - 0s 735us/step - loss: 0.8365 - accuracy: 0.6871 - val_loss: 1.4725 - val_accuracy: 0.5333\n",
      "Epoch 254/300\n",
      "70/70 [==============================] - 0s 754us/step - loss: 0.8337 - accuracy: 0.6900 - val_loss: 1.4840 - val_accuracy: 0.5433\n",
      "Epoch 255/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 0.8329 - accuracy: 0.6929 - val_loss: 1.4718 - val_accuracy: 0.5333\n",
      "Epoch 256/300\n",
      "70/70 [==============================] - 0s 716us/step - loss: 0.8322 - accuracy: 0.7000 - val_loss: 1.4806 - val_accuracy: 0.5367\n",
      "Epoch 257/300\n",
      "70/70 [==============================] - 0s 726us/step - loss: 0.8327 - accuracy: 0.6986 - val_loss: 1.4868 - val_accuracy: 0.5300\n",
      "Epoch 258/300\n",
      "70/70 [==============================] - 0s 739us/step - loss: 0.8320 - accuracy: 0.6929 - val_loss: 1.4829 - val_accuracy: 0.5333\n",
      "Epoch 259/300\n",
      "70/70 [==============================] - 0s 725us/step - loss: 0.8309 - accuracy: 0.6943 - val_loss: 1.4830 - val_accuracy: 0.5433\n",
      "Epoch 260/300\n",
      "70/70 [==============================] - 0s 756us/step - loss: 0.8282 - accuracy: 0.6871 - val_loss: 1.4854 - val_accuracy: 0.5300\n",
      "Epoch 261/300\n",
      "70/70 [==============================] - 0s 733us/step - loss: 0.8283 - accuracy: 0.6929 - val_loss: 1.4841 - val_accuracy: 0.5300\n",
      "Epoch 262/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 0.8265 - accuracy: 0.6957 - val_loss: 1.4843 - val_accuracy: 0.5367\n",
      "Epoch 263/300\n",
      "70/70 [==============================] - 0s 743us/step - loss: 0.8244 - accuracy: 0.6914 - val_loss: 1.4816 - val_accuracy: 0.5367\n",
      "Epoch 264/300\n",
      "70/70 [==============================] - 0s 718us/step - loss: 0.8244 - accuracy: 0.6900 - val_loss: 1.4835 - val_accuracy: 0.5333\n",
      "Epoch 265/300\n",
      "70/70 [==============================] - 0s 710us/step - loss: 0.8233 - accuracy: 0.6929 - val_loss: 1.4923 - val_accuracy: 0.5433\n",
      "Epoch 266/300\n",
      "70/70 [==============================] - 0s 786us/step - loss: 0.8213 - accuracy: 0.6971 - val_loss: 1.4943 - val_accuracy: 0.5367\n",
      "Epoch 267/300\n",
      "70/70 [==============================] - 0s 719us/step - loss: 0.8225 - accuracy: 0.6929 - val_loss: 1.4880 - val_accuracy: 0.5267\n",
      "Epoch 268/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 0.8196 - accuracy: 0.6986 - val_loss: 1.4909 - val_accuracy: 0.5400\n",
      "Epoch 269/300\n",
      "70/70 [==============================] - 0s 742us/step - loss: 0.8198 - accuracy: 0.6986 - val_loss: 1.4950 - val_accuracy: 0.5333\n",
      "Epoch 270/300\n",
      "70/70 [==============================] - 0s 720us/step - loss: 0.8201 - accuracy: 0.6943 - val_loss: 1.4934 - val_accuracy: 0.5300\n",
      "Epoch 271/300\n",
      "70/70 [==============================] - 0s 719us/step - loss: 0.8173 - accuracy: 0.6971 - val_loss: 1.4913 - val_accuracy: 0.5300\n",
      "Epoch 272/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 0.8182 - accuracy: 0.6871 - val_loss: 1.4965 - val_accuracy: 0.5467\n",
      "Epoch 273/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 0.8155 - accuracy: 0.7000 - val_loss: 1.4979 - val_accuracy: 0.5333\n",
      "Epoch 274/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 0.8153 - accuracy: 0.6929 - val_loss: 1.5039 - val_accuracy: 0.5433\n",
      "Epoch 275/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 0.8129 - accuracy: 0.6929 - val_loss: 1.5083 - val_accuracy: 0.5500\n",
      "Epoch 276/300\n",
      "70/70 [==============================] - 0s 724us/step - loss: 0.8131 - accuracy: 0.7029 - val_loss: 1.5024 - val_accuracy: 0.5467\n",
      "Epoch 277/300\n",
      "70/70 [==============================] - 0s 722us/step - loss: 0.8122 - accuracy: 0.6943 - val_loss: 1.5035 - val_accuracy: 0.5333\n",
      "Epoch 278/300\n",
      "70/70 [==============================] - 0s 761us/step - loss: 0.8130 - accuracy: 0.6957 - val_loss: 1.4994 - val_accuracy: 0.5300\n",
      "Epoch 279/300\n",
      "70/70 [==============================] - 0s 717us/step - loss: 0.8094 - accuracy: 0.7043 - val_loss: 1.5074 - val_accuracy: 0.5200\n",
      "Epoch 280/300\n",
      "70/70 [==============================] - 0s 763us/step - loss: 0.8095 - accuracy: 0.7057 - val_loss: 1.5063 - val_accuracy: 0.5367\n",
      "Epoch 281/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 0.8075 - accuracy: 0.6943 - val_loss: 1.5136 - val_accuracy: 0.5400\n",
      "Epoch 282/300\n",
      "70/70 [==============================] - 0s 725us/step - loss: 0.8072 - accuracy: 0.6986 - val_loss: 1.5075 - val_accuracy: 0.5300\n",
      "Epoch 283/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 720us/step - loss: 0.8070 - accuracy: 0.7014 - val_loss: 1.5170 - val_accuracy: 0.5467\n",
      "Epoch 284/300\n",
      "70/70 [==============================] - 0s 729us/step - loss: 0.8044 - accuracy: 0.7071 - val_loss: 1.5226 - val_accuracy: 0.5500\n",
      "Epoch 285/300\n",
      "70/70 [==============================] - 0s 782us/step - loss: 0.8047 - accuracy: 0.6986 - val_loss: 1.5155 - val_accuracy: 0.5267\n",
      "Epoch 286/300\n",
      "70/70 [==============================] - 0s 769us/step - loss: 0.8050 - accuracy: 0.7057 - val_loss: 1.5120 - val_accuracy: 0.5400\n",
      "Epoch 287/300\n",
      "70/70 [==============================] - 0s 878us/step - loss: 0.8041 - accuracy: 0.7129 - val_loss: 1.5158 - val_accuracy: 0.5400\n",
      "Epoch 288/300\n",
      "70/70 [==============================] - 0s 779us/step - loss: 0.8008 - accuracy: 0.7029 - val_loss: 1.5286 - val_accuracy: 0.5300\n",
      "Epoch 289/300\n",
      "70/70 [==============================] - 0s 785us/step - loss: 0.8021 - accuracy: 0.7043 - val_loss: 1.5180 - val_accuracy: 0.5433\n",
      "Epoch 290/300\n",
      "70/70 [==============================] - 0s 727us/step - loss: 0.7973 - accuracy: 0.7100 - val_loss: 1.5191 - val_accuracy: 0.5400\n",
      "Epoch 291/300\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.7995 - accuracy: 0.7029 - val_loss: 1.5224 - val_accuracy: 0.5300\n",
      "Epoch 292/300\n",
      "70/70 [==============================] - 0s 744us/step - loss: 0.7981 - accuracy: 0.6971 - val_loss: 1.5232 - val_accuracy: 0.5233\n",
      "Epoch 293/300\n",
      "70/70 [==============================] - 0s 732us/step - loss: 0.7983 - accuracy: 0.7014 - val_loss: 1.5236 - val_accuracy: 0.5333\n",
      "Epoch 294/300\n",
      "70/70 [==============================] - 0s 733us/step - loss: 0.7947 - accuracy: 0.7000 - val_loss: 1.5285 - val_accuracy: 0.5367\n",
      "Epoch 295/300\n",
      "70/70 [==============================] - 0s 743us/step - loss: 0.7938 - accuracy: 0.7114 - val_loss: 1.5318 - val_accuracy: 0.5467\n",
      "Epoch 296/300\n",
      "70/70 [==============================] - 0s 736us/step - loss: 0.7945 - accuracy: 0.7057 - val_loss: 1.5286 - val_accuracy: 0.5367\n",
      "Epoch 297/300\n",
      "70/70 [==============================] - 0s 723us/step - loss: 0.7920 - accuracy: 0.7129 - val_loss: 1.5303 - val_accuracy: 0.5367\n",
      "Epoch 298/300\n",
      "70/70 [==============================] - 0s 737us/step - loss: 0.7927 - accuracy: 0.7029 - val_loss: 1.5321 - val_accuracy: 0.5467\n",
      "Epoch 299/300\n",
      "70/70 [==============================] - 0s 731us/step - loss: 0.7919 - accuracy: 0.7086 - val_loss: 1.5367 - val_accuracy: 0.5467\n",
      "Epoch 300/300\n",
      "70/70 [==============================] - 0s 719us/step - loss: 0.7887 - accuracy: 0.7071 - val_loss: 1.5340 - val_accuracy: 0.5467\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(X_train, Y_train, epochs=300, batch_size=10, validation_data=(X_val, Y_val))\n",
    "#validation loss가 올라가기 시작하면 과적합에 빠진것이다. \n",
    "#과적합에 빠지기전에 중단하는게 좋다.\n",
    "# 70/70 [==============================] \n",
    "# - 0s 825us/step - loss: 0.2678 - accuracy: 0.9314 \n",
    "# - val_loss: 5.2011 - val_accuracy: 0.4567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "#한 150 에폭 정도에 있는 과적합은 감내 하겠다..\n",
    "# 과적합은 50즈음에 일어남."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 687us/step - loss: 1.5120 - accuracy: 0.5075\n",
      "\n",
      "loss : 1.5120080709457397\n",
      "accuracy : 0.5074999928474426\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "print('')\n",
    "print('loss : ' + str(loss_and_metrics[0]))\n",
    "print('accuracy : ' + str(loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 조기종료를 시키는 early stopping 코드가 있다.\n",
    "- 종료를 하는 환경이 되면 알아서 종료\n",
    "- 사용자가 설정한 환경이 되었을때, 시스템에 의해 자동으로 호출되는 함수 => 콜백함수\n",
    "    - monitor: 관찰항목(val loss, val acc)\n",
    "    - min_delta: 개선되고 있다고 판단하기 위한 최소 변화량\n",
    "        - 변화량이 mindelta보다 작으면 개선이 안된것으로 판단\n",
    "    - patience: 개선이 없다고 해서 바로 종료되는게 아니라..\n",
    "        - 몇 epochs동안 참아줄 수 있나? (ex. 10이면, 개선이 없는 에폭 10번을 참겠다)\n",
    "    - mode: 개선이 없다고 판단하기 위한 기준\n",
    "        - ex)관찰항목: val loss인 경우, 감소되는 것이 멈출때 트레이닝을 종료하므로, \n",
    "            이런경우에는 mode를 min으로 설정\n",
    "        - acc 는 mode를 max로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es=EarlyStopping() #default val-loss가 안하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본설정\n",
    "# tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor=\"val_loss\",\n",
    "#     min_delta=0,\n",
    "#     patience=0,\n",
    "#     verbose=0,\n",
    "#     mode=\"auto\",\n",
    "#     baseline=None,\n",
    "#     restore_best_weights=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=2 , input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax')) #0~9까지의 숫자니까 unit=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 2.2897 - accuracy: 0.1029 - val_loss: 2.2874 - val_accuracy: 0.0800\n",
      "Epoch 2/500\n",
      "70/70 [==============================] - 0s 766us/step - loss: 2.2638 - accuracy: 0.1229 - val_loss: 2.2709 - val_accuracy: 0.1133\n",
      "Epoch 3/500\n",
      "70/70 [==============================] - 0s 732us/step - loss: 2.2418 - accuracy: 0.1343 - val_loss: 2.2625 - val_accuracy: 0.1133\n",
      "Epoch 4/500\n",
      "70/70 [==============================] - 0s 767us/step - loss: 2.2237 - accuracy: 0.1329 - val_loss: 2.2432 - val_accuracy: 0.1233\n",
      "Epoch 5/500\n",
      "70/70 [==============================] - 0s 737us/step - loss: 2.2052 - accuracy: 0.1757 - val_loss: 2.2249 - val_accuracy: 0.1467\n",
      "Epoch 6/500\n",
      "70/70 [==============================] - 0s 768us/step - loss: 2.1861 - accuracy: 0.1843 - val_loss: 2.2100 - val_accuracy: 0.1567\n",
      "Epoch 7/500\n",
      "70/70 [==============================] - 0s 759us/step - loss: 2.1601 - accuracy: 0.2043 - val_loss: 2.1836 - val_accuracy: 0.1767\n",
      "Epoch 8/500\n",
      "70/70 [==============================] - 0s 730us/step - loss: 2.1271 - accuracy: 0.2043 - val_loss: 2.1497 - val_accuracy: 0.2233\n",
      "Epoch 9/500\n",
      "70/70 [==============================] - 0s 737us/step - loss: 2.0846 - accuracy: 0.2271 - val_loss: 2.1057 - val_accuracy: 0.2167\n",
      "Epoch 10/500\n",
      "70/70 [==============================] - 0s 722us/step - loss: 2.0343 - accuracy: 0.2586 - val_loss: 2.0576 - val_accuracy: 0.2767\n",
      "Epoch 11/500\n",
      "70/70 [==============================] - 0s 756us/step - loss: 1.9829 - accuracy: 0.3057 - val_loss: 2.0179 - val_accuracy: 0.2933\n",
      "Epoch 12/500\n",
      "70/70 [==============================] - 0s 775us/step - loss: 1.9345 - accuracy: 0.3114 - val_loss: 1.9668 - val_accuracy: 0.2933\n",
      "Epoch 13/500\n",
      "70/70 [==============================] - 0s 768us/step - loss: 1.8887 - accuracy: 0.3243 - val_loss: 1.9327 - val_accuracy: 0.3000\n",
      "Epoch 14/500\n",
      "70/70 [==============================] - 0s 770us/step - loss: 1.8478 - accuracy: 0.3357 - val_loss: 1.8861 - val_accuracy: 0.3033\n",
      "Epoch 15/500\n",
      "70/70 [==============================] - 0s 784us/step - loss: 1.8118 - accuracy: 0.3257 - val_loss: 1.8548 - val_accuracy: 0.3367\n",
      "Epoch 16/500\n",
      "70/70 [==============================] - 0s 731us/step - loss: 1.7783 - accuracy: 0.3400 - val_loss: 1.8213 - val_accuracy: 0.3600\n",
      "Epoch 17/500\n",
      "70/70 [==============================] - 0s 765us/step - loss: 1.7495 - accuracy: 0.3543 - val_loss: 1.8013 - val_accuracy: 0.3667\n",
      "Epoch 18/500\n",
      "70/70 [==============================] - 0s 718us/step - loss: 1.7228 - accuracy: 0.3571 - val_loss: 1.7793 - val_accuracy: 0.3900\n",
      "Epoch 19/500\n",
      "70/70 [==============================] - 0s 769us/step - loss: 1.6984 - accuracy: 0.3714 - val_loss: 1.7529 - val_accuracy: 0.4000\n",
      "Epoch 20/500\n",
      "70/70 [==============================] - 0s 791us/step - loss: 1.6752 - accuracy: 0.3943 - val_loss: 1.7392 - val_accuracy: 0.3833\n",
      "Epoch 21/500\n",
      "70/70 [==============================] - 0s 748us/step - loss: 1.6538 - accuracy: 0.3757 - val_loss: 1.7146 - val_accuracy: 0.4133\n",
      "Epoch 22/500\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.6331 - accuracy: 0.4129 - val_loss: 1.7017 - val_accuracy: 0.3967\n",
      "Epoch 23/500\n",
      "70/70 [==============================] - 0s 762us/step - loss: 1.6140 - accuracy: 0.3929 - val_loss: 1.6824 - val_accuracy: 0.4133\n",
      "Epoch 24/500\n",
      "70/70 [==============================] - 0s 804us/step - loss: 1.5955 - accuracy: 0.4100 - val_loss: 1.6621 - val_accuracy: 0.4367\n",
      "Epoch 25/500\n",
      "70/70 [==============================] - 0s 782us/step - loss: 1.5785 - accuracy: 0.4271 - val_loss: 1.6511 - val_accuracy: 0.4433\n",
      "Epoch 26/500\n",
      "70/70 [==============================] - 0s 780us/step - loss: 1.5614 - accuracy: 0.4071 - val_loss: 1.6440 - val_accuracy: 0.4500\n",
      "Epoch 27/500\n",
      "70/70 [==============================] - 0s 768us/step - loss: 1.5470 - accuracy: 0.4386 - val_loss: 1.6255 - val_accuracy: 0.4500\n",
      "Epoch 28/500\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.5296 - accuracy: 0.4400 - val_loss: 1.6172 - val_accuracy: 0.4467\n",
      "Epoch 29/500\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.5164 - accuracy: 0.4557 - val_loss: 1.5987 - val_accuracy: 0.4500\n",
      "Epoch 30/500\n",
      "70/70 [==============================] - 0s 721us/step - loss: 1.5021 - accuracy: 0.4457 - val_loss: 1.5823 - val_accuracy: 0.4600\n",
      "Epoch 31/500\n",
      "70/70 [==============================] - 0s 754us/step - loss: 1.4880 - accuracy: 0.4514 - val_loss: 1.5740 - val_accuracy: 0.4667\n",
      "Epoch 32/500\n",
      "70/70 [==============================] - 0s 753us/step - loss: 1.4752 - accuracy: 0.4857 - val_loss: 1.5592 - val_accuracy: 0.4667\n",
      "Epoch 33/500\n",
      "70/70 [==============================] - 0s 784us/step - loss: 1.4634 - accuracy: 0.4671 - val_loss: 1.5663 - val_accuracy: 0.4467\n",
      "Epoch 34/500\n",
      "70/70 [==============================] - 0s 762us/step - loss: 1.4521 - accuracy: 0.4729 - val_loss: 1.5543 - val_accuracy: 0.4467\n",
      "Epoch 35/500\n",
      "70/70 [==============================] - 0s 731us/step - loss: 1.4402 - accuracy: 0.4986 - val_loss: 1.5429 - val_accuracy: 0.4667\n",
      "Epoch 36/500\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.4274 - accuracy: 0.4943 - val_loss: 1.5405 - val_accuracy: 0.4633\n",
      "Epoch 37/500\n",
      "70/70 [==============================] - 0s 764us/step - loss: 1.4185 - accuracy: 0.5100 - val_loss: 1.5255 - val_accuracy: 0.4800\n",
      "Epoch 38/500\n",
      "70/70 [==============================] - 0s 732us/step - loss: 1.4079 - accuracy: 0.5186 - val_loss: 1.5168 - val_accuracy: 0.4533\n",
      "Epoch 39/500\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.3954 - accuracy: 0.5000 - val_loss: 1.5166 - val_accuracy: 0.4433\n",
      "Epoch 40/500\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.3865 - accuracy: 0.5057 - val_loss: 1.5075 - val_accuracy: 0.4733\n",
      "Epoch 41/500\n",
      "70/70 [==============================] - 0s 751us/step - loss: 1.3776 - accuracy: 0.5243 - val_loss: 1.4970 - val_accuracy: 0.4767\n",
      "Epoch 42/500\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.3684 - accuracy: 0.5057 - val_loss: 1.4977 - val_accuracy: 0.4733\n",
      "Epoch 43/500\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.3586 - accuracy: 0.5386 - val_loss: 1.4947 - val_accuracy: 0.4767\n",
      "Epoch 44/500\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.3507 - accuracy: 0.5386 - val_loss: 1.4815 - val_accuracy: 0.4667\n",
      "Epoch 45/500\n",
      "70/70 [==============================] - 0s 724us/step - loss: 1.3407 - accuracy: 0.5300 - val_loss: 1.4720 - val_accuracy: 0.4833\n",
      "Epoch 46/500\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.3346 - accuracy: 0.5329 - val_loss: 1.4639 - val_accuracy: 0.4733\n",
      "Epoch 47/500\n",
      "70/70 [==============================] - 0s 729us/step - loss: 1.3239 - accuracy: 0.5443 - val_loss: 1.4631 - val_accuracy: 0.4733\n",
      "Epoch 48/500\n",
      "70/70 [==============================] - 0s 735us/step - loss: 1.3164 - accuracy: 0.5429 - val_loss: 1.4556 - val_accuracy: 0.4933\n",
      "Epoch 49/500\n",
      "70/70 [==============================] - 0s 703us/step - loss: 1.3084 - accuracy: 0.5543 - val_loss: 1.4572 - val_accuracy: 0.5100\n",
      "Epoch 50/500\n",
      "70/70 [==============================] - 0s 732us/step - loss: 1.3000 - accuracy: 0.5614 - val_loss: 1.4538 - val_accuracy: 0.5033\n",
      "Epoch 51/500\n",
      "70/70 [==============================] - 0s 723us/step - loss: 1.2915 - accuracy: 0.5700 - val_loss: 1.4480 - val_accuracy: 0.4833\n",
      "Epoch 52/500\n",
      "70/70 [==============================] - 0s 772us/step - loss: 1.2874 - accuracy: 0.5571 - val_loss: 1.4445 - val_accuracy: 0.5000\n",
      "Epoch 53/500\n",
      "70/70 [==============================] - 0s 763us/step - loss: 1.2781 - accuracy: 0.5614 - val_loss: 1.4559 - val_accuracy: 0.4967\n",
      "Epoch 54/500\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.2728 - accuracy: 0.5686 - val_loss: 1.4350 - val_accuracy: 0.4833\n",
      "Epoch 55/500\n",
      "70/70 [==============================] - 0s 753us/step - loss: 1.2624 - accuracy: 0.5586 - val_loss: 1.4288 - val_accuracy: 0.4833\n",
      "Epoch 56/500\n",
      "70/70 [==============================] - 0s 867us/step - loss: 1.2573 - accuracy: 0.5743 - val_loss: 1.4337 - val_accuracy: 0.4900\n",
      "Epoch 57/500\n",
      "70/70 [==============================] - 0s 795us/step - loss: 1.2517 - accuracy: 0.5700 - val_loss: 1.4232 - val_accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "70/70 [==============================] - 0s 763us/step - loss: 1.2446 - accuracy: 0.5571 - val_loss: 1.4156 - val_accuracy: 0.5033\n",
      "Epoch 59/500\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.2376 - accuracy: 0.5843 - val_loss: 1.4127 - val_accuracy: 0.5133\n",
      "Epoch 60/500\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.2309 - accuracy: 0.5786 - val_loss: 1.4056 - val_accuracy: 0.5100\n",
      "Epoch 61/500\n",
      "70/70 [==============================] - 0s 747us/step - loss: 1.2244 - accuracy: 0.5800 - val_loss: 1.4086 - val_accuracy: 0.5167\n",
      "Epoch 62/500\n",
      "70/70 [==============================] - 0s 737us/step - loss: 1.2195 - accuracy: 0.5671 - val_loss: 1.4097 - val_accuracy: 0.5033\n",
      "Epoch 63/500\n",
      "70/70 [==============================] - 0s 750us/step - loss: 1.2125 - accuracy: 0.5886 - val_loss: 1.4110 - val_accuracy: 0.4867\n",
      "Epoch 64/500\n",
      "70/70 [==============================] - 0s 768us/step - loss: 1.2060 - accuracy: 0.6014 - val_loss: 1.3964 - val_accuracy: 0.5100\n",
      "Epoch 65/500\n",
      "70/70 [==============================] - 0s 723us/step - loss: 1.2020 - accuracy: 0.5914 - val_loss: 1.3940 - val_accuracy: 0.5233\n",
      "Epoch 66/500\n",
      "70/70 [==============================] - 0s 760us/step - loss: 1.1958 - accuracy: 0.5914 - val_loss: 1.3907 - val_accuracy: 0.5133\n",
      "Epoch 67/500\n",
      "70/70 [==============================] - 0s 712us/step - loss: 1.1874 - accuracy: 0.6057 - val_loss: 1.4045 - val_accuracy: 0.5133\n",
      "Epoch 68/500\n",
      "70/70 [==============================] - 0s 724us/step - loss: 1.1823 - accuracy: 0.5943 - val_loss: 1.4013 - val_accuracy: 0.5067\n",
      "Epoch 69/500\n",
      "70/70 [==============================] - 0s 757us/step - loss: 1.1760 - accuracy: 0.5871 - val_loss: 1.3934 - val_accuracy: 0.5033\n",
      "Epoch 70/500\n",
      "70/70 [==============================] - 0s 742us/step - loss: 1.1699 - accuracy: 0.6129 - val_loss: 1.3814 - val_accuracy: 0.5200\n",
      "Epoch 71/500\n",
      "70/70 [==============================] - 0s 775us/step - loss: 1.1659 - accuracy: 0.6171 - val_loss: 1.3914 - val_accuracy: 0.5300\n",
      "Epoch 72/500\n",
      "70/70 [==============================] - 0s 749us/step - loss: 1.1627 - accuracy: 0.6171 - val_loss: 1.3793 - val_accuracy: 0.5167\n",
      "Epoch 73/500\n",
      "70/70 [==============================] - 0s 757us/step - loss: 1.1550 - accuracy: 0.6157 - val_loss: 1.3849 - val_accuracy: 0.5200\n",
      "Epoch 74/500\n",
      "70/70 [==============================] - 0s 713us/step - loss: 1.1542 - accuracy: 0.6086 - val_loss: 1.3671 - val_accuracy: 0.5267\n",
      "Epoch 75/500\n",
      "70/70 [==============================] - 0s 792us/step - loss: 1.1466 - accuracy: 0.6157 - val_loss: 1.3669 - val_accuracy: 0.5233\n",
      "Epoch 76/500\n",
      "70/70 [==============================] - 0s 742us/step - loss: 1.1403 - accuracy: 0.6186 - val_loss: 1.3597 - val_accuracy: 0.5300\n",
      "Epoch 77/500\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.1354 - accuracy: 0.6057 - val_loss: 1.3758 - val_accuracy: 0.5233\n",
      "Epoch 78/500\n",
      "70/70 [==============================] - 0s 713us/step - loss: 1.1320 - accuracy: 0.6186 - val_loss: 1.3734 - val_accuracy: 0.5233\n",
      "Epoch 79/500\n",
      "70/70 [==============================] - 0s 741us/step - loss: 1.1266 - accuracy: 0.6086 - val_loss: 1.3763 - val_accuracy: 0.5200\n",
      "Epoch 80/500\n",
      "70/70 [==============================] - 0s 728us/step - loss: 1.1232 - accuracy: 0.6186 - val_loss: 1.3641 - val_accuracy: 0.5433\n",
      "Epoch 81/500\n",
      "70/70 [==============================] - 0s 759us/step - loss: 1.1188 - accuracy: 0.6229 - val_loss: 1.3485 - val_accuracy: 0.5533\n",
      "Epoch 82/500\n",
      "70/70 [==============================] - 0s 758us/step - loss: 1.1163 - accuracy: 0.6114 - val_loss: 1.3484 - val_accuracy: 0.5333\n",
      "Epoch 83/500\n",
      "70/70 [==============================] - 0s 769us/step - loss: 1.1089 - accuracy: 0.6186 - val_loss: 1.3523 - val_accuracy: 0.5433\n",
      "Epoch 84/500\n",
      "70/70 [==============================] - 0s 728us/step - loss: 1.1082 - accuracy: 0.6114 - val_loss: 1.3491 - val_accuracy: 0.5400\n",
      "Epoch 85/500\n",
      "70/70 [==============================] - 0s 759us/step - loss: 1.1012 - accuracy: 0.6329 - val_loss: 1.3459 - val_accuracy: 0.5267\n",
      "Epoch 86/500\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.0952 - accuracy: 0.6271 - val_loss: 1.3490 - val_accuracy: 0.5233\n",
      "Epoch 87/500\n",
      "70/70 [==============================] - 0s 764us/step - loss: 1.0939 - accuracy: 0.6329 - val_loss: 1.3516 - val_accuracy: 0.5267\n",
      "Epoch 88/500\n",
      "70/70 [==============================] - 0s 762us/step - loss: 1.0883 - accuracy: 0.6400 - val_loss: 1.3380 - val_accuracy: 0.5500\n",
      "Epoch 89/500\n",
      "70/70 [==============================] - 0s 783us/step - loss: 1.0828 - accuracy: 0.6129 - val_loss: 1.3473 - val_accuracy: 0.5233\n",
      "Epoch 90/500\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.0813 - accuracy: 0.6329 - val_loss: 1.3401 - val_accuracy: 0.5333\n",
      "Epoch 91/500\n",
      "70/70 [==============================] - 0s 727us/step - loss: 1.0774 - accuracy: 0.6229 - val_loss: 1.3612 - val_accuracy: 0.5167\n",
      "Epoch 92/500\n",
      "70/70 [==============================] - 0s 785us/step - loss: 1.0737 - accuracy: 0.6200 - val_loss: 1.3439 - val_accuracy: 0.5267\n",
      "Epoch 93/500\n",
      "70/70 [==============================] - 0s 752us/step - loss: 1.0694 - accuracy: 0.6314 - val_loss: 1.3303 - val_accuracy: 0.5467\n",
      "Epoch 94/500\n",
      "70/70 [==============================] - 0s 775us/step - loss: 1.0664 - accuracy: 0.6500 - val_loss: 1.3274 - val_accuracy: 0.5367\n",
      "Epoch 95/500\n",
      "70/70 [==============================] - 0s 737us/step - loss: 1.0621 - accuracy: 0.6414 - val_loss: 1.3324 - val_accuracy: 0.5400\n",
      "Epoch 96/500\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.0577 - accuracy: 0.6443 - val_loss: 1.3605 - val_accuracy: 0.5267\n",
      "Epoch 97/500\n",
      "70/70 [==============================] - 0s 736us/step - loss: 1.0590 - accuracy: 0.6371 - val_loss: 1.3366 - val_accuracy: 0.5067\n",
      "Epoch 98/500\n",
      "70/70 [==============================] - 0s 736us/step - loss: 1.0546 - accuracy: 0.6343 - val_loss: 1.3345 - val_accuracy: 0.5233\n",
      "Epoch 99/500\n",
      "70/70 [==============================] - 0s 728us/step - loss: 1.0450 - accuracy: 0.6457 - val_loss: 1.3443 - val_accuracy: 0.5233\n",
      "Epoch 100/500\n",
      "70/70 [==============================] - 0s 721us/step - loss: 1.0471 - accuracy: 0.6386 - val_loss: 1.3364 - val_accuracy: 0.5400\n",
      "Epoch 101/500\n",
      "70/70 [==============================] - 0s 714us/step - loss: 1.0458 - accuracy: 0.6214 - val_loss: 1.3373 - val_accuracy: 0.5067\n",
      "Epoch 102/500\n",
      "70/70 [==============================] - 0s 747us/step - loss: 1.0415 - accuracy: 0.6386 - val_loss: 1.3323 - val_accuracy: 0.5167\n",
      "Epoch 103/500\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.0357 - accuracy: 0.6257 - val_loss: 1.3278 - val_accuracy: 0.5367\n",
      "Epoch 104/500\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.0335 - accuracy: 0.6400 - val_loss: 1.3228 - val_accuracy: 0.5367\n",
      "Epoch 105/500\n",
      "70/70 [==============================] - 0s 756us/step - loss: 1.0308 - accuracy: 0.6414 - val_loss: 1.3221 - val_accuracy: 0.5167\n",
      "Epoch 106/500\n",
      "70/70 [==============================] - 0s 736us/step - loss: 1.0265 - accuracy: 0.6343 - val_loss: 1.3259 - val_accuracy: 0.5400\n",
      "Epoch 107/500\n",
      "70/70 [==============================] - 0s 752us/step - loss: 1.0263 - accuracy: 0.6557 - val_loss: 1.3202 - val_accuracy: 0.5233\n",
      "Epoch 108/500\n",
      "70/70 [==============================] - 0s 759us/step - loss: 1.0244 - accuracy: 0.6500 - val_loss: 1.3280 - val_accuracy: 0.4867\n",
      "Epoch 109/500\n",
      "70/70 [==============================] - 0s 782us/step - loss: 1.0168 - accuracy: 0.6529 - val_loss: 1.3195 - val_accuracy: 0.5167\n",
      "Epoch 110/500\n",
      "70/70 [==============================] - 0s 801us/step - loss: 1.0182 - accuracy: 0.6457 - val_loss: 1.3087 - val_accuracy: 0.5400\n",
      "Epoch 111/500\n",
      "70/70 [==============================] - 0s 758us/step - loss: 1.0128 - accuracy: 0.6514 - val_loss: 1.3061 - val_accuracy: 0.5433\n",
      "Epoch 112/500\n",
      "70/70 [==============================] - 0s 757us/step - loss: 1.0086 - accuracy: 0.6386 - val_loss: 1.3226 - val_accuracy: 0.5267\n",
      "Epoch 113/500\n",
      "70/70 [==============================] - 0s 748us/step - loss: 1.0122 - accuracy: 0.6457 - val_loss: 1.3116 - val_accuracy: 0.5333\n",
      "Epoch 114/500\n",
      "70/70 [==============================] - 0s 726us/step - loss: 1.0051 - accuracy: 0.6500 - val_loss: 1.3227 - val_accuracy: 0.5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "70/70 [==============================] - 0s 763us/step - loss: 1.0002 - accuracy: 0.6557 - val_loss: 1.3168 - val_accuracy: 0.5333\n",
      "Epoch 116/500\n",
      "70/70 [==============================] - 0s 768us/step - loss: 0.9986 - accuracy: 0.6543 - val_loss: 1.3332 - val_accuracy: 0.5233\n",
      "Epoch 117/500\n",
      "70/70 [==============================] - 0s 770us/step - loss: 0.9994 - accuracy: 0.6571 - val_loss: 1.3162 - val_accuracy: 0.5200\n",
      "Epoch 118/500\n",
      "70/70 [==============================] - 0s 754us/step - loss: 0.9942 - accuracy: 0.6471 - val_loss: 1.3132 - val_accuracy: 0.5500\n",
      "Epoch 119/500\n",
      "70/70 [==============================] - 0s 709us/step - loss: 0.9910 - accuracy: 0.6500 - val_loss: 1.3224 - val_accuracy: 0.5233\n",
      "Epoch 120/500\n",
      "70/70 [==============================] - 0s 762us/step - loss: 0.9904 - accuracy: 0.6329 - val_loss: 1.3191 - val_accuracy: 0.5500\n",
      "Epoch 121/500\n",
      "70/70 [==============================] - 0s 761us/step - loss: 0.9879 - accuracy: 0.6429 - val_loss: 1.3198 - val_accuracy: 0.4967\n",
      "Epoch 122/500\n",
      "70/70 [==============================] - 0s 755us/step - loss: 0.9844 - accuracy: 0.6614 - val_loss: 1.3184 - val_accuracy: 0.5233\n",
      "Epoch 123/500\n",
      "70/70 [==============================] - 0s 723us/step - loss: 0.9822 - accuracy: 0.6529 - val_loss: 1.3167 - val_accuracy: 0.5267\n",
      "Epoch 124/500\n",
      "70/70 [==============================] - 0s 776us/step - loss: 0.9811 - accuracy: 0.6629 - val_loss: 1.3235 - val_accuracy: 0.5267\n",
      "Epoch 125/500\n",
      "70/70 [==============================] - 0s 739us/step - loss: 0.9776 - accuracy: 0.6729 - val_loss: 1.3326 - val_accuracy: 0.5233\n",
      "Epoch 126/500\n",
      "70/70 [==============================] - 0s 743us/step - loss: 0.9728 - accuracy: 0.6557 - val_loss: 1.3110 - val_accuracy: 0.5033\n",
      "Epoch 127/500\n",
      "70/70 [==============================] - 0s 726us/step - loss: 0.9773 - accuracy: 0.6571 - val_loss: 1.3112 - val_accuracy: 0.5400\n",
      "Epoch 128/500\n",
      "70/70 [==============================] - 0s 727us/step - loss: 0.9716 - accuracy: 0.6757 - val_loss: 1.3158 - val_accuracy: 0.5333\n",
      "Epoch 129/500\n",
      "70/70 [==============================] - 0s 734us/step - loss: 0.9697 - accuracy: 0.6486 - val_loss: 1.3159 - val_accuracy: 0.5300\n",
      "Epoch 130/500\n",
      "70/70 [==============================] - 0s 741us/step - loss: 0.9675 - accuracy: 0.6557 - val_loss: 1.3012 - val_accuracy: 0.5367\n",
      "Epoch 131/500\n",
      "70/70 [==============================] - 0s 739us/step - loss: 0.9635 - accuracy: 0.6600 - val_loss: 1.3134 - val_accuracy: 0.5100\n",
      "Epoch 132/500\n",
      "70/70 [==============================] - 0s 733us/step - loss: 0.9598 - accuracy: 0.6586 - val_loss: 1.3169 - val_accuracy: 0.5167\n",
      "Epoch 133/500\n",
      "70/70 [==============================] - 0s 725us/step - loss: 0.9620 - accuracy: 0.6686 - val_loss: 1.3168 - val_accuracy: 0.5367\n",
      "Epoch 134/500\n",
      "70/70 [==============================] - 0s 724us/step - loss: 0.9620 - accuracy: 0.6600 - val_loss: 1.3137 - val_accuracy: 0.5167\n",
      "Epoch 135/500\n",
      "70/70 [==============================] - 0s 737us/step - loss: 0.9572 - accuracy: 0.6614 - val_loss: 1.3107 - val_accuracy: 0.5467\n",
      "Epoch 136/500\n",
      "70/70 [==============================] - 0s 779us/step - loss: 0.9557 - accuracy: 0.6571 - val_loss: 1.3091 - val_accuracy: 0.5300\n",
      "Epoch 137/500\n",
      "70/70 [==============================] - 0s 761us/step - loss: 0.9509 - accuracy: 0.6671 - val_loss: 1.3141 - val_accuracy: 0.5367\n",
      "Epoch 138/500\n",
      "70/70 [==============================] - 0s 766us/step - loss: 0.9501 - accuracy: 0.6557 - val_loss: 1.3220 - val_accuracy: 0.5133\n",
      "Epoch 139/500\n",
      "70/70 [==============================] - 0s 740us/step - loss: 0.9498 - accuracy: 0.6529 - val_loss: 1.3062 - val_accuracy: 0.5467\n",
      "Epoch 140/500\n",
      "70/70 [==============================] - 0s 726us/step - loss: 0.9488 - accuracy: 0.6700 - val_loss: 1.3062 - val_accuracy: 0.5267\n",
      "Epoch 141/500\n",
      "70/70 [==============================] - 0s 764us/step - loss: 0.9445 - accuracy: 0.6614 - val_loss: 1.3227 - val_accuracy: 0.5233\n",
      "Epoch 142/500\n",
      "70/70 [==============================] - 0s 765us/step - loss: 0.9453 - accuracy: 0.6600 - val_loss: 1.3003 - val_accuracy: 0.5567\n",
      "Epoch 143/500\n",
      "70/70 [==============================] - 0s 775us/step - loss: 0.9386 - accuracy: 0.6557 - val_loss: 1.2978 - val_accuracy: 0.5400\n",
      "Epoch 144/500\n",
      "70/70 [==============================] - 0s 758us/step - loss: 0.9361 - accuracy: 0.6657 - val_loss: 1.3313 - val_accuracy: 0.5067\n",
      "Epoch 145/500\n",
      "70/70 [==============================] - 0s 724us/step - loss: 0.9373 - accuracy: 0.6543 - val_loss: 1.3039 - val_accuracy: 0.5467\n",
      "Epoch 146/500\n",
      "70/70 [==============================] - 0s 725us/step - loss: 0.9292 - accuracy: 0.6586 - val_loss: 1.3163 - val_accuracy: 0.5467\n",
      "Epoch 147/500\n",
      "70/70 [==============================] - 0s 738us/step - loss: 0.9324 - accuracy: 0.6600 - val_loss: 1.3203 - val_accuracy: 0.5467\n",
      "Epoch 148/500\n",
      "70/70 [==============================] - 0s 727us/step - loss: 0.9310 - accuracy: 0.6629 - val_loss: 1.3254 - val_accuracy: 0.5167\n",
      "Epoch 149/500\n",
      "70/70 [==============================] - 0s 759us/step - loss: 0.9300 - accuracy: 0.6529 - val_loss: 1.3060 - val_accuracy: 0.5400\n",
      "Epoch 150/500\n",
      "70/70 [==============================] - 0s 758us/step - loss: 0.9292 - accuracy: 0.6671 - val_loss: 1.3111 - val_accuracy: 0.5233\n",
      "Epoch 151/500\n",
      "70/70 [==============================] - 0s 730us/step - loss: 0.9234 - accuracy: 0.6657 - val_loss: 1.3201 - val_accuracy: 0.5533\n",
      "Epoch 152/500\n",
      "70/70 [==============================] - 0s 721us/step - loss: 0.9255 - accuracy: 0.6514 - val_loss: 1.3091 - val_accuracy: 0.5500\n",
      "Epoch 153/500\n",
      "70/70 [==============================] - 0s 741us/step - loss: 0.9215 - accuracy: 0.6614 - val_loss: 1.3166 - val_accuracy: 0.5167\n",
      "Epoch 154/500\n",
      "70/70 [==============================] - 0s 746us/step - loss: 0.9205 - accuracy: 0.6571 - val_loss: 1.3068 - val_accuracy: 0.5267\n",
      "Epoch 155/500\n",
      "70/70 [==============================] - 0s 724us/step - loss: 0.9193 - accuracy: 0.6629 - val_loss: 1.3181 - val_accuracy: 0.5233\n",
      "Epoch 156/500\n",
      "70/70 [==============================] - 0s 732us/step - loss: 0.9197 - accuracy: 0.6571 - val_loss: 1.3117 - val_accuracy: 0.5567\n",
      "Epoch 157/500\n",
      "70/70 [==============================] - 0s 738us/step - loss: 0.9117 - accuracy: 0.6714 - val_loss: 1.3169 - val_accuracy: 0.5600\n",
      "Epoch 158/500\n",
      "70/70 [==============================] - 0s 723us/step - loss: 0.9151 - accuracy: 0.6643 - val_loss: 1.3006 - val_accuracy: 0.5300\n",
      "Epoch 159/500\n",
      "70/70 [==============================] - 0s 739us/step - loss: 0.9119 - accuracy: 0.6629 - val_loss: 1.3054 - val_accuracy: 0.5333\n",
      "Epoch 160/500\n",
      "70/70 [==============================] - 0s 722us/step - loss: 0.9108 - accuracy: 0.6786 - val_loss: 1.3019 - val_accuracy: 0.5600\n",
      "Epoch 161/500\n",
      "70/70 [==============================] - 0s 770us/step - loss: 0.9059 - accuracy: 0.6700 - val_loss: 1.3211 - val_accuracy: 0.5300\n",
      "Epoch 162/500\n",
      "70/70 [==============================] - 0s 728us/step - loss: 0.9091 - accuracy: 0.6700 - val_loss: 1.3147 - val_accuracy: 0.5233\n",
      "Epoch 163/500\n",
      "70/70 [==============================] - 0s 730us/step - loss: 0.9079 - accuracy: 0.6671 - val_loss: 1.3076 - val_accuracy: 0.5300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb1c969a8b0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=500, batch_size=10, \n",
    "               validation_data=(X_val, Y_val), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=2 , input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax')) #0~9까지의 숫자니까 unit=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/163\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 2.2390 - accuracy: 0.1229 - val_loss: 2.1779 - val_accuracy: 0.1100\n",
      "Epoch 2/163\n",
      "70/70 [==============================] - 0s 732us/step - loss: 2.1569 - accuracy: 0.1657 - val_loss: 2.1086 - val_accuracy: 0.1167\n",
      "Epoch 3/163\n",
      "70/70 [==============================] - 0s 709us/step - loss: 2.0944 - accuracy: 0.1271 - val_loss: 2.0556 - val_accuracy: 0.1267\n",
      "Epoch 4/163\n",
      "70/70 [==============================] - 0s 723us/step - loss: 2.0409 - accuracy: 0.2157 - val_loss: 2.0049 - val_accuracy: 0.1900\n",
      "Epoch 5/163\n",
      "70/70 [==============================] - 0s 737us/step - loss: 1.9907 - accuracy: 0.2429 - val_loss: 1.9648 - val_accuracy: 0.2133\n",
      "Epoch 6/163\n",
      "70/70 [==============================] - 0s 721us/step - loss: 1.9465 - accuracy: 0.2543 - val_loss: 1.9254 - val_accuracy: 0.2133\n",
      "Epoch 7/163\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.9049 - accuracy: 0.2714 - val_loss: 1.8875 - val_accuracy: 0.2167\n",
      "Epoch 8/163\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.8676 - accuracy: 0.2743 - val_loss: 1.8587 - val_accuracy: 0.2333\n",
      "Epoch 9/163\n",
      "70/70 [==============================] - 0s 722us/step - loss: 1.8332 - accuracy: 0.2871 - val_loss: 1.8310 - val_accuracy: 0.2333\n",
      "Epoch 10/163\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.8019 - accuracy: 0.3114 - val_loss: 1.8017 - val_accuracy: 0.2500\n",
      "Epoch 11/163\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.7736 - accuracy: 0.3057 - val_loss: 1.7771 - val_accuracy: 0.2367\n",
      "Epoch 12/163\n",
      "70/70 [==============================] - 0s 727us/step - loss: 1.7455 - accuracy: 0.3057 - val_loss: 1.7593 - val_accuracy: 0.2600\n",
      "Epoch 13/163\n",
      "70/70 [==============================] - 0s 718us/step - loss: 1.7185 - accuracy: 0.3329 - val_loss: 1.7335 - val_accuracy: 0.2833\n",
      "Epoch 14/163\n",
      "70/70 [==============================] - 0s 723us/step - loss: 1.6957 - accuracy: 0.3286 - val_loss: 1.7127 - val_accuracy: 0.2900\n",
      "Epoch 15/163\n",
      "70/70 [==============================] - 0s 722us/step - loss: 1.6751 - accuracy: 0.3571 - val_loss: 1.6961 - val_accuracy: 0.3167\n",
      "Epoch 16/163\n",
      "70/70 [==============================] - 0s 731us/step - loss: 1.6527 - accuracy: 0.3900 - val_loss: 1.6781 - val_accuracy: 0.3400\n",
      "Epoch 17/163\n",
      "70/70 [==============================] - 0s 761us/step - loss: 1.6323 - accuracy: 0.4029 - val_loss: 1.6579 - val_accuracy: 0.3400\n",
      "Epoch 18/163\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.6139 - accuracy: 0.4157 - val_loss: 1.6489 - val_accuracy: 0.3867\n",
      "Epoch 19/163\n",
      "70/70 [==============================] - 0s 702us/step - loss: 1.5943 - accuracy: 0.4257 - val_loss: 1.6230 - val_accuracy: 0.3500\n",
      "Epoch 20/163\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.5766 - accuracy: 0.4171 - val_loss: 1.6087 - val_accuracy: 0.3433\n",
      "Epoch 21/163\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.5606 - accuracy: 0.4557 - val_loss: 1.5957 - val_accuracy: 0.3733\n",
      "Epoch 22/163\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.5445 - accuracy: 0.4586 - val_loss: 1.5856 - val_accuracy: 0.3800\n",
      "Epoch 23/163\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.5294 - accuracy: 0.4357 - val_loss: 1.5720 - val_accuracy: 0.3933\n",
      "Epoch 24/163\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.5143 - accuracy: 0.4414 - val_loss: 1.5667 - val_accuracy: 0.3767\n",
      "Epoch 25/163\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.5002 - accuracy: 0.4429 - val_loss: 1.5544 - val_accuracy: 0.3767\n",
      "Epoch 26/163\n",
      "70/70 [==============================] - 0s 717us/step - loss: 1.4880 - accuracy: 0.4457 - val_loss: 1.5354 - val_accuracy: 0.4233\n",
      "Epoch 27/163\n",
      "70/70 [==============================] - 0s 753us/step - loss: 1.4754 - accuracy: 0.4543 - val_loss: 1.5282 - val_accuracy: 0.3933\n",
      "Epoch 28/163\n",
      "70/70 [==============================] - 0s 745us/step - loss: 1.4648 - accuracy: 0.4400 - val_loss: 1.5272 - val_accuracy: 0.3933\n",
      "Epoch 29/163\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.4525 - accuracy: 0.4600 - val_loss: 1.5071 - val_accuracy: 0.4033\n",
      "Epoch 30/163\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.4405 - accuracy: 0.4600 - val_loss: 1.5034 - val_accuracy: 0.4067\n",
      "Epoch 31/163\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.4328 - accuracy: 0.4500 - val_loss: 1.4863 - val_accuracy: 0.4100\n",
      "Epoch 32/163\n",
      "70/70 [==============================] - 0s 755us/step - loss: 1.4224 - accuracy: 0.4429 - val_loss: 1.4834 - val_accuracy: 0.4600\n",
      "Epoch 33/163\n",
      "70/70 [==============================] - 0s 750us/step - loss: 1.4106 - accuracy: 0.4671 - val_loss: 1.4802 - val_accuracy: 0.4567\n",
      "Epoch 34/163\n",
      "70/70 [==============================] - 0s 749us/step - loss: 1.4034 - accuracy: 0.4586 - val_loss: 1.4636 - val_accuracy: 0.4733\n",
      "Epoch 35/163\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.3930 - accuracy: 0.4771 - val_loss: 1.4582 - val_accuracy: 0.4433\n",
      "Epoch 36/163\n",
      "70/70 [==============================] - 0s 748us/step - loss: 1.3869 - accuracy: 0.4857 - val_loss: 1.4600 - val_accuracy: 0.4667\n",
      "Epoch 37/163\n",
      "70/70 [==============================] - 0s 758us/step - loss: 1.3755 - accuracy: 0.4800 - val_loss: 1.4471 - val_accuracy: 0.4533\n",
      "Epoch 38/163\n",
      "70/70 [==============================] - 0s 765us/step - loss: 1.3711 - accuracy: 0.4671 - val_loss: 1.4486 - val_accuracy: 0.4567\n",
      "Epoch 39/163\n",
      "70/70 [==============================] - 0s 736us/step - loss: 1.3605 - accuracy: 0.4771 - val_loss: 1.4385 - val_accuracy: 0.4800\n",
      "Epoch 40/163\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.3549 - accuracy: 0.4900 - val_loss: 1.4312 - val_accuracy: 0.5000\n",
      "Epoch 41/163\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.3471 - accuracy: 0.4843 - val_loss: 1.4167 - val_accuracy: 0.4767\n",
      "Epoch 42/163\n",
      "70/70 [==============================] - 0s 742us/step - loss: 1.3399 - accuracy: 0.4786 - val_loss: 1.4132 - val_accuracy: 0.4900\n",
      "Epoch 43/163\n",
      "70/70 [==============================] - 0s 753us/step - loss: 1.3340 - accuracy: 0.4914 - val_loss: 1.4085 - val_accuracy: 0.4900\n",
      "Epoch 44/163\n",
      "70/70 [==============================] - 0s 741us/step - loss: 1.3254 - accuracy: 0.5014 - val_loss: 1.3965 - val_accuracy: 0.4767\n",
      "Epoch 45/163\n",
      "70/70 [==============================] - 0s 730us/step - loss: 1.3201 - accuracy: 0.5000 - val_loss: 1.4063 - val_accuracy: 0.4600\n",
      "Epoch 46/163\n",
      "70/70 [==============================] - 0s 798us/step - loss: 1.3148 - accuracy: 0.4971 - val_loss: 1.3911 - val_accuracy: 0.4833\n",
      "Epoch 47/163\n",
      "70/70 [==============================] - 0s 803us/step - loss: 1.3083 - accuracy: 0.5043 - val_loss: 1.3859 - val_accuracy: 0.5033\n",
      "Epoch 48/163\n",
      "70/70 [==============================] - 0s 765us/step - loss: 1.2994 - accuracy: 0.5100 - val_loss: 1.3782 - val_accuracy: 0.4667\n",
      "Epoch 49/163\n",
      "70/70 [==============================] - 0s 799us/step - loss: 1.2959 - accuracy: 0.4886 - val_loss: 1.3872 - val_accuracy: 0.5000\n",
      "Epoch 50/163\n",
      "70/70 [==============================] - 0s 723us/step - loss: 1.2914 - accuracy: 0.5157 - val_loss: 1.3844 - val_accuracy: 0.4967\n",
      "Epoch 51/163\n",
      "70/70 [==============================] - 0s 701us/step - loss: 1.2846 - accuracy: 0.5100 - val_loss: 1.3740 - val_accuracy: 0.5100\n",
      "Epoch 52/163\n",
      "70/70 [==============================] - 0s 706us/step - loss: 1.2794 - accuracy: 0.5114 - val_loss: 1.3685 - val_accuracy: 0.5200\n",
      "Epoch 53/163\n",
      "70/70 [==============================] - 0s 718us/step - loss: 1.2747 - accuracy: 0.5129 - val_loss: 1.3741 - val_accuracy: 0.4733\n",
      "Epoch 54/163\n",
      "70/70 [==============================] - 0s 752us/step - loss: 1.2687 - accuracy: 0.5157 - val_loss: 1.3671 - val_accuracy: 0.4967\n",
      "Epoch 55/163\n",
      "70/70 [==============================] - 0s 876us/step - loss: 1.2649 - accuracy: 0.5329 - val_loss: 1.3551 - val_accuracy: 0.5100\n",
      "Epoch 56/163\n",
      "70/70 [==============================] - 0s 787us/step - loss: 1.2591 - accuracy: 0.5286 - val_loss: 1.3588 - val_accuracy: 0.5100\n",
      "Epoch 57/163\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.2532 - accuracy: 0.5357 - val_loss: 1.3577 - val_accuracy: 0.4967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/163\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.2485 - accuracy: 0.5329 - val_loss: 1.3609 - val_accuracy: 0.4900\n",
      "Epoch 59/163\n",
      "70/70 [==============================] - 0s 730us/step - loss: 1.2434 - accuracy: 0.5429 - val_loss: 1.3468 - val_accuracy: 0.5000\n",
      "Epoch 60/163\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.2370 - accuracy: 0.5386 - val_loss: 1.3399 - val_accuracy: 0.4933\n",
      "Epoch 61/163\n",
      "70/70 [==============================] - 0s 713us/step - loss: 1.2333 - accuracy: 0.5314 - val_loss: 1.3489 - val_accuracy: 0.4833\n",
      "Epoch 62/163\n",
      "70/70 [==============================] - 0s 778us/step - loss: 1.2300 - accuracy: 0.5414 - val_loss: 1.3377 - val_accuracy: 0.4900\n",
      "Epoch 63/163\n",
      "70/70 [==============================] - 0s 713us/step - loss: 1.2239 - accuracy: 0.5443 - val_loss: 1.3304 - val_accuracy: 0.5067\n",
      "Epoch 64/163\n",
      "70/70 [==============================] - 0s 717us/step - loss: 1.2191 - accuracy: 0.5614 - val_loss: 1.3349 - val_accuracy: 0.5067\n",
      "Epoch 65/163\n",
      "70/70 [==============================] - 0s 952us/step - loss: 1.2190 - accuracy: 0.5486 - val_loss: 1.3320 - val_accuracy: 0.5000\n",
      "Epoch 66/163\n",
      "70/70 [==============================] - 0s 753us/step - loss: 1.2124 - accuracy: 0.5400 - val_loss: 1.3377 - val_accuracy: 0.4800\n",
      "Epoch 67/163\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.2091 - accuracy: 0.5343 - val_loss: 1.3243 - val_accuracy: 0.5067\n",
      "Epoch 68/163\n",
      "70/70 [==============================] - 0s 710us/step - loss: 1.2027 - accuracy: 0.5600 - val_loss: 1.3165 - val_accuracy: 0.5233\n",
      "Epoch 69/163\n",
      "70/70 [==============================] - 0s 759us/step - loss: 1.1996 - accuracy: 0.5471 - val_loss: 1.3177 - val_accuracy: 0.5267\n",
      "Epoch 70/163\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.1964 - accuracy: 0.5700 - val_loss: 1.3200 - val_accuracy: 0.4967\n",
      "Epoch 71/163\n",
      "70/70 [==============================] - 0s 696us/step - loss: 1.1905 - accuracy: 0.5414 - val_loss: 1.3194 - val_accuracy: 0.5133\n",
      "Epoch 72/163\n",
      "70/70 [==============================] - 0s 787us/step - loss: 1.1899 - accuracy: 0.5471 - val_loss: 1.3084 - val_accuracy: 0.5133\n",
      "Epoch 73/163\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.1835 - accuracy: 0.5514 - val_loss: 1.3106 - val_accuracy: 0.4967\n",
      "Epoch 74/163\n",
      "70/70 [==============================] - 0s 735us/step - loss: 1.1809 - accuracy: 0.5600 - val_loss: 1.3235 - val_accuracy: 0.5100\n",
      "Epoch 75/163\n",
      "70/70 [==============================] - 0s 732us/step - loss: 1.1782 - accuracy: 0.5657 - val_loss: 1.2999 - val_accuracy: 0.5100\n",
      "Epoch 76/163\n",
      "70/70 [==============================] - 0s 715us/step - loss: 1.1741 - accuracy: 0.5729 - val_loss: 1.2976 - val_accuracy: 0.5133\n",
      "Epoch 77/163\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.1678 - accuracy: 0.5714 - val_loss: 1.3199 - val_accuracy: 0.4933\n",
      "Epoch 78/163\n",
      "70/70 [==============================] - 0s 756us/step - loss: 1.1663 - accuracy: 0.5629 - val_loss: 1.3045 - val_accuracy: 0.5033\n",
      "Epoch 79/163\n",
      "70/70 [==============================] - 0s 773us/step - loss: 1.1625 - accuracy: 0.5814 - val_loss: 1.2982 - val_accuracy: 0.5100\n",
      "Epoch 80/163\n",
      "70/70 [==============================] - 0s 724us/step - loss: 1.1592 - accuracy: 0.5614 - val_loss: 1.2985 - val_accuracy: 0.5067\n",
      "Epoch 81/163\n",
      "70/70 [==============================] - 0s 722us/step - loss: 1.1594 - accuracy: 0.5686 - val_loss: 1.3026 - val_accuracy: 0.5100\n",
      "Epoch 82/163\n",
      "70/70 [==============================] - 0s 762us/step - loss: 1.1570 - accuracy: 0.5743 - val_loss: 1.2997 - val_accuracy: 0.5133\n",
      "Epoch 83/163\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.1508 - accuracy: 0.5729 - val_loss: 1.3138 - val_accuracy: 0.4833\n",
      "Epoch 84/163\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.1477 - accuracy: 0.5800 - val_loss: 1.3031 - val_accuracy: 0.5100\n",
      "Epoch 85/163\n",
      "70/70 [==============================] - 0s 727us/step - loss: 1.1451 - accuracy: 0.5743 - val_loss: 1.3077 - val_accuracy: 0.5033\n",
      "Epoch 86/163\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.1392 - accuracy: 0.5757 - val_loss: 1.3006 - val_accuracy: 0.5167\n",
      "Epoch 87/163\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.1392 - accuracy: 0.5729 - val_loss: 1.3021 - val_accuracy: 0.5067\n",
      "Epoch 88/163\n",
      "70/70 [==============================] - 0s 719us/step - loss: 1.1355 - accuracy: 0.5971 - val_loss: 1.2952 - val_accuracy: 0.4933\n",
      "Epoch 89/163\n",
      "70/70 [==============================] - 0s 727us/step - loss: 1.1338 - accuracy: 0.5843 - val_loss: 1.2956 - val_accuracy: 0.5267\n",
      "Epoch 90/163\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.1306 - accuracy: 0.5857 - val_loss: 1.2900 - val_accuracy: 0.5100\n",
      "Epoch 91/163\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.1257 - accuracy: 0.5943 - val_loss: 1.2858 - val_accuracy: 0.5133\n",
      "Epoch 92/163\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.1228 - accuracy: 0.5871 - val_loss: 1.2819 - val_accuracy: 0.5300\n",
      "Epoch 93/163\n",
      "70/70 [==============================] - 0s 711us/step - loss: 1.1198 - accuracy: 0.5871 - val_loss: 1.2920 - val_accuracy: 0.5200\n",
      "Epoch 94/163\n",
      "70/70 [==============================] - 0s 732us/step - loss: 1.1144 - accuracy: 0.5943 - val_loss: 1.3012 - val_accuracy: 0.5233\n",
      "Epoch 95/163\n",
      "70/70 [==============================] - 0s 728us/step - loss: 1.1134 - accuracy: 0.5943 - val_loss: 1.3066 - val_accuracy: 0.5133\n",
      "Epoch 96/163\n",
      "70/70 [==============================] - 0s 717us/step - loss: 1.1123 - accuracy: 0.5943 - val_loss: 1.2935 - val_accuracy: 0.5167\n",
      "Epoch 97/163\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.1088 - accuracy: 0.5957 - val_loss: 1.3036 - val_accuracy: 0.5200\n",
      "Epoch 98/163\n",
      "70/70 [==============================] - 0s 763us/step - loss: 1.1056 - accuracy: 0.6043 - val_loss: 1.2828 - val_accuracy: 0.5200\n",
      "Epoch 99/163\n",
      "70/70 [==============================] - 0s 753us/step - loss: 1.1031 - accuracy: 0.5957 - val_loss: 1.2784 - val_accuracy: 0.5333\n",
      "Epoch 100/163\n",
      "70/70 [==============================] - 0s 732us/step - loss: 1.1008 - accuracy: 0.5957 - val_loss: 1.3000 - val_accuracy: 0.5200\n",
      "Epoch 101/163\n",
      "70/70 [==============================] - 0s 711us/step - loss: 1.1006 - accuracy: 0.6071 - val_loss: 1.2762 - val_accuracy: 0.5267\n",
      "Epoch 102/163\n",
      "70/70 [==============================] - 0s 715us/step - loss: 1.0963 - accuracy: 0.6014 - val_loss: 1.2788 - val_accuracy: 0.5333\n",
      "Epoch 103/163\n",
      "70/70 [==============================] - 0s 778us/step - loss: 1.0928 - accuracy: 0.5971 - val_loss: 1.2784 - val_accuracy: 0.5300\n",
      "Epoch 104/163\n",
      "70/70 [==============================] - 0s 741us/step - loss: 1.0925 - accuracy: 0.6043 - val_loss: 1.2728 - val_accuracy: 0.5300\n",
      "Epoch 105/163\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.0883 - accuracy: 0.6043 - val_loss: 1.2710 - val_accuracy: 0.5400\n",
      "Epoch 106/163\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.0876 - accuracy: 0.6043 - val_loss: 1.2836 - val_accuracy: 0.5267\n",
      "Epoch 107/163\n",
      "70/70 [==============================] - 0s 740us/step - loss: 1.0833 - accuracy: 0.6114 - val_loss: 1.2827 - val_accuracy: 0.5133\n",
      "Epoch 108/163\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.0834 - accuracy: 0.6029 - val_loss: 1.2722 - val_accuracy: 0.5200\n",
      "Epoch 109/163\n",
      "70/70 [==============================] - 0s 746us/step - loss: 1.0808 - accuracy: 0.6157 - val_loss: 1.2802 - val_accuracy: 0.5433\n",
      "Epoch 110/163\n",
      "70/70 [==============================] - 0s 733us/step - loss: 1.0747 - accuracy: 0.6143 - val_loss: 1.2880 - val_accuracy: 0.5267\n",
      "Epoch 111/163\n",
      "70/70 [==============================] - 0s 716us/step - loss: 1.0757 - accuracy: 0.6171 - val_loss: 1.2798 - val_accuracy: 0.5433\n",
      "Epoch 112/163\n",
      "70/70 [==============================] - 0s 769us/step - loss: 1.0726 - accuracy: 0.6057 - val_loss: 1.2600 - val_accuracy: 0.5300\n",
      "Epoch 113/163\n",
      "70/70 [==============================] - 0s 796us/step - loss: 1.0684 - accuracy: 0.6086 - val_loss: 1.2688 - val_accuracy: 0.5300\n",
      "Epoch 114/163\n",
      "70/70 [==============================] - 0s 755us/step - loss: 1.0681 - accuracy: 0.6129 - val_loss: 1.2737 - val_accuracy: 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/163\n",
      "70/70 [==============================] - 0s 747us/step - loss: 1.0617 - accuracy: 0.6086 - val_loss: 1.2626 - val_accuracy: 0.5200\n",
      "Epoch 116/163\n",
      "70/70 [==============================] - 0s 710us/step - loss: 1.0625 - accuracy: 0.6243 - val_loss: 1.2708 - val_accuracy: 0.5433\n",
      "Epoch 117/163\n",
      "70/70 [==============================] - 0s 717us/step - loss: 1.0595 - accuracy: 0.6186 - val_loss: 1.2611 - val_accuracy: 0.5400\n",
      "Epoch 118/163\n",
      "70/70 [==============================] - 0s 765us/step - loss: 1.0586 - accuracy: 0.6100 - val_loss: 1.2734 - val_accuracy: 0.5467\n",
      "Epoch 119/163\n",
      "70/70 [==============================] - 0s 755us/step - loss: 1.0548 - accuracy: 0.6243 - val_loss: 1.2657 - val_accuracy: 0.5433\n",
      "Epoch 120/163\n",
      "70/70 [==============================] - 0s 760us/step - loss: 1.0521 - accuracy: 0.6257 - val_loss: 1.2747 - val_accuracy: 0.5400\n",
      "Epoch 121/163\n",
      "70/70 [==============================] - 0s 785us/step - loss: 1.0472 - accuracy: 0.6214 - val_loss: 1.2827 - val_accuracy: 0.5200\n",
      "Epoch 122/163\n",
      "70/70 [==============================] - 0s 725us/step - loss: 1.0480 - accuracy: 0.6271 - val_loss: 1.2725 - val_accuracy: 0.5467\n",
      "Epoch 123/163\n",
      "70/70 [==============================] - 0s 720us/step - loss: 1.0450 - accuracy: 0.6200 - val_loss: 1.2578 - val_accuracy: 0.5500\n",
      "Epoch 124/163\n",
      "70/70 [==============================] - 0s 777us/step - loss: 1.0442 - accuracy: 0.6357 - val_loss: 1.2799 - val_accuracy: 0.5433\n",
      "Epoch 125/163\n",
      "70/70 [==============================] - 0s 742us/step - loss: 1.0418 - accuracy: 0.6357 - val_loss: 1.2599 - val_accuracy: 0.5333\n",
      "Epoch 126/163\n",
      "70/70 [==============================] - 0s 751us/step - loss: 1.0406 - accuracy: 0.6257 - val_loss: 1.2710 - val_accuracy: 0.5433\n",
      "Epoch 127/163\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.0344 - accuracy: 0.6271 - val_loss: 1.2684 - val_accuracy: 0.5533\n",
      "Epoch 128/163\n",
      "70/70 [==============================] - 0s 757us/step - loss: 1.0353 - accuracy: 0.6343 - val_loss: 1.2726 - val_accuracy: 0.5433\n",
      "Epoch 129/163\n",
      "70/70 [==============================] - 0s 735us/step - loss: 1.0294 - accuracy: 0.6243 - val_loss: 1.2744 - val_accuracy: 0.5367\n",
      "Epoch 130/163\n",
      "70/70 [==============================] - 0s 758us/step - loss: 1.0307 - accuracy: 0.6314 - val_loss: 1.2647 - val_accuracy: 0.5533\n",
      "Epoch 131/163\n",
      "70/70 [==============================] - 0s 741us/step - loss: 1.0286 - accuracy: 0.6386 - val_loss: 1.2653 - val_accuracy: 0.5533\n",
      "Epoch 132/163\n",
      "70/70 [==============================] - 0s 764us/step - loss: 1.0222 - accuracy: 0.6343 - val_loss: 1.2576 - val_accuracy: 0.5367\n",
      "Epoch 133/163\n",
      "70/70 [==============================] - 0s 787us/step - loss: 1.0204 - accuracy: 0.6414 - val_loss: 1.2480 - val_accuracy: 0.5567\n",
      "Epoch 134/163\n",
      "70/70 [==============================] - 0s 744us/step - loss: 1.0217 - accuracy: 0.6443 - val_loss: 1.2579 - val_accuracy: 0.5633\n",
      "Epoch 135/163\n",
      "70/70 [==============================] - 0s 754us/step - loss: 1.0197 - accuracy: 0.6400 - val_loss: 1.2572 - val_accuracy: 0.5533\n",
      "Epoch 136/163\n",
      "70/70 [==============================] - 0s 756us/step - loss: 1.0194 - accuracy: 0.6371 - val_loss: 1.2763 - val_accuracy: 0.5633\n",
      "Epoch 137/163\n",
      "70/70 [==============================] - 0s 739us/step - loss: 1.0171 - accuracy: 0.6457 - val_loss: 1.2715 - val_accuracy: 0.5433\n",
      "Epoch 138/163\n",
      "70/70 [==============================] - 0s 741us/step - loss: 1.0123 - accuracy: 0.6457 - val_loss: 1.2868 - val_accuracy: 0.5433\n",
      "Epoch 139/163\n",
      "70/70 [==============================] - 0s 734us/step - loss: 1.0109 - accuracy: 0.6371 - val_loss: 1.2548 - val_accuracy: 0.5633\n",
      "Epoch 140/163\n",
      "70/70 [==============================] - 0s 727us/step - loss: 1.0086 - accuracy: 0.6429 - val_loss: 1.2481 - val_accuracy: 0.5600\n",
      "Epoch 141/163\n",
      "70/70 [==============================] - 0s 722us/step - loss: 1.0054 - accuracy: 0.6343 - val_loss: 1.2769 - val_accuracy: 0.5433\n",
      "Epoch 142/163\n",
      "70/70 [==============================] - 0s 747us/step - loss: 1.0056 - accuracy: 0.6429 - val_loss: 1.2727 - val_accuracy: 0.5633\n",
      "Epoch 143/163\n",
      "70/70 [==============================] - 0s 764us/step - loss: 1.0035 - accuracy: 0.6471 - val_loss: 1.2805 - val_accuracy: 0.5500\n",
      "Epoch 144/163\n",
      "70/70 [==============================] - 0s 738us/step - loss: 1.0023 - accuracy: 0.6400 - val_loss: 1.2790 - val_accuracy: 0.5667\n",
      "Epoch 145/163\n",
      "70/70 [==============================] - 0s 729us/step - loss: 1.0002 - accuracy: 0.6357 - val_loss: 1.2517 - val_accuracy: 0.5600\n",
      "Epoch 146/163\n",
      "70/70 [==============================] - 0s 743us/step - loss: 1.0004 - accuracy: 0.6486 - val_loss: 1.2660 - val_accuracy: 0.5700\n",
      "Epoch 147/163\n",
      "70/70 [==============================] - 0s 722us/step - loss: 0.9948 - accuracy: 0.6457 - val_loss: 1.2546 - val_accuracy: 0.5600\n",
      "Epoch 148/163\n",
      "70/70 [==============================] - 0s 756us/step - loss: 0.9931 - accuracy: 0.6429 - val_loss: 1.2734 - val_accuracy: 0.5700\n",
      "Epoch 149/163\n",
      "70/70 [==============================] - 0s 771us/step - loss: 0.9870 - accuracy: 0.6429 - val_loss: 1.2522 - val_accuracy: 0.5733\n",
      "Epoch 150/163\n",
      "70/70 [==============================] - 0s 749us/step - loss: 0.9931 - accuracy: 0.6571 - val_loss: 1.2595 - val_accuracy: 0.5667\n",
      "Epoch 151/163\n",
      "70/70 [==============================] - 0s 748us/step - loss: 0.9872 - accuracy: 0.6386 - val_loss: 1.2555 - val_accuracy: 0.5667\n",
      "Epoch 152/163\n",
      "70/70 [==============================] - 0s 714us/step - loss: 0.9853 - accuracy: 0.6514 - val_loss: 1.2564 - val_accuracy: 0.5533\n",
      "Epoch 153/163\n",
      "70/70 [==============================] - 0s 753us/step - loss: 0.9839 - accuracy: 0.6500 - val_loss: 1.2651 - val_accuracy: 0.5667\n"
     ]
    }
   ],
   "source": [
    "# early stopping 이후에 적절한 epoch를 찾아서 모델 저장\n",
    "hist=model.fit(X_train, Y_train, epochs=163, batch_size=10, \n",
    "               validation_data=(X_val, Y_val), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "# 에폭 과적합을 20번정도 참은 결과."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체적인 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 1.1430 - accuracy: 0.7202 - val_loss: 0.6508 - val_accuracy: 0.8414\n",
      "Epoch 2/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.5153 - accuracy: 0.8690 - val_loss: 0.4802 - val_accuracy: 0.8694\n",
      "Epoch 3/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4124 - accuracy: 0.8868 - val_loss: 0.4127 - val_accuracy: 0.8851\n",
      "Epoch 4/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3659 - accuracy: 0.8971 - val_loss: 0.3804 - val_accuracy: 0.8937\n",
      "Epoch 5/5\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3382 - accuracy: 0.9056 - val_loss: 0.3557 - val_accuracy: 0.8990\n",
      "313/313 [==============================] - 0s 490us/step - loss: 0.3281 - accuracy: 0.9078\n",
      "\n",
      "loss_and_metrics : [0.32811465859413147, 0.907800018787384]\n",
      "WARNING:tensorflow:From <ipython-input-57-1d4893233842>:47: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "True : 9, Predict : 9\n",
      "True : 9, Predict : 9\n",
      "True : 2, Predict : 2\n",
      "True : 1, Predict : 1\n",
      "True : 9, Predict : 9\n"
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터셋 전처리\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 원핫인코딩 (one-hot encoding) 처리\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[:42000] # 훈련셋의 30%를 검증셋으로 사용\n",
    "x_train = x_train[42000:]\n",
    "y_val = y_train[:42000] # 훈련셋의 30%를 검증셋으로 사용\n",
    "y_train = y_train[42000:]\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('')\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
    "\n",
    "# 6. 모델 사용하기\n",
    "xhat_idx = np.random.choice(x_test.shape[0], 5)\n",
    "xhat = x_test[xhat_idx]\n",
    "yhat = model.predict_classes(xhat)\n",
    "\n",
    "for i in range(5):\n",
    "    print('True : ' + str(argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat[i]))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델을 저장하고 다시 사용하고 싶을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mymnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # parameter: weight, bias 가 저장되는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50240\n",
      "weight: 784*64\n",
      "bias: +64\n"
     ]
    }
   ],
   "source": [
    "print(784*64+64)\n",
    "print(\"weight: 784*64\")\n",
    "print(\"bias: +64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 재탕하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "model=load_model(\"mymnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "xhat_idx = np.random.choice(x_test.shape[0], 5)\n",
    "xhat = x_test[xhat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict_classes(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : 0, Predict : 0\n",
      "True : 4, Predict : 4\n",
      "True : 8, Predict : 8\n",
      "True : 9, Predict : 9\n",
      "True : 6, Predict : 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('True : ' + str(argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat[i]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN을 이용한 수열 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [0.0 0.1 0.2 0.3] -> 0.4 예측\n",
    "\n",
    "- 모든 RNN-cell에서 출력이 있을경우 \n",
    "- 옵션: \n",
    "- return_sequences = True (다 : 다)\n",
    "    - 맨 마지막에 출력할경우 -> return_sequences = False : 감성분석 맨마지막에만 결과 긍정/부정 (다 : 1)\n",
    "- return_state: cell상태 출력\n",
    "- activation = tanh, relu 등등.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn cell을 만드는 방법은 여러가지가 있는데 그중 하나.\n",
    "# SimpleRNN 간단한 rnn 생성\n",
    "# 4개가 들어갔을때 값이 1개가 나오도록 만들기\n",
    "model=Sequential([SimpleRNN(units=1, #출력되어야할 데이터는 1개\n",
    "                  activation='tanh',\n",
    "                  return_sequences=False,\n",
    "                  return_state=True)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. ]\n",
      "  [0.1]\n",
      "  [0.2]\n",
      "  [0.3]]\n",
      "\n",
      " [[0.1]\n",
      "  [0.2]\n",
      "  [0.3]\n",
      "  [0.4]]\n",
      "\n",
      " [[0.2]\n",
      "  [0.3]\n",
      "  [0.4]\n",
      "  [0.5]]\n",
      "\n",
      " [[0.3]\n",
      "  [0.4]\n",
      "  [0.5]\n",
      "  [0.6]]\n",
      "\n",
      " [[0.4]\n",
      "  [0.5]\n",
      "  [0.6]\n",
      "  [0.7]]\n",
      "\n",
      " [[0.5]\n",
      "  [0.6]\n",
      "  [0.7]\n",
      "  [0.8]]]\n",
      "[0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "# 입출력을 이런 순서대로 한단계씩 나아가면서 진행\n",
    "# 전체적으로 트레이닝을 100번 해보기 : 100 epochs\n",
    "X = [] \n",
    "Y = [] \n",
    "for i in range(6): \n",
    "    lst = list(range(i,i+4)) \n",
    "    X.append(list(map(lambda c: [c/10], lst))) \n",
    "    Y.append((i+4)/10) \n",
    "X = np.array(X) \n",
    "Y = np.array(Y) \n",
    "print(X)  #6,4,1 (shape)\n",
    "print(Y)\n",
    "\n",
    "#예측은 전혀 다른 값 주기\n",
    "# 1.0, 1.1, 1.2, 1.3 -> 1.4예측을 원함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # (6건, 4행, 1열)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([SimpleRNN(units=10, #히든계층: 10개\n",
    "                  return_sequences=False,\n",
    "                  input_shape=[4,1]),\n",
    "                 Dense(1)]) #Dense(1), output:1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6209\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 524us/step - loss: 0.5806\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 595us/step - loss: 0.5419\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 560us/step - loss: 0.5047\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 533us/step - loss: 0.4692\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 747us/step - loss: 0.4352\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 549us/step - loss: 0.4028\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 536us/step - loss: 0.3720\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 465us/step - loss: 0.3427\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 540us/step - loss: 0.3149\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 568us/step - loss: 0.2887\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 660us/step - loss: 0.2639\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 658us/step - loss: 0.2405\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 608us/step - loss: 0.2185\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 518us/step - loss: 0.1979\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 560us/step - loss: 0.1786\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 704us/step - loss: 0.1606\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 565us/step - loss: 0.1438\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 648us/step - loss: 0.1282\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 600us/step - loss: 0.1138\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 746us/step - loss: 0.1005\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 535us/step - loss: 0.0883\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 542us/step - loss: 0.0771\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 602us/step - loss: 0.0669\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 630us/step - loss: 0.0576\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 536us/step - loss: 0.0493\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 519us/step - loss: 0.0418\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 575us/step - loss: 0.0351\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 569us/step - loss: 0.0293\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 649us/step - loss: 0.0241\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 580us/step - loss: 0.0196\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 473us/step - loss: 0.0157\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 548us/step - loss: 0.0124\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 530us/step - loss: 0.0097\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 616us/step - loss: 0.0074\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 657us/step - loss: 0.0056\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 591us/step - loss: 0.0041\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 672us/step - loss: 0.0030\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 466us/step - loss: 0.0022\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 559us/step - loss: 0.0016\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 587us/step - loss: 0.0013\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 707us/step - loss: 0.0011\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 488us/step - loss: 0.0011\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 537us/step - loss: 0.0011\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 642us/step - loss: 0.0013\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 442us/step - loss: 0.0015\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 532us/step - loss: 0.0017\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 564us/step - loss: 0.0019\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 591us/step - loss: 0.0022\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 586us/step - loss: 0.0024\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 549us/step - loss: 0.0026\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 548us/step - loss: 0.0027\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 635us/step - loss: 0.0028\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 596us/step - loss: 0.0029\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 623us/step - loss: 0.0030\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 606us/step - loss: 0.0030\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 554us/step - loss: 0.0029\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 558us/step - loss: 0.0029\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 544us/step - loss: 0.0028\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 595us/step - loss: 0.0027\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 564us/step - loss: 0.0025\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 514us/step - loss: 0.0024\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 547us/step - loss: 0.0023\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 455us/step - loss: 0.0021\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 746us/step - loss: 0.0020\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 597us/step - loss: 0.0018\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 551us/step - loss: 0.0017\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 479us/step - loss: 0.0016\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 487us/step - loss: 0.0015\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 531us/step - loss: 0.0014\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 526us/step - loss: 0.0013\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 553us/step - loss: 0.0012\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 786us/step - loss: 0.0011\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 707us/step - loss: 0.0011\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 831us/step - loss: 0.0010\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 961us/step - loss: 9.6603e-04\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.3651e-04\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 627us/step - loss: 9.1442e-04\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 660us/step - loss: 8.9864e-04\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.8805e-04\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.8158e-04\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 617us/step - loss: 8.7824e-04\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7714e-04\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 726us/step - loss: 8.7747e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 755us/step - loss: 8.7856e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 501us/step - loss: 8.7984e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 680us/step - loss: 8.8085e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 507us/step - loss: 8.8127e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 622us/step - loss: 8.8084e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 627us/step - loss: 8.7942e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 571us/step - loss: 8.7693e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.7338e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 690us/step - loss: 8.6881e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 653us/step - loss: 8.6331e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 491us/step - loss: 8.5702e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 614us/step - loss: 8.5007e-04\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 783us/step - loss: 8.4262e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 775us/step - loss: 8.3484e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.2686e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.1883e-04\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X,Y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb1ba1ad940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.97039825]]\n"
     ]
    }
   ],
   "source": [
    "# 6건 4행 1열\n",
    "sooneung=np.array([[[1.0],[1.1],[1.2],[1.3]]])\n",
    "print(model.predict(sooneung))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92008185]]\n"
     ]
    }
   ],
   "source": [
    "sooneung=np.array([[[0.6],[0.7],[0.8],[0.9]]])\n",
    "print(model.predict(sooneung))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6170091]]\n"
     ]
    }
   ],
   "source": [
    "sooneung=np.array([[[0.2],[0.3],[0.4],[0.5]]])\n",
    "print(model.predict(sooneung))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 예제\n",
    "- 입력(거래량, 하한가, 상한가, 시가, 종가 등 데이터) : 2017.1.1~2017.1.31\n",
    "- 출력(종가) : 2017.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN 기반 문장 생성\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"인공지능을 공부하면서 코딩을 하고 있다\\n\n",
    "파이썬 코딩을 배우고 익혔다\\n\n",
    "딥러닝을 배우고 코딩을 하고 있다\\n\n",
    "파이썬 기반에서 판다스를 배우고 코딩을 했다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능을 공부하면서 코딩을 하고 있다\n",
      "\n",
      "파이썬 코딩을 배우고 익혔다\n",
      "\n",
      "딥러닝을 배우고 코딩을 하고 있다\n",
      "\n",
      "파이썬 기반에서 판다스를 배우고 코딩을 했다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "t.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'코딩을': 1,\n",
       " '배우고': 2,\n",
       " '하고': 3,\n",
       " '있다': 4,\n",
       " '파이썬': 5,\n",
       " '인공지능을': 6,\n",
       " '공부하면서': 7,\n",
       " '익혔다': 8,\n",
       " '딥러닝을': 9,\n",
       " '기반에서': 10,\n",
       " '판다스를': 11,\n",
       " '했다': 12}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단순히 띄어쓰기 단위로 피팅..\n",
    "# 형태소 분석 아님\n",
    "#fit 후에..\n",
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '코딩을',\n",
       " 2: '배우고',\n",
       " 3: '하고',\n",
       " 4: '있다',\n",
       " 5: '파이썬',\n",
       " 6: '인공지능을',\n",
       " 7: '공부하면서',\n",
       " 8: '익혔다',\n",
       " 9: '딥러닝을',\n",
       " 10: '기반에서',\n",
       " 11: '판다스를',\n",
       " 12: '했다'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('인공지능을', 1),\n",
       "             ('공부하면서', 1),\n",
       "             ('코딩을', 4),\n",
       "             ('하고', 2),\n",
       "             ('있다', 2),\n",
       "             ('파이썬', 2),\n",
       "             ('배우고', 3),\n",
       "             ('익혔다', 1),\n",
       "             ('딥러닝을', 1),\n",
       "             ('기반에서', 1),\n",
       "             ('판다스를', 1),\n",
       "             ('했다', 1)])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 크기: 13\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝시에 길이를 맞추려고 한다. \n",
    "# 이부분을 다시 공부\n",
    "vocab_size=len(t.word_index)+1\n",
    "print('단어 크기: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): \n",
    "#     print(line)\n",
    "#     print(t.texts_to_sequences([line])) #index가 나온다.\n",
    "#     print(t.texts_to_sequences([line])[0])\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "#     print(encoded)\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7],\n",
       " [6, 7, 1],\n",
       " [6, 7, 1, 3],\n",
       " [6, 7, 1, 3, 4],\n",
       " [5, 1],\n",
       " [5, 1, 2],\n",
       " [5, 1, 2, 8],\n",
       " [9, 2],\n",
       " [9, 2, 1],\n",
       " [9, 2, 1, 3],\n",
       " [9, 2, 1, 3, 4],\n",
       " [5, 10],\n",
       " [5, 10, 11],\n",
       " [5, 10, 11, 2],\n",
       " [5, 10, 11, 2, 1],\n",
       " [5, 10, 11, 2, 1, 12]]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences #입력데이터의 길이가 다 같아야한다. 주로 0으로 패팅한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=(max(len(i) for i in sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=pad_sequences(sequences, maxlen=max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  6,  7],\n",
       "       [ 0,  0,  0,  6,  7,  1],\n",
       "       [ 0,  0,  6,  7,  1,  3],\n",
       "       [ 0,  6,  7,  1,  3,  4],\n",
       "       [ 0,  0,  0,  0,  5,  1],\n",
       "       [ 0,  0,  0,  5,  1,  2],\n",
       "       [ 0,  0,  5,  1,  2,  8],\n",
       "       [ 0,  0,  0,  0,  9,  2],\n",
       "       [ 0,  0,  0,  9,  2,  1],\n",
       "       [ 0,  0,  9,  2,  1,  3],\n",
       "       [ 0,  9,  2,  1,  3,  4],\n",
       "       [ 0,  0,  0,  0,  5, 10],\n",
       "       [ 0,  0,  0,  5, 10, 11],\n",
       "       [ 0,  0,  5, 10, 11,  2],\n",
       "       [ 0,  5, 10, 11,  2,  1],\n",
       "       [ 5, 10, 11,  2,  1, 12]], dtype=int32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sequences[:,:-1]\n",
    "y=sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  6],\n",
       "       [ 0,  0,  0,  6,  7],\n",
       "       [ 0,  0,  6,  7,  1],\n",
       "       [ 0,  6,  7,  1,  3],\n",
       "       [ 0,  0,  0,  0,  5],\n",
       "       [ 0,  0,  0,  5,  1],\n",
       "       [ 0,  0,  5,  1,  2],\n",
       "       [ 0,  0,  0,  0,  9],\n",
       "       [ 0,  0,  0,  9,  2],\n",
       "       [ 0,  0,  9,  2,  1],\n",
       "       [ 0,  9,  2,  1,  3],\n",
       "       [ 0,  0,  0,  0,  5],\n",
       "       [ 0,  0,  0,  5, 10],\n",
       "       [ 0,  0,  5, 10, 11],\n",
       "       [ 0,  5, 10, 11,  2],\n",
       "       [ 5, 10, 11,  2,  1]], dtype=int32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  1,  3,  4,  1,  2,  8,  2,  1,  3,  4, 10, 11,  2,  1, 12],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(y, num_classes=vocab_size) #자리수에 맞추어 원핫인코딩. 0 번 인덱스제외, 1번부터 사용 (케라스특성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 5)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 13)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 임베딩 클래스 사용하면 고차원-> 저차원 -> 효율 증가.\n",
    "- embedding을 통해서 고차원의 벡터를 차원축소를 진행\n",
    "- Embeddeing-> 저차원(5) 공간에 단어 표시 ex) [0.7, -1.5, 2.4, 3.1, 1.0]\n",
    "- embedding -> Turns positive integer(indexes) into dense vectors of fixed size\n",
    "- https://keras.io/api/layers/core_layers/embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential(Embedding(vocab_size, 10, input_length=max_len-1)) # 13차원 -> 10차원으로 임베딩\n",
    "model.add(SimpleRNN(32)) #출력 차원 (rnn cell: 하이퍼파라미터)\n",
    "model.add(Dense(vocab_size, activation='softmax')) #출력 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5624 - accuracy: 0.1250\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 616us/step - loss: 2.5496 - accuracy: 0.1250\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 659us/step - loss: 2.5371 - accuracy: 0.1250\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 747us/step - loss: 2.5245 - accuracy: 0.1250\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 574us/step - loss: 2.5119 - accuracy: 0.1250\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 640us/step - loss: 2.4992 - accuracy: 0.3125\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 573us/step - loss: 2.4862 - accuracy: 0.3125\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 607us/step - loss: 2.4729 - accuracy: 0.5625\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 585us/step - loss: 2.4593 - accuracy: 0.5625\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 622us/step - loss: 2.4452 - accuracy: 0.5625\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 623us/step - loss: 2.4306 - accuracy: 0.5625\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 555us/step - loss: 2.4154 - accuracy: 0.5625\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 603us/step - loss: 2.3996 - accuracy: 0.5625\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 736us/step - loss: 2.3831 - accuracy: 0.5625\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 586us/step - loss: 2.3659 - accuracy: 0.5000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 681us/step - loss: 2.3480 - accuracy: 0.5000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 642us/step - loss: 2.3295 - accuracy: 0.5000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 603us/step - loss: 2.3102 - accuracy: 0.5000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 596us/step - loss: 2.2903 - accuracy: 0.5000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 561us/step - loss: 2.2697 - accuracy: 0.5000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 859us/step - loss: 2.2487 - accuracy: 0.4375\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 668us/step - loss: 2.2272 - accuracy: 0.4375\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 612us/step - loss: 2.2055 - accuracy: 0.4375\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 691us/step - loss: 2.1836 - accuracy: 0.3750\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 588us/step - loss: 2.1618 - accuracy: 0.3750\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 658us/step - loss: 2.1403 - accuracy: 0.2500\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 846us/step - loss: 2.1191 - accuracy: 0.2500\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 626us/step - loss: 2.0985 - accuracy: 0.2500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 658us/step - loss: 2.0786 - accuracy: 0.2500\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 562us/step - loss: 2.0595 - accuracy: 0.2500\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 637us/step - loss: 2.0411 - accuracy: 0.2500\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 685us/step - loss: 2.0235 - accuracy: 0.2500\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 633us/step - loss: 2.0064 - accuracy: 0.2500\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 648us/step - loss: 1.9897 - accuracy: 0.2500\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 659us/step - loss: 1.9733 - accuracy: 0.2500\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 642us/step - loss: 1.9568 - accuracy: 0.3125\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 671us/step - loss: 1.9402 - accuracy: 0.3125\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 626us/step - loss: 1.9234 - accuracy: 0.3750\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 545us/step - loss: 1.9063 - accuracy: 0.3750\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 611us/step - loss: 1.8888 - accuracy: 0.5000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 648us/step - loss: 1.8711 - accuracy: 0.5625\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 616us/step - loss: 1.8530 - accuracy: 0.6250\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 610us/step - loss: 1.8346 - accuracy: 0.6250\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 683us/step - loss: 1.8160 - accuracy: 0.6875\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 623us/step - loss: 1.7971 - accuracy: 0.6875\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 628us/step - loss: 1.7779 - accuracy: 0.6875\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 569us/step - loss: 1.7585 - accuracy: 0.6875\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 604us/step - loss: 1.7389 - accuracy: 0.6875\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 676us/step - loss: 1.7189 - accuracy: 0.6875\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 589us/step - loss: 1.6987 - accuracy: 0.6875\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 541us/step - loss: 1.6782 - accuracy: 0.6875\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 611us/step - loss: 1.6575 - accuracy: 0.6875\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 581us/step - loss: 1.6366 - accuracy: 0.6875\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 826us/step - loss: 1.6155 - accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 646us/step - loss: 1.5942 - accuracy: 0.6875\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 831us/step - loss: 1.5729 - accuracy: 0.6875\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 725us/step - loss: 1.5515 - accuracy: 0.6875\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 655us/step - loss: 1.5302 - accuracy: 0.6875\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 812us/step - loss: 1.5089 - accuracy: 0.6875\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 646us/step - loss: 1.4877 - accuracy: 0.6875\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 723us/step - loss: 1.4667 - accuracy: 0.6875\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 638us/step - loss: 1.4459 - accuracy: 0.6875\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 698us/step - loss: 1.4252 - accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 676us/step - loss: 1.4048 - accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 738us/step - loss: 1.3847 - accuracy: 0.6875\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 562us/step - loss: 1.3648 - accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 817us/step - loss: 1.3453 - accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 624us/step - loss: 1.3260 - accuracy: 0.6875\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 694us/step - loss: 1.3070 - accuracy: 0.6875\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 971us/step - loss: 1.2883 - accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 767us/step - loss: 1.2699 - accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 721us/step - loss: 1.2518 - accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 777us/step - loss: 1.2341 - accuracy: 0.6875\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 994us/step - loss: 1.2167 - accuracy: 0.6875\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 744us/step - loss: 1.1995 - accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 633us/step - loss: 1.1827 - accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 668us/step - loss: 1.1662 - accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 666us/step - loss: 1.1500 - accuracy: 0.6875\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 552us/step - loss: 1.1341 - accuracy: 0.6875\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 685us/step - loss: 1.1185 - accuracy: 0.6875\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 900us/step - loss: 1.1032 - accuracy: 0.6875\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 736us/step - loss: 1.0881 - accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 574us/step - loss: 1.0733 - accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 636us/step - loss: 1.0587 - accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0443 - accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 742us/step - loss: 1.0302 - accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 651us/step - loss: 1.0163 - accuracy: 0.6875\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0026 - accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 612us/step - loss: 0.9890 - accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 603us/step - loss: 0.9757 - accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 636us/step - loss: 0.9625 - accuracy: 0.7500\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 768us/step - loss: 0.9495 - accuracy: 0.7500\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 603us/step - loss: 0.9367 - accuracy: 0.7500\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 556us/step - loss: 0.9240 - accuracy: 0.7500\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 793us/step - loss: 0.9114 - accuracy: 0.7500\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 843us/step - loss: 0.8990 - accuracy: 0.7500\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 703us/step - loss: 0.8867 - accuracy: 0.7500\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 800us/step - loss: 0.8746 - accuracy: 0.7500\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 600us/step - loss: 0.8626 - accuracy: 0.7500\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 652us/step - loss: 0.8507 - accuracy: 0.7500\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 602us/step - loss: 0.8390 - accuracy: 0.7500\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 708us/step - loss: 0.8274 - accuracy: 0.7500\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 757us/step - loss: 0.8159 - accuracy: 0.7500\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 800us/step - loss: 0.8045 - accuracy: 0.8125\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7933 - accuracy: 0.8125\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 575us/step - loss: 0.7822 - accuracy: 0.8125\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 753us/step - loss: 0.7712 - accuracy: 0.8125\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 600us/step - loss: 0.7603 - accuracy: 0.8125\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 757us/step - loss: 0.7496 - accuracy: 0.8125\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 645us/step - loss: 0.7390 - accuracy: 0.8125\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 667us/step - loss: 0.7285 - accuracy: 0.8125\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 667us/step - loss: 0.7181 - accuracy: 0.8125\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 682us/step - loss: 0.7079 - accuracy: 0.8125\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 893us/step - loss: 0.6978 - accuracy: 0.8125\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 657us/step - loss: 0.6878 - accuracy: 0.8125\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 710us/step - loss: 0.6779 - accuracy: 0.8125\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 603us/step - loss: 0.6682 - accuracy: 0.8125\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 726us/step - loss: 0.6586 - accuracy: 0.8125\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 726us/step - loss: 0.6491 - accuracy: 0.8125\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 669us/step - loss: 0.6398 - accuracy: 0.8125\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 620us/step - loss: 0.6306 - accuracy: 0.8750\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 883us/step - loss: 0.6215 - accuracy: 0.8750\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6125 - accuracy: 0.8750\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 788us/step - loss: 0.6037 - accuracy: 0.8750\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 668us/step - loss: 0.5950 - accuracy: 0.8750\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 837us/step - loss: 0.5864 - accuracy: 0.8750\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 727us/step - loss: 0.5780 - accuracy: 0.8750\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 547us/step - loss: 0.5696 - accuracy: 0.8750\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 727us/step - loss: 0.5614 - accuracy: 0.8750\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 694us/step - loss: 0.5534 - accuracy: 0.8750\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 714us/step - loss: 0.5454 - accuracy: 0.8750\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 720us/step - loss: 0.5376 - accuracy: 0.8750\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 663us/step - loss: 0.5299 - accuracy: 0.8750\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.8750\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 871us/step - loss: 0.5149 - accuracy: 0.8750\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 605us/step - loss: 0.5076 - accuracy: 0.8750\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 747us/step - loss: 0.5004 - accuracy: 0.8750\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 679us/step - loss: 0.4934 - accuracy: 0.8750\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 723us/step - loss: 0.4864 - accuracy: 0.8750\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 652us/step - loss: 0.4796 - accuracy: 0.8750\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 652us/step - loss: 0.4729 - accuracy: 0.8750\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 708us/step - loss: 0.4663 - accuracy: 0.8750\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 757us/step - loss: 0.4598 - accuracy: 0.8750\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 641us/step - loss: 0.4535 - accuracy: 0.8750\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 912us/step - loss: 0.4472 - accuracy: 0.8750\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 650us/step - loss: 0.4411 - accuracy: 0.8750\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 675us/step - loss: 0.4351 - accuracy: 0.8750\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 603us/step - loss: 0.4293 - accuracy: 0.8750\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 720us/step - loss: 0.4235 - accuracy: 0.8750\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 597us/step - loss: 0.4178 - accuracy: 0.8750\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 722us/step - loss: 0.4123 - accuracy: 0.8750\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 728us/step - loss: 0.4068 - accuracy: 0.8750\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 602us/step - loss: 0.4015 - accuracy: 0.8750\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 671us/step - loss: 0.3963 - accuracy: 0.8750\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 730us/step - loss: 0.3912 - accuracy: 0.8750\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 541us/step - loss: 0.3862 - accuracy: 0.8750\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 855us/step - loss: 0.3813 - accuracy: 0.8750\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 775us/step - loss: 0.3764 - accuracy: 0.8750\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 903us/step - loss: 0.3717 - accuracy: 0.8750\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 731us/step - loss: 0.3671 - accuracy: 0.8750\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 709us/step - loss: 0.3626 - accuracy: 0.8750\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 739us/step - loss: 0.3582 - accuracy: 0.8750\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 593us/step - loss: 0.3539 - accuracy: 0.8750\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 679us/step - loss: 0.3496 - accuracy: 0.8750\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 685us/step - loss: 0.3455 - accuracy: 0.8750\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 697us/step - loss: 0.3415 - accuracy: 0.8750\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 637us/step - loss: 0.3375 - accuracy: 0.8750\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 719us/step - loss: 0.3336 - accuracy: 0.8750\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 645us/step - loss: 0.3298 - accuracy: 0.8750\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 669us/step - loss: 0.3261 - accuracy: 0.8750\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 571us/step - loss: 0.3225 - accuracy: 0.8750\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 650us/step - loss: 0.3189 - accuracy: 0.8750\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 626us/step - loss: 0.3154 - accuracy: 0.8750\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 738us/step - loss: 0.3120 - accuracy: 0.9375\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 704us/step - loss: 0.3087 - accuracy: 0.9375\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 783us/step - loss: 0.3054 - accuracy: 0.9375\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 682us/step - loss: 0.3022 - accuracy: 0.9375\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 760us/step - loss: 0.2991 - accuracy: 0.9375\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 673us/step - loss: 0.2960 - accuracy: 0.9375\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 588us/step - loss: 0.2930 - accuracy: 0.9375\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 720us/step - loss: 0.2901 - accuracy: 0.9375\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 685us/step - loss: 0.2872 - accuracy: 0.9375\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 711us/step - loss: 0.2844 - accuracy: 0.9375\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 720us/step - loss: 0.2817 - accuracy: 0.9375\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 584us/step - loss: 0.2790 - accuracy: 0.9375\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 664us/step - loss: 0.2764 - accuracy: 0.9375\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 767us/step - loss: 0.2738 - accuracy: 0.9375\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 676us/step - loss: 0.2712 - accuracy: 0.9375\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 614us/step - loss: 0.2688 - accuracy: 0.9375\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 728us/step - loss: 0.2663 - accuracy: 0.9375\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 841us/step - loss: 0.2639 - accuracy: 0.9375\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 709us/step - loss: 0.2616 - accuracy: 0.9375\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 799us/step - loss: 0.2593 - accuracy: 0.9375\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2571 - accuracy: 0.9375\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2549 - accuracy: 0.9375\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 600us/step - loss: 0.2527 - accuracy: 0.9375\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 679us/step - loss: 0.2506 - accuracy: 0.9375\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 939us/step - loss: 0.2485 - accuracy: 0.9375\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 686us/step - loss: 0.2465 - accuracy: 0.9375\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 635us/step - loss: 0.2445 - accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb1bd94edc0>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam', #adam or rmsprop 논문에서 전체적으로 adam이 무난하다\n",
    "             metrics=['accuracy'])\n",
    "model.fit(X,y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 6]], dtype=int32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences([[6]], maxlen=5, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼산택 코딩을 배우고 코딩을 하고 있다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '삼산택', 5)) #인공지능을에 해당하는 index\n",
    "# 출력하려면 모델링할때 만든 사이즈에 맞추어서 입력해 주어야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '코딩을',\n",
       " 2: '배우고',\n",
       " 3: '하고',\n",
       " 4: '있다',\n",
       " 5: '파이썬',\n",
       " 6: '인공지능을',\n",
       " 7: '공부하면서',\n",
       " 8: '익혔다',\n",
       " 9: '딥러닝을',\n",
       " 10: '기반에서',\n",
       " 11: '판다스를',\n",
       " 12: '했다'}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t.word_index\n",
    "# t.index_word\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

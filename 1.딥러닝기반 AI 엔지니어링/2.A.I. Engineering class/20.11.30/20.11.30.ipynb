{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20.11.30.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOp5lyCkErt4W5VM2J+hbwc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"u0C3b8wZmqpM"},"source":["Tagging / Seq2Seq"]},{"cell_type":"code","metadata":{"id":"XbTprh1wmoKU"},"source":["import nltk\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvOj7JuRmqPa"},"source":["nltk.download('treebank')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AHJSqG1nv-Y"},"source":["nltk.corpus.treebank.tagged_sents()  # 2차원 구조인 것을 확인 [[]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMhkRj-dnx8C"},"source":["tagged_sentences = nltk.corpus.treebank.tagged_sents() # 토큰화에 품사 태깅이 된 데이터 받아오기\n","print(\"품사 태깅이 된 문장 개수: \", len(tagged_sentences)) # 문장 샘플의 개수 출력"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtlV4Ye-nz1g"},"source":["tagged_sentences[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEkVrpNNoVlG"},"source":["tagged_sentences[3913]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEn1HpHfoatu"},"source":["from itertools import zip_longest\n","# 요소간에 데이터 연결 (숫자/문자)가능\n","x = [1, 2, 3]\n","y = [4, 5, 6]\n"," \n","zipped = zip(x, y)\n","print(list(zipped)) \n","'''\n","결과\n","[(1, 4), (2, 5), (3, 6), (None, 7)]\n","'''\n"," \n","# 2) fillvalue= 인자에 값을 지정\n","zipped = zip_longest(x, y, fillvalue=0)\n","print(list(zipped)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BBK8Nj2sbJF"},"source":["sentences, pos_tags = [], [] \n","for tagged_sentence in tagged_sentences: # 3,914개의 문장 샘플을 1개씩 불러온다.\n","    sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 품사 태깅 정보들은 tag_info에 저장한다.\n","    sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\n","    pos_tags.append(list(tag_info)) # 각 샘플에서 품사 태깅 정보만 저장한다. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNgCvDwytb4Z"},"source":["print(sentences[0])\n","print(pos_tags[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5UnGgJZteXL"},"source":["len(pos_tags)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLJurOaBu0tZ"},"source":["# sentences 최대 길이와 평균길이를 출력\n","print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n","print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\n","plt.hist([len(s) for s in sentences], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OBgyu0wWvUnh"},"source":["def tokenize(samples):\n","  tokenizer = Tokenizer()\n","  tokenizer.fit_on_texts(samples)\n","  return tokenizer "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uh8VNzgQvfxZ"},"source":["src_tokenizer = tokenize(sentences)\n","tar_tokenizer = tokenize(pos_tags)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tGx4Tablv0qe"},"source":["src_tokenizer.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CoQLNf5v5PI"},"source":["vocab_size = len(src_tokenizer.word_index)+1 # 11387+1    # oov처리할때 +2 하고 여기서는 1번부터 시작해서 +1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A_X9DJL6v8VA"},"source":["tag_size = len(tar_tokenizer.word_index)+1 # 11387+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkz_I_zrwGkH"},"source":["X_train = src_tokenizer.texts_to_sequences(sentences)\n","y_train = tar_tokenizer.texts_to_sequences(pos_tags) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fiEbTGDtw8C5"},"source":["y_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usQvcvrVw_uo"},"source":["max_len = 100\n","X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n","# X_train의 모든 샘플의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\n","y_train = pad_sequences(y_train, padding='post', maxlen=max_len)\n","# y_train의 모든 샘플의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lXHSXa-xANL"},"source":["# one-hot 인코딩\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777) \n","y_train = to_categorical(y_train, num_classes=tag_size)\n","y_test = to_categorical(y_test, num_classes=tag_size) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VzKUxwZxG6e"},"source":["print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n","print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n","print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n","print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OS38SnqaxRpn"},"source":["from keras.models import Sequential\n","from keras.layers import Dense, LSTM, InputLayer, Bidirectional, Embedding\n","from keras.optimizers import Adam "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhdu8-Cpxwpt"},"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 128, input_length=max_len, mask_zero=True))\n","model.add(Bidirectional(LSTM(256, return_sequences=True)))\n","model.add(Dense(tag_size, activation=('softmax')))\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy']) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ECa_d3Fx3z8"},"source":["mask_zero = True  \n","0 으로 패딩된 값을 마스킹하여 네트워크의 뒤로 전달되지 않게 함.  \n","인위적으로 패딩된 부분이 학습에 형향을 미치지 않도록 하겠다는 의미임.  \n"]},{"cell_type":"code","metadata":{"id":"AtMHNhTnx2il"},"source":["model.fit(X_train, y_train, batch_size=128, epochs=6,  validation_data=(X_test, y_test)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfDXSQ_Lyidg"},"source":["print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qm1zusYzQCr"},"source":["index_to_word=src_tokenizer.index_word\n","index_to_tag=tar_tokenizer.index_word "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ax68lBNRzsyk"},"source":["X_test[10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZaj-komztIy"},"source":["y_test[10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hakAIYIY00yS"},"source":["index_to_word=src_tokenizer.index_word\n","index_to_tag=tar_tokenizer.index_word\n","\n","i=10 # 확인하고 싶은 테스트용 샘플의 인덱스.\n","y_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n","y_predicted = np.argmax(y_predicted, axis=-1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n","true = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n","\n","print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n","print(35 * \"-\")\n","\n","for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n","    if w != 0: # PAD값은 제외함.\n","        print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_tag[t].upper(), index_to_tag[pred].upper())) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VeSMjNRT6dnN"},"source":["### 개채명 인식"]},{"cell_type":"code","metadata":{"id":"4vBMxrFd5KO9"},"source":["nltk.download('maxent_ne_chunker')\n","nltk.download('words') nltk.download('punkt') nltk.download('averaged_perceptron_tagger') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PE_bZ-96ZsE"},"source":["from nltk import word_tokenize, pos_tag, ne_chunk "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"poBKP47d6bmL"},"source":["sentence = \"Jinwon is working at Samsung in Suwon\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DR-33lQ6jSF"},"source":["word_tokenize(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpC2yXV56nLW"},"source":["pos_tag(word_tokenize(sentence))\n","# NNP : 고유명사"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3eeyCETT6w0P"},"source":[""],"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {} # 파이썬의 dictionary 자료형\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "    result = []\n",
    "\n",
    "    for word in sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    sentences.append(result) \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#빈도수 기준으로 내림차순 정렬\n",
    "#sorted\n",
    "vocab.items() # items()함수가 딕셔너리의 내용을 튜플 형식으로 묶어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('word', 2),\n",
       " ('went', 1),\n",
       " ('secret', 6),\n",
       " ('person', 3),\n",
       " ('mountain', 1),\n",
       " ('knew', 1),\n",
       " ('kept', 4),\n",
       " ('keeping', 2),\n",
       " ('huge', 5),\n",
       " ('good', 1),\n",
       " ('driving', 1),\n",
       " ('crazy', 1),\n",
       " ('barber', 8)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.items()) #키를 기준으로 오름차순 정렬\n",
    "sorted(vocab.items(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8),\n",
       " ('secret', 6),\n",
       " ('huge', 5),\n",
       " ('kept', 4),\n",
       " ('person', 3),\n",
       " ('word', 2),\n",
       " ('keeping', 2),\n",
       " ('good', 1),\n",
       " ('knew', 1),\n",
       " ('driving', 1),\n",
       " ('crazy', 1),\n",
       " ('went', 1),\n",
       " ('mountain', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#키가 아니라 수치값을 기준으로 정렬하고 싶은경우 직접 키를 지정해주어야 한다.\n",
    "vocab_sorted = sorted(vocab.items(), reverse=True, key=lambda x: x[1]) \n",
    "vocab_sorted\n",
    "# key:정렬 기준, x에는 튜플이 전달됨, 키를 수치값으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "#가장 빈도수가 높은 단어들부터 인덱스가 순차적으로 부여\n",
    "word_to_index = {}\n",
    "i=0\n",
    "for (word, freq) in vocab_sorted:\n",
    "    if freq>1:\n",
    "        i+=1\n",
    "        word_to_index[word]=i\n",
    "print(word_to_index) #가장 빈도수가 높은 단어들부터 인덱스가 순차적으로 부여됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barber', 'secret', 'huge', 'kept', 'person']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#빈도수가 가장 높은 vs개 단어 추출\n",
    "vs = 5\n",
    "# vs개 만큼의 단어들만 추출\n",
    "[w[0] for w in word_to_index.items() if w[1] <= vs] #이 방법이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word', 'keeping']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스가 5를 넘는 단어 제거\n",
    "words_freq=[w for w, c in word_to_index.items() if c>vs]\n",
    "words_freq\n",
    "# vs개 만큼의 단어들만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "for w in words_freq:\n",
    "    del(word_to_index[w])\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences #워드 토큰화가 되어진 문장\n",
    "#안에 단어들을 word_to_index 의 인덱스 값과 매칭시켜서 숫자로 변환\n",
    "#sentences(자연어처리대상 텍스트)의 단어 중에서 word_to_index(코퍼스, Vocabulary)에 없는 단어는 OOV(Out Of Vocab.)\n",
    "word_to_index['OOV']=len(word_to_index)+1 #인코딩 전에 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences #아래 구조는 유지하고 모두 숫자로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded=[]\n",
    "for s in sentences:\n",
    "    temp=[]\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:  # <- 에러의 제목 발생시 아래것을 수행\n",
    "            temp.append(word_to_index['OOV']) \n",
    "    encoded.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 6, 5],\n",
       " [1, 3, 5],\n",
       " [6, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [6, 6, 3, 2, 6, 1, 6],\n",
       " [1, 6, 3, 6]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩: \n",
    "- 위 encoded에서 가장 길이가긴 [6, 6, 3, 2, 6, 1, 6] = 7 을 찾은후에 나머지 모든 리스트의 길이를 동일하게(7) 맞추는 작업\n",
    "- 동일한 크기로 해야하는 이유: 딥러닝 네트워크를 만들때, 구조를 동일하게 만들어 주는 작업이 필요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#함수를 활용하는 방법:\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [] #에 sentences데이터를 1차원으로 저장\n",
    "for li in sentences:\n",
    "    for word in li:\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber' 'person' 'barber' 'good' 'person' 'barber' 'huge' 'person'\n",
      " 'knew' 'secret' 'secret' 'kept' 'huge' 'secret' 'huge' 'secret' 'barber'\n",
      " 'kept' 'word' 'barber' 'kept' 'word' 'barber' 'kept' 'secret' 'keeping'\n",
      " 'keeping' 'huge' 'secret' 'driving' 'barber' 'crazy' 'barber' 'went'\n",
      " 'huge' 'mountain']\n"
     ]
    }
   ],
   "source": [
    "words=np.hstack(sentences) #스텍함수 사용\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "words=sum(sentences,[])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 함수를 활용해서 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab['barber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs=5\n",
    "vocab=vocab.most_common(vs) #가장 빈도가 높은 순서대로 정렬 + top5\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "word_to_index={}\n",
    "for (w, f) in vocab:\n",
    "    i+=1\n",
    "    word_to_index[w]=i\n",
    "    \n",
    "word_to_index['OOV']=len(word_to_index)+1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex) 법령코퍼스 = {'헌법':1, '피고인':2,....., 'OOV':1000}  \n",
    "OOV : 자연어, 물, 컵 (__특정 코퍼스와 관련되지 않는 단어들__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=FreqDist(np.hstack(sentences))\n",
    "vocab\n",
    "vocab['barber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=vocab.most_common(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{w[0] : i+1 for i, w in enumerate (vocab)} #dictionary 컴프리핸션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩\n",
    "- 문장의 길이를 가장 긴문장과 동일하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded=tokenizer.texts_to_sequences(sentences)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#가장 긴 길이를 출력 : 7\n",
    "max_len=max([len(i) for i in encoded])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5, 0, 0, 0, 0, 0], [1, 8, 5, 0, 0, 0, 0], [1, 3, 5, 0, 0, 0, 0], [9, 2, 0, 0, 0, 0, 0], [2, 4, 3, 2, 0, 0, 0], [3, 2, 0, 0, 0, 0, 0], [1, 4, 6, 0, 0, 0, 0], [1, 4, 6, 0, 0, 0, 0], [1, 4, 2, 0, 0, 0, 0], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#한가지 방법을 정하기. \n",
    "#좌측 패딩, 우측 패딩\n",
    "#오른쪽패딩일 경우 0이 오른쪽으로 채워짐 : [1, 5, 0, 0, 0, 0, 0]\n",
    "#주로 우측을 많이 사용함. 이유: RNN할때 왼쪽부터 읽음\n",
    "for item in encoded:\n",
    "    while len(item)<max_len:\n",
    "        item.append(0)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#오후 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded=tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(encoded)\n",
    "\n",
    "pad_sequences(encoded, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0],\n",
       "       [ 3,  2,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0],\n",
       "       [ 3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0]], dtype=int32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(encoded, padding='post', maxlen=5) #maxlen:문서의 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5, 14, 14, 14],\n",
       "       [ 1,  8,  5, 14, 14],\n",
       "       [ 1,  3,  5, 14, 14],\n",
       "       [ 9,  2, 14, 14, 14],\n",
       "       [ 2,  4,  3,  2, 14],\n",
       "       [ 3,  2, 14, 14, 14],\n",
       "       [ 1,  4,  6, 14, 14],\n",
       "       [ 1,  4,  6, 14, 14],\n",
       "       [ 1,  4,  2, 14, 14],\n",
       "       [ 3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13, 14]], dtype=int32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(encoded, padding='post', maxlen=5, value=14) #maxlen:문서의 최대 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원핫인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run, runs가 의미는 같은데 다른 단어로 인식된다.\n",
    "- 그러므로 원핫 인코딩을 하기전에는 단어집합이 만들어져 있어야한다.\n",
    "- 단어집합 (자연어, 처리, 파이썬,...) -> 정수인코딩(1,2,3,...) -> 원핫인코딩(단어 집합 크기를 벡터의 차원으로 간주)\n",
    "- 10차원 벡터 (단어가 10개, 인덱스: 1~10번 까지)에서 해당 단어의 위치에는 1을 주고, 나머지에는 0을 주는 방식으로 단어를 벡터 형태로 표현하게 됨.\n",
    "```\n",
    "자연어:1, 처리:2, 파이썬:3, .... 물:10  \n",
    "자연어  -> 10000000000  \n",
    "처리   -> 0100000000  \n",
    "파이썬  -> 0010000000  \n",
    "...  \n",
    "물     -> 0000000001  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "#Okt 객체를 생성\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 자연어 처리를 배운다\") #형태소 분석\n",
    "print(token) #나누어진 단어의 형태 = 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "# {나: 0,..., 배운다: 5}\n",
    "word2index={}\n",
    "for v in token:\n",
    "    if v not in word2index.keys():\n",
    "        word2index[v]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(word, word2index):\n",
    "    n=word2index[word] #2\n",
    "    encode_init=np.zeros(len(word2index))\n",
    "    encode_init[n] = 1\n",
    "    return encode_init.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(ohe(\"자연어\", word2index)) #인덱스는 2일때 자연어: 00100 로 결과를 리턴하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선생님\n",
    "def ohe(word, word2index):\n",
    "    ohv=[0]*len(word2index)\n",
    "    idx=word2index[word]\n",
    "    ohv[idx]=1\n",
    "    return ohv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(ohe(\"자연어\", word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"나랑 코엑스에 브로슈어 가지러 갈래 갈래 갈래 브로슈어 가지러 가자\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=Tokenizer()\n",
    "tok\n",
    "tok.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '갈래', 2: '브로슈어', 3: '가지러', 4: '나랑', 5: '코엑스에', 6: '가자'}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'갈래': 1, '브로슈어': 2, '가지러': 3, '나랑': 4, '코엑스에': 5, '가자': 6}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index #단어집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=\"코엑스에 갈래 브로슈어 가지러 함께 가자 어서\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 1, 2, 3, 6]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded=tok.texts_to_sequences([text2])[0] #정수 인코딩\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OneHot encoding\n",
    "oh=to_categorical(encoded) \n",
    "oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원핫인코딩의 한계:\n",
    "- 단어의 개수가 많아지면 많아질수록 벡터의 차원이 늘어남 (부담스럽다) : 단어의 개수 = 벡터차원 개수\n",
    "- 메모리 관점에서는 효율이 매우 떨어진다.\n",
    "- 유사도 구하기가 어렵다.\n",
    "\n",
    "```\n",
    "국어=[1000]\n",
    "수학=[0100]\n",
    "과학=[0010]\n",
    "영어=[0001]\n",
    "4차원 벡터\n",
    "여기서 유사도를 찾기가 매우 어렵다.\n",
    "\n",
    "ex) 검색어 : 제주도 숙박(숙식, 호텔, 모텔, 게스트하우스, 잠잘곳,... <매우 많은 표현>)\n",
    "연관검색 엔진을 원핫으로 만들면 썩 좋지 않다.\n",
    "```\n",
    "- 이런 문제를 해결하기 위해서는 단어들이 가지고 있는 의미를 반영해서 벡터화 진행\n",
    "    - 단어의 의미를 부여하여 벡터화 (word2vec, fasttext(요즘 사용트렌드), lsa, lda...)\n",
    "        - 자연어 처리를 rnn 을 넘어가서 배우면 fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 언어 모델?\n",
    "- 단어의 순서에 확률을 적용한 모델\n",
    "    - 통계적 기법 (ex: 베이지안)\n",
    "        - 이전 단어 등장시 다음 단어를 예측\n",
    "    - 신경망 기법 (ex-> LSTM, BERT)\n",
    "        - 이전 단어와 다음 단어로부터 현재 단어를 예측(BERT)\n",
    "        - 통계적 기법보다 성능이 더 좋다. \n",
    "        - (BERT이용해서 프로젝트 진행시 논문급)\n",
    "        - ex) 코로나 ??? 결과가 음성입니다  (??? => 검사) 예측결과 \n",
    "\n",
    "- 어느정도 내공이 쌓이면 논문을 봐야한다. 책에는 더이상 없다..\n",
    "\n",
    "***\n",
    "\n",
    "#### 통계적 기법:\n",
    "```\n",
    "번역: \n",
    "- 먹는다가 나올 확률이 마신다 보다 크다.\n",
    "P(나는 밥을 먹는다) >> P(나는 밥을 마신다)\n",
    "\n",
    "- 손님~ 커피가\n",
    "P(나왔습니다) >> P(나오셨습니다)\n",
    ": \"손님~ 커피가 나왔습니다\"\n",
    "\n",
    "- 두문장중 더 확률이 높은 문장?\n",
    "나는 밥을 먹습니다. (50%)\n",
    "나는 날을 먹습니다. (10%)\n",
    "나는 달을 먹습니다. (0.5%)\n",
    "\n",
    "\n",
    "단어(w), 시퀀스(W) 단어가 순서대로 나열\n",
    "다어가 n개, n개 나어가 단어-시퀀스 w 를 보일 확률\n",
    "\n",
    "*다음단어 예측 확률*\n",
    "n-1개 단어가 주어짐,\n",
    "n번째 단어는 무엇이 올까?\n",
    "p(Wn|W1,W2,...Wn-1)\n",
    "\n",
    "*전체단어 시퀀스 w의 확률?\n",
    "p(W) = n(pi)i=1 * (Wn|W1,...,Wn-1)\n",
    "\n",
    "\n",
    "P(B|A) = P(A,B) / P(A)\n",
    "P(A,B) = P(A) * P(B|A)\n",
    "\n",
    "P(A,B,C,D) = P(A)*P(B|A) * P(C|A,B) * P(D|A,B,C)\n",
    "P(X1,...,Xn) = P(X1)... ... P(Xn|X1...Xn-1)\n",
    "\n",
    "P(\"An adorable little boy is spreading smiles\") = 문장이 될 확률\n",
    "\n",
    "- 문장의 확률? 이전 단어가 왔을때 다음 단어로 등장할 확률\n",
    "- P(A|B) = 사건 B가 발생했을때 A가 나올 확률\n",
    "\n",
    "- an이 나올 확률 * an이라는 단어가 나왔을때 그 다음단어로 adorable이 나올확률... 연쇄적으로 확률 계산: \n",
    "- P(An) * P(adorable|An) * P(little|An adorable) *...* P(smiles|An adorable little boy is spreading)\n",
    "\n",
    "카운트기반 문장 확률\n",
    "P(is | An adorable little boy) =? \n",
    "```\n",
    "$$ count(An\\;adorable\\;little\\;boy\\;is) \\over count(An\\;adorable\\;little\\;boy) $$\n",
    "- count(An adorable little boy) => 전체 문장에서 이 문장이 나온 수\n",
    "\n",
    "###### N-gram 언어 모델\n",
    "- n개 만큼의 단어들을 참조해서 그 다음단어가 무엇이 나올지 예상\n",
    "- n => 1,2,3,4... 여러개의 숫자가 올수 있다.\n",
    "    - n=1 <- 유니그램\n",
    "    - n=2 <- 바이그램\n",
    "    - n=3 <- 트라이그램\n",
    "        - 일반적으로 바이그램, 트리아그램이 많이 사용된다 (논문 4이하).\n",
    "        - 신경망에서 word2vec할때 나온다.\n",
    "- ex) \"An adorable little boy is spreading\" + \"???\"(예상)\n",
    "$$ P(w|boy\\;is\\;spreading) = count(boy\\;is\\;spreading\\;w) \\over count(boy\\;is\\;spreading) $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 신경망 기법:\n",
    "- \n",
    "\n",
    "___\n",
    "- rnn 개선: GRU(조경현교수님 개발), LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BOW)\n",
    "- 단어 출현 순서 고려 X\n",
    "- 빈도수 기반 단어 표현\n",
    "    1. 문서의 모든 단어 추출 -> index\n",
    "    2. 단어의 등장 횟수 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram 유사도 문제\n",
    "> ex:  \n",
    "`문장1`: 오늘 강남에서 맛있는 된장찌개를 먹었다.  \n",
    "`문장2`: 강남에서 먹었던 오늘의 된장찌개는 맛있었다.\n",
    "\n",
    "- 2 gram유사도\n",
    "    - 오늘/늘(공백)/(공백)강/강남/남에/에서/서(공백)/(공백)맛/맛있/.../었다.\n",
    "    - 강남/남에/에서/.../었다.\n",
    "    - 1번문장과 2번문장과 확률기반으로 접근하면 의미가 다르게 해석할수가 있어서 n-gram으로 접근\n",
    "    - 1번문장과 2번문장을 2gram으로 나누었더니 나누어진 조각의 개수가 다를수 있다.\n",
    "        - 1번과 2번이 겹치는 조각은 ex)13조각일때\n",
    "    - 그러면 1번이(50조각), 2번이(52)조각 일때, 작은것이 분모로 => 13/50 = 유사도\n",
    "- 3 gram유사도\n",
    "- 4 gram유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram (N= 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 grams 유사도: 76.19% \n",
      "분자: 16 \n",
      "분모: 21 \n",
      " ['오늘', '늘 ', ' 강', '강남', '남에', '에서', '서 ', ' 맛', '맛있', '있는', '는 ', ' 된', '된장', '장찌', '찌개', '개를', '를 ', ' 먹', '먹었', '었다', '다.'] \n",
      " ['강남', '남에', '에서', '서 ', ' 먹', '먹었', '었던', '던 ', ' 오', '오늘', '늘의', '의 ', ' 된', '된장', '장찌', '찌개', '개는', '는 ', ' 맛', '맛있', '있었', '었다', '다.'] \n",
      " ----------------------------------------\n",
      "3 grams 유사도: 45.00% \n",
      "분자: 9 \n",
      "분모: 20 \n",
      " ['오늘 ', '늘 강', ' 강남', '강남에', '남에서', '에서 ', '서 맛', ' 맛있', '맛있는', '있는 ', '는 된', ' 된장', '된장찌', '장찌개', '찌개를', '개를 ', '를 먹', ' 먹었', '먹었다', '었다.'] \n",
      " ['강남에', '남에서', '에서 ', '서 먹', ' 먹었', '먹었던', '었던 ', '던 오', ' 오늘', '오늘의', '늘의 ', '의 된', ' 된장', '된장찌', '장찌개', '찌개는', '개는 ', '는 맛', ' 맛있', '맛있었', '있었다', '었다.'] \n",
      " ----------------------------------------\n",
      "4 grams 유사도: 21.05% \n",
      "분자: 4 \n",
      "분모: 19 \n",
      " ['오늘 강', '늘 강남', ' 강남에', '강남에서', '남에서 ', '에서 맛', '서 맛있', ' 맛있는', '맛있는 ', '있는 된', '는 된장', ' 된장찌', '된장찌개', '장찌개를', '찌개를 ', '개를 먹', '를 먹었', ' 먹었다', '먹었다.'] \n",
      " ['강남에서', '남에서 ', '에서 먹', '서 먹었', ' 먹었던', '먹었던 ', '었던 오', '던 오늘', ' 오늘의', '오늘의 ', '늘의 된', '의 된장', ' 된장찌', '된장찌개', '장찌개는', '찌개는 ', '개는 맛', '는 맛있', ' 맛있었', '맛있었다', '있었다.'] \n",
      " ----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "s1 = \"오늘 강남에서 맛있는 된장찌개를 먹었다.\"\n",
    "s2 = \"강남에서 먹었던 오늘의 된장찌개는 맛있었다.\"\n",
    "\n",
    "def grams(sentence,n):\n",
    "    i=0\n",
    "    temp=[]\n",
    "    while i != len(sentence)-(n-1):\n",
    "        temp.append(sentence[i:i+n])\n",
    "        i+=1\n",
    "    return temp\n",
    "\n",
    "def s1_s2(n):\n",
    "    temp_denominator=[]\n",
    "    s1_Ngrams=grams(s1, n)\n",
    "    s2_Ngrams=grams(s2, n)\n",
    "    temp_denominator.append(len(s1_Ngrams))\n",
    "    temp_denominator.append(len(s2_Ngrams))\n",
    "    set1=set(s1_Ngrams)\n",
    "    set2=set(s2_Ngrams)\n",
    "    inter=len(set1&set2)\n",
    "    denominator = min(temp_denominator) #낮은 개수의 조각을 선택\n",
    "    similarity = inter/denominator\n",
    "    return print(\"{} grams 유사도: {:.2f}%\".format(n, similarity*100), \"\\n분자:\",inter,\"\\n분모:\",denominator,\"\\n\",s1_Ngrams,\"\\n\",s2_Ngrams,\"\\n\",\"--\"*20)\n",
    "\n",
    "n = [2,3,4]\n",
    "for i in n:\n",
    "    s1_s2(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

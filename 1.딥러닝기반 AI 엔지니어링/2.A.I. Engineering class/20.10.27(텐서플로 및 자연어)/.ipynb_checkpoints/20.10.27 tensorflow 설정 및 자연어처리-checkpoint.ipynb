{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow 버전2 보다는 1이 더 안정적인면이 있다.\n",
    "#keras 는 텐서플로우 안에 포함이 되어있는 프레임웍이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "#텐서플로우 코드를 좀 더 쉽게 작성할 수 있도록 해주는 라이브러리\n",
    "#텐서플로우를 좀더 쉽게 사용할 수 있도록 해주는 추상화된 환경\n",
    "# import tf.keras\n",
    "# 두가지 방법으로 import 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자연어 처리 패키지 : nltk, konlpy 설치\n",
    "# 아나콘다에서 버전을 확인해야함.\n",
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk는 안에있는 데이터를 받아서 사용해야한다. (모두 다운받도록 한다. 자연어처리 도구임)\n",
    "# 영어로된 패키지를 분석하는데 적절하다.\n",
    "# 한국어는 konlpy\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#konlpy 는 java가 있어야한다.\n",
    "#https://www.oracle.com/technetwork/java/javase/downloads/index.html\n",
    "# 위 주소를 통해 jdk를 설치한다.\n",
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "윈도우 환경설정을 통해서 맥도 환경설정 알아보기  \n",
    "자바 환경변수 설정을 해야한다.   \n",
    "https://whitepaek.tistory.com/28  \n",
    "이거 설정후   \n",
    "자바를 연결시켜주는 파이프 설치  \n",
    "https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype #파이썬 버전이 3.8 이면 38을 받기 받은것을 편한 경로에 추가하기 develop 폴더  \n",
    "http://corazzon.github.io/Konlpy_JPype_install_struggle #참고\n",
    "#### pip install JPype1 난 이걸로했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install JPype1\n",
    "# https://jpype.readthedocs.io/en/latest/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'After', 'preventing', 'President', 'Obama', \"'s\", 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Don't After preventing President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- token 의 의미: 의미를 갖는단위 처리하는단위.. 자연어 처리 대상(단위: 문장, 단어, 문단...)\n",
    "- 토큰: 문법적으로 더 이상 나눌 수 없는 요소\n",
    "- Obama's -> 'Obama', \"'s\" 서로 분리됨\n",
    "- Don't -> 'Do', \"n't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'After', 'preventing', 'President', 'Obama', \"'\", 's', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr', '.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(\"Don't After preventing President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPunctTokenizer() 는 Punctuation (구두점 따로 분리) \n",
    "- 적절한 토큰화 도구 선택이 필요하다.\n",
    "    - Don't -> 'Don', \"'\", 't'\n",
    "    - Obama's -> 'Obama', \"'\", 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'after', 'preventing', 'president', \"obama's\", 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', 'republicans', 'moved', 'swiftly', 'to', 'deliver', 'president', 'trump', 'a', 'victory', 'days', 'before', 'the', 'election', 'both', 'presidential', 'candidates', 'campaigned', 'in', 'pennsylvania', 'where', 'joe', 'biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'mr', 'trump', 'mocked', 'kamala', 'harris']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(\"Don't After preventing President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 알파벳이 소문자로 변경됨\n",
    "\",/!/.\" 또한 제거됨 그런데 ' (apostrophe)는 보존됨\n",
    "- 무조건적인 특수문자 제거는 좋지 않다.\n",
    "    - $\\$$100 에서 $\\$$ 기호가 사라져 버리면 100의 의미가 불분명해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 토큰화 \n",
    "- 토큰의 단위가 문장\n",
    "- 코퍼스 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Don't After preventing Ph.D. President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't After preventing Ph.D. President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election.\", 'Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text)) #문장화로 변환할때 Ph.D. 의 점을 나누지 않았다. 하나의 단어로 인식해줌.\n",
    "print(len(sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어가 어려운 이유: 형태소\n",
    "1. 형태소:\n",
    "    - 자립형태소 (명사, 감탄사, 부사)\n",
    "    - 의존형태소 (조사)\n",
    "    - 길동이가 딥러닝을 합니다. (가, 을 의존형태소)\n",
    "2. 띄어쓰기 :\n",
    "    - 지금이문장을해석하는데어려움이있나요.\n",
    "    - 아버지가방에들어가신다.\n",
    "3. 품사에 따라 단어의 의미가 달라짐\n",
    "    - mine: 지뢰(명사), 광물을 캐다(동사)\n",
    "    - 한국어도 마찬가지 -> 못: 못(명사), 못한다,못먹는다 (부사)\n",
    "- 품사태깅 : 특정 단어가 사용되기 앞서 어떤 품사로 사용되는지 구분\n",
    "    - tagging natural language (이 주제가 논문거리임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"After preventing Ph.D. Obama's from filling a vacancy four years ago.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'preventing', 'Ph.D.', 'Obama', \"'s\", 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'preventing', 'ph', 'd', \"obama's\", 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0438064cfd8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "a = word_tokenize(text)\n",
    "from nltk.tag import pos_tag\n",
    "pos_tag(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://konlpy.org/en/latest/api/konlpy.tag/#okt-class\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0\n",
    "\n",
    "nltk (Categorizing and Tagging words) tag의 의미들이 다 나와있다.\n",
    "https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석: 형태소 단위로 토큰화 한다는 의미\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '공부', '하고', '있는', '우리', '들', ',', '수료', '후', '에는', '꼭', '취업', '에', '성공해요']\n"
     ]
    }
   ],
   "source": [
    "print(okt.morphs(\"열심히 공부하고 있는 우리들, 수료 후에는 꼭 취업에 성공해요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('공부', 'Noun'), ('하고', 'Josa'), ('있는', 'Adjective'), ('우리', 'Noun'), ('들', 'Suffix'), (',', 'Punctuation'), ('수료', 'Noun'), ('후', 'Noun'), ('에는', 'Josa'), ('꼭', 'Noun'), ('취업', 'Noun'), ('에', 'Josa'), ('성공해요', 'Adjective')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(\"열심히 공부하고 있는 우리들, 수료 후에는 꼭 취업에 성공해요\"))\n",
    "# 형태소 단위로 품사 정보도 함께 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "km=Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['공부', '우리', '수료', '후', '꼭', '취업']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(\"열심히 공부하고 있는 우리들, 수료 후에는 꼭 취업에 성공해요\")) #이거사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['공부', '우리', '수료', '후', '취업', '성공']\n"
     ]
    }
   ],
   "source": [
    "print(km.nouns(\"열심히 공부하고 있는 우리들, 수료 후에는 꼭 취업에 성공해요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 전처리\n",
    "- 언어 모델 공부 : ngram, dp 알고리즘, 유사문장 찾는 알고리즘 등등..\n",
    "- 문서단어 벡터 : (tf-idf), 문서 유사도\n",
    "- 토픽모델링 (lda)\n",
    "- 머신러닝 기반 텍스트 분류 ~\n",
    "- 더 나아가서: \n",
    "    - rnn -> lstm 기반 문장생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 정제 : 코퍼스(특정 도메인 단어 집합)로 부터 노이즈 데이터 제거\n",
    "- ex) 인공지는 코퍼스 (상관없는 단어가 포함되어있을 경우 노이즈로 간주하고 제거) : 연필? <- 제거\n",
    "- ex) 법률 코퍼스 : (당뇨병? 제거)\n",
    "- 정규화 : 동의어들 -> 같은 단어로 통합\n",
    "- 불용어 제거\n",
    "- 빈도수가 낮은 단어 제거 : 연산량이 낮은단어 제거 (분류,예측작업할때 빈도수가 적으면 도움이 안됨)\n",
    "- 길이가 짧은단어도 제거대상\n",
    "    - 정규표현식을 이용하여 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "were\n",
      "have\n",
      "die\n",
      "watch\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize(\"were\")) #표제어 추출 함수\n",
    "print(wnl.lemmatize(\"has\", 'v'))\n",
    "print(wnl.lemmatize(\"dies\", 'v'))\n",
    "print(wnl.lemmatize(\"watched\", 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'going', 'doing', 'love']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [wnl.lemmatize(w) for w in ['lives','going','doing','love']]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표제어 추출과 비교할것 : 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어간 추출(stemming)\n",
    "#(만든사람이름)stemmer \n",
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "ls=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"After preventing President Obama from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'preventing', 'President', 'Obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'prevent', 'presid', 'obama', 'from', 'fill', 'a', 'vacanc', 'four', 'year', 'ago', ',', 'republican', 'move', 'swiftli', 'to', 'deliv', 'presid', 'trump', 'a', 'victori', 'day', 'befor', 'the', 'elect', '.', 'both', 'presidenti', 'candid', 'campaign', 'in', 'pennsylvania', ',', 'where', 'joe', 'biden', 'said', 'he', 'would', 'expand', 'hi', 'elector', 'map', 'and', 'mr.', 'trump', 'mock', 'kamala', 'harri', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ps.stem(w) for w in words]) #PorterStemer\n",
    "# 대문자 -> 소문자\n",
    "# 어간 추출 (Porter가 만든 규칙에 의거하여 추출됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aft', 'prev', 'presid', 'obam', 'from', 'fil', 'a', 'vac', 'four', 'year', 'ago', ',', 'republ', 'mov', 'swift', 'to', 'del', 'presid', 'trump', 'a', 'vict', 'day', 'bef', 'the', 'elect', '.', 'both', 'presid', 'candid', 'campaign', 'in', 'pennsylvan', ',', 'wher', 'joe', 'bid', 'said', 'he', 'would', 'expand', 'his', 'elect', 'map', 'and', 'mr.', 'trump', 'mock', 'kamal', 'har', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ls.stem(w) for w in words]) #LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어간(stem), 어미(ending)\n",
    "    - 어간: 긋다, 긋고, ...\n",
    "    - 어미: 이것에따라 문법적으로 의미가 달라짐. -> 잡다, 잡고, 잡기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk 불용어\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\")) #불용어사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = \"Family is not an important thing. It's everything.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=set(stopwords.words('english')) #중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word=word_tokenize(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family\n",
      "important\n",
      "thing\n",
      ".\n",
      "It\n",
      "'s\n",
      "everything\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in tokenized_word: #wt 변수에 저장된 단어들을 하나씩 읽어서 w에 저장\n",
    "    if word not in stopwords: #w 가 sw에 존재하지 않는다면\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트 컴프리핸션\n",
    "[word for word in tokenized_word if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어 제거 순서\n",
    "- 토큰화 -> 조사 -> 접속사 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"텐서플로우와 머신러닝으로 자연어 처리를 하거든. 머신러닝으로만 자연어 처리를 하면 안돼.\"\n",
    "sw = \"어쨌든 아무거나 같다 비슷하다 하면 하거든 으로\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=sw.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt=word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['텐서플로우와', '머신러닝으로', '자연어', '처리를', '하거든', '.', '머신러닝으로만', '자연어', '처리를', '하면', '안돼', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[w for w in wt if w not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['텐서플로우와', '머신러닝으로', '자연어', '처리를', '.', '머신러닝으로만', '자연어', '처리를', '안돼', '.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ranks.nl/stopwords/korean  \n",
    "불용어 만들어져 있는건데 절대적인것은 아니므로 절대적으로 사용하면 안된다.  \n",
    "- 통계적으로 많이 사용되지 않는 단어들을 모은 불용어 파일 (강사님 공유)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"전화번호 : 010-1234-5678 주소: 서울시 강남구 나이:28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '5678', '28']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#방법 1\n",
    "#숫자들만 모두 추출\n",
    "re.findall(\"\\d+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"대한민국 한국 코리아 korea 남한 고려\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국 대한민국 코리아 korea 남한 고려'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 방법 2\n",
    "# re.sub(\"정규표현식\", 치환문자(단어), 변수)\n",
    "re.sub(\"한국\", \"대한민국\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국 대한민국 코리아 대한민국 대한민국 고려'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"대한민국 한국 코리아 korea 남한 고려\"\n",
    "re.sub(\"한국|korea|남한\", \"대한민국\", text) #한번에 여러개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Her impact could be felt right away. There are major election disputes awaiting immediate action by the Supreme Court from the battleground states of North Carolina and Pennsylvania. Both concern the date by which absentee ballots may be accepted. With Justice Barrett’s elevation in place of Justice Ginsburg, a liberal icon, the court is expected to tilt decisively to the right. It is gaining a conservative who could sway cases in every area of American life, including abortion rights, gay rights, business regulation and the environment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 토큰화\n",
    "text=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Her impact could be felt right away.',\n",
       " 'There are major election disputes awaiting immediate action by the Supreme Court from the battleground states of North Carolina and Pennsylvania.',\n",
       " 'Both concern the date by which absentee ballots may be accepted.',\n",
       " 'With Justice Barrett’s elevation in place of Justice Ginsburg, a liberal icon, the court is expected to tilt decisively to the right.',\n",
       " 'It is gaining a conservative who could sway cases in every area of American life, including abortion rights, gay rights, business regulation and the environment.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['impact', 'could', 'felt', 'right', 'away'], ['major', 'election', 'disputes', 'awaiting', 'immediate', 'action', 'supreme', 'court', 'battleground', 'states', 'north', 'carolina', 'pennsylvania'], ['concern', 'date', 'absentee', 'ballots', 'may', 'accepted'], ['justice', 'barrett', 'elevation', 'place', 'justice', 'ginsburg', 'liberal', 'icon', 'court', 'expected', 'tilt', 'decisively', 'right'], ['gaining', 'conservative', 'could', 'sway', 'cases', 'every', 'area', 'american', 'life', 'including', 'abortion', 'rights', 'gay', 'rights', 'business', 'regulation', 'environment']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {} #빈 딕셔너리\n",
    "sentences = []\n",
    "stopWords=set(stopwords.words(\"english\"))\n",
    "# 텍스트 크리닝, 단어 토큰화\n",
    "# 문장 -> 단어 -> 모두 소문자화 -> 불용어 제거 -> 짧은 길이의 단어 제거...\n",
    "for i in text: # Her impact could be felt right away.\n",
    "    sentence=word_tokenize(i)\n",
    "    res=[]\n",
    "#     print(sentence)\n",
    "    for word in sentence:\n",
    "        word=word.lower() #소문자화\n",
    "        if word not in stopWords: #불용어제거\n",
    "            if len(word)>2:\n",
    "                res.append(word) #['her']\n",
    "                if word not in vocab: # 초기에 빈 vocab에 key값을 만들어줌\n",
    "                    vocab[word]=0 #word가 key, 0은 초기값 {\"her\":0, ...} \n",
    "                vocab[word]+=1 #{\"her\":0, ...} #카운트역할\n",
    "    sentences.append(res)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'impact': 1, 'could': 2, 'felt': 1, 'right': 2, 'away': 1, 'major': 1, 'election': 1, 'disputes': 1, 'awaiting': 1, 'immediate': 1, 'action': 1, 'supreme': 1, 'court': 2, 'battleground': 1, 'states': 1, 'north': 1, 'carolina': 1, 'pennsylvania': 1, 'concern': 1, 'date': 1, 'absentee': 1, 'ballots': 1, 'may': 1, 'accepted': 1, 'justice': 2, 'barrett': 1, 'elevation': 1, 'place': 1, 'ginsburg': 1, 'liberal': 1, 'icon': 1, 'expected': 1, 'tilt': 1, 'decisively': 1, 'gaining': 1, 'conservative': 1, 'sway': 1, 'cases': 1, 'every': 1, 'area': 1, 'american': 1, 'life': 1, 'including': 1, 'abortion': 1, 'rights': 2, 'gay': 1, 'business': 1, 'regulation': 1, 'environment': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 케라스 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 라고 생각하기 (아래 sentences)\n",
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], \n",
    "           ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], \n",
    "           ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
    "           ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "           ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(코퍼스) -> 의미 : 코퍼스에 등장하는 단어들에 대한 빈도수를 기준으로 단어 집합을 생성\n",
    "# 빈도수가 높은->낮은 단어 순으로 번호를 부여 -> 확인은 word_index 사용\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index #단어와 인덱스 번호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('barber', 8),\n",
       "             ('person', 3),\n",
       "             ('good', 1),\n",
       "             ('huge', 5),\n",
       "             ('knew', 1),\n",
       "             ('secret', 6),\n",
       "             ('kept', 4),\n",
       "             ('word', 2),\n",
       "             ('keeping', 2),\n",
       "             ('driving', 1),\n",
       "             ('crazy', 1),\n",
       "             ('went', 1),\n",
       "             ('mountain', 1)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts #단어별 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#각 단어별 인덱스로 변환\n",
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중요!! (단어 빈도수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 5 #가장 빈도수가 높은 5개 단어만 추출\n",
    "tokenizer = Tokenizer(num_words = vs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "#가장 빈도가 높은 5개 단어들:\n",
    "\"\"\"\n",
    "'barber': 1, \n",
    "'secret': 2, \n",
    "'huge': 3, \n",
    "'kept': 4, \n",
    "'person': 5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 5],\n",
       " [1, 3, 5],\n",
       " [2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4],\n",
       " [1, 4],\n",
       " [1, 4, 2],\n",
       " [3, 2, 1],\n",
       " [1, 3]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
